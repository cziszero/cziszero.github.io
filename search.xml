<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python中list set dict deque PriorityQueue的实现及各种操作时间复杂度]]></title>
    <url>%2F2019%2F02%2F28%2F2019-02-28-python_container%2F</url>
    <content type="text"><![CDATA[以前刷题都是使用C++, 最近觉着用Python写起来更简单一些, 但一直那Python当脚本用, 对Python里面各种数据类型的内部实现及时间复杂度反而不是很熟悉, 所以查了下比较常用的几个数据类型的内部实现, 总结如下. listlist这个名字比较具有迷惑性, 其实python中的list实际上是一个动态数组, 相当于c++中的vector. key notes 对于一个具有N个元素的列表，当一次Append操作发生时，新列表要分配多少内存（额外M个元素，需多分配一个元素存储长度）呢？答案是：M = (N &gt;&gt; 3) + (N &lt;9 ? 3 : 6) + 1 key notes 自带一个inplace的排序方法sort, 其算法是timsort, 防挂备份, 是一种稳定的排序方法, 简单的说就是混用插入排序与归并排序，二分搜索等算法，亮点是充分利用待排序数据可能部分有序的事实，并且依据待排序数据内容动态改变排序策略——选择性进行归并以及galloping. 知乎上有位答主概括的很好了, 引用如下: 完善的基本工作过程是：1.扫描数组，确定其中的单调上升段和严格单调下降段，将严格下降段反转。我们将这样的段称之为run。2.定义最小run长度，短于此的run通过插入排序合并为长度高于最小run长度；3.反复归并一些相邻run，过程中需要避免归并长度相差很大的run，直至整个排序完成；4.如何避免归并长度相差很大run呢， 依次将run压入栈中，若栈顶run X，run Y，run Z 的长度违反了X&gt;Y+Z 或 Y&gt;Z 则Y run与较小长度的run合并，并再次放入栈中。 依据这个法则，能够尽量使得大小相同的run合并，以提高性能。注意Timsort是稳定排序故只有相邻的run才能归并。5.Merge操作还可以辅之以galloping，具体细节可以自行研究。 dequedeque位于collections模块中, 使用需要from collections import deque.内部实现为一个分段数组，容器中的元素分段存放在一个个大小固定的数组中，此外容器还需要维护一个存放这些数组首地址的索引数组, 其示意图如下图所示: key notes 初始化deque的时候可以给他传一个参数maxlen，如果deque中的元素超过maxlen的值，那么就会从deque中的一边去删除元素，也就是deque始终保持maxlen最大长度的元素，如果超过了就会自动把以前的元素弹出. deque不支持切片操作 由其实现可以得知其随机索引要比list慢 set/dictpython中的set和dict都是基于hash的实现, 这个是人尽皆知的事情, 重点是如何分配hash表的长度(如何适应数据的不断增长)以及使用什么碰撞策略.其示意图如下所示:其处理碰撞的策略即为开放寻址法(Open addressing), 生成一个二次探测序列(quadratic probing sequence)依次探测.其hash表大小最初为8, 存储(k,v)对时即存在hash(k) &amp; (len-1)的位置, 使用超过了总容量的2/3时，分配一个长度更大的数组，同时将旧表中的条目复制到新的表中。数组长度调整后的长度不小于活动槽数量的4倍，即minused = 24 = 4*ma_used。而当活动槽的数量非常大（大于50000）时，调整后长度应不小于活动槽数量的2倍，即2*ma_used。 详细请探究这个blog, 中文版, 防挂备份 PriorityQueueheappq模块和queue.PriorityQueue类, PriorityQueue只是对heappq中各种操作的简单封装, 还有个很大的问题是无法将一个非堆用函数的方式调整为一个堆, 不过可以使用heappg.heapify()调整好后赋给PriorityQueue实例的queue成员. 各种操作时间复杂度其实知道内部实现之后各种操作的时间复杂度很容易就知道了, 下面附上python的官方说明, TimeComplexity]]></content>
      <categories>
        <category>语言</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenAI Spinning Up 阅读笔记]]></title>
    <url>%2F2019%2F02%2F16%2F2019-02-16-OpenAIspinningup%2F</url>
    <content type="text"><![CDATA[本篇Blog是针对OpenAI Spinning Up的一个阅读笔记, 同时也正好复习一下以前学过的知识, 总结如下. Part 1: Key Concepts in RLPolicies通常用$a_t = \mu_{\theta}(s_t)$来表示确定性的Policy, 用$a_t \sim \pi_{\theta}(\cdot | s_t)$表示随机性的Policy. Stochastic Policies通常有两种形式表示随机策略, categorical policies 和 diagonal Gaussian policies, 分别应用于离散动作空间和连续动作空间的场景. 用作描述随机策略的方法必须满足: 根据$s$可以生成随机策略 根据特定的$action$和$s$可以计算log likelihoods: $\log \pi_{\theta}(a|s)$. categorical policies所谓的categorical policies其实和分类类似, 最后经过一个softmax表示各个action的概率, 和分类的区别在于最后取哪个动作还需要按照刚算出来的概率进行抽样, 是非确定的. Diagonal Gaussian Policies通常使用多元高斯分布来作为连续action的随机policy, 描述一个多元高斯分布需要每个随机变量的均值$\mu$和他们的协方差矩阵$\Sigma$, 写作 p\left(\boldsymbol{x}\right)=\left|2\pi\Sigma\right|^{-1/2}\exp\left(-\frac{1}{2}\left(\boldsymbol{x}-\boldsymbol{\mu}\right)^T\Sigma^{-1}\left(\boldsymbol{x}-\boldsymbol{\mu}\right)\right)但在RL中, 通常使用只有对角线上非零的协方差矩阵, 协方差一般用来刻画两个随机变量的线性相关性, 也就是说表示多个action的随机变量是线性不相关的, 这样可以只用一个向量来表示协方差矩阵$\Sigma$, 通常使用log standard deviations, 即$\log \sigma_{\theta}(s)$, 因为它的取值范围是$(-\infty, \infty)$, 而标准差必须为非负数, 这样不利于训练. 其一般也有两种方式, 即: 不依赖于$s$, 使用其他方式确定, VPG, TRPO, PPO 都是使用这种方法 依赖于$s$, 和均值$\mu$一起由神经网络确定 抽样使用公式, 其中$z \sim \mathcal{N}(0, I)$, $\odot$ 表示诸元素相乘: a = \mu_{\theta}(s) + \sigma_{\theta}(s) \odot z计算对数似然使用公式: \log \pi_{\theta}(a|s) = -\frac{1}{2}\left(\sum_{i=1}^k \left(\frac{(a_i - \mu_i)^2}{\sigma_i^2} + 2 \log \sigma_i \right) + k \log 2\pi \right)Trajectories一个trajectory / episode / history $\tau$ 表示state, reward, action的序列, \tau = (s_0, a_0, r_0, s_1, a_1, r_1 ...)其中$s_0 \sim \rho_0(\cdot)$. 当前状态$s_t$根据当前动作$a_t$转移到$s_{t+1}$, 具体如何转移是由环境决定的,可以为确定性的: s_{t+1} = f(s_t, a_t)也可以为非确定性的: s_{t+1} \sim P(\cdot|s_t, a_t)Reward and Returnreward 依赖于$s_t, a_t, s_{t+1}$ , 即 $r_t = R(s_t, a_t, s_{t+1})$, 但通常可简化为$r_t = R(s_t)$ 或者$r_t = R(s_t,a_t)$而Return的定义也有两种: finite-horizon undiscounted return: $R(\tau) = \sum_{t=0}^T r_t$ infinite-horizon discounted return: $R(\tau) = \sum_{t=0}^{\infty} \gamma^t r_t$ 加折扣的原因在于: 直觉上来看越早拿到收益越好 数学上来看, 不加折扣的话可能不收敛, 这样就很难处理了 The RL ProblemRL的目标是找到一个policy使得期望回报最大化, 求期望首先要有概率分布, 以只考虑有限步长为例: P(\tau|\pi) = \rho_0 (s_0) \prod_{t=0}^{T-1} P(s_{t+1} | s_t, a_t) \pi(a_t | s_t).则目标函数为: J(\pi) = \mathop{E}\limits_{\tau \sim \pi}{R(\tau)} = \int_{\tau} P(\tau|\pi) R(\tau)我们的目标是找到最优策略$\pi^*$: \pi^* = \arg \max_{\pi} J(\pi)Value Functions On-Policy Value Function $V^{\pi}(s)$ : V^{\pi}(s) = \mathop{E}\limits_{\tau \sim \pi} {\left[R(\tau)| s_0 = s\right]} On-Policy Action-Value Function $Q^{\pi}(s,a)$ : Q^{\pi}(s,a) = \mathop{E}\limits_{\tau \sim \pi}{[R(\tau)| s_0 = s, a_0 = a]} Optimal Value Function $V^*(s)$: V^*(s) = \max_{\pi} \mathop{E}\limits_{\tau \sim \pi}{[R(\tau)| s_0 = s]} Optimal Action-Value Function $Q^*(s,a)$: Q^*(s,a) = \max_{\pi} \mathop{E}\limits_{\tau \sim \pi}{[R(\tau)| s_0 = s, a_0 = a]} 使用这种写法的时候都是指的infinite-horizon discounted return, 而finite-horizon undiscounted retur需要考虑时间$t$. 对于$V$和$Q$显然有: V^{\pi}(s) = \mathop{E}\limits_{a\sim \pi}{\left[Q^{\pi}(s,a)\right]}V^*(s) = \max\limits_{a}{Q^*(s,a)}Bellman Equationsthe on-policy 中, Bellman方程如下: V^{\pi}(s) = \mathop{E}\limits_{a \sim \pi s'\sim P}{\left[ r(s,a) + \gamma V^{\pi}(s')\right]}Q^{\pi}(s,a) = \mathop{E}\limits_{s'\sim P}{\left[r(s,a) + \gamma \mathop{E}\limits_{a'\sim \pi}{\left[Q^{\pi}(s',a')\right]}\right]}Bellman最优方程如下: V^*(s) = \max_a \mathop{E}\limits_{s'\sim P}{\left[r(s,a) + \gamma V^*(s')\right]}Q^*(s,a) = \mathop{E}\limits_{s'\sim P}{\left[r(s,a) + \gamma \max_{a' \sim \pi} Q^*(s',a')\right]}其中: $s’ \sim P$ 是 $s’ \sim P(\cdot |s,a)$ 的简写, 表示$s’$是从环境决定的转移分布中抽样的; Bellman backup 即指 Bellman方程的右边, 也称作TD-Target Advantage Functions根据公式$V^{\pi}(s) = \mathop{E}\limits_{a\sim \pi}{\left[Q^{\pi}(s,a)\right]}$, 即$V^{\pi}(s)$表示了在状态$s$处按照策略$\pi$随机抽取一个action的平均价值, 而Advantage Functions就用来表示某个action比”平均”情况好多少. A^{\pi}(s,a) = Q^{\pi}(s,a) - V^{\pi}(s)Part 2: Kinds of RL Algorithms在本文档中给出了一个主要RL方法的分类, 如下图所示: Model-Based vs Model-Free区别在于是否已知Environment的信息, 即状态转移的分布及reward的信息. 如果已知这些信息显然可以做plan, 在真正执行一个动作之前就可以大概知道其结果, 可以提高数据效率. 但缺点是很难对Environment建模, 建模好的Environment容易与真实环境存在很大bias, 使得在虚拟环境中学到的policy在真实环境中效果很差. what to learn and how to learn Policy (确定性的deterministic或非确定性的stochastic): 直接优化参数$\theta$表示的Policy $\pi_\theta(a|s)$, 通常是on policy的, 例如: A2C/A3C直接优化$J(\pi_\theta)$ PPO 通过最大化一个代替的目标函数来使优化policy Q/V 学习 $Q^*$ 或 $V^*$, 然后通过$a(s) = \arg \max_a Q^*_{\theta}(s,a)$转化得到最优策略, 通常使off-policy的. 例如: DQN C51 Q和Policy共同学习 DDPG SAC : 随机策略, 熵正则化(entropy regularization)使得学习更稳定, 效果更好 Trade-offs Policy Based的方法更稳定一些, 而value based的方法不太稳定 Value Based的方法比Policy Based的样本利用效率更高. Model-Based RL Pure Planning: 只根据已知的Model进行plan Expert Iteration: 有$\pi_\theta(a|s)$, 会产生一些样本, 然后用Monte Carlo Tree Search等方法在Model中搜索以提供一个更好的action, 来训练$\pi_\theta(a|s)$. 例子有EXIT, AlphaZero等 Data Augmentation for Model-Free Mothods: 使用Model-Free的RL方法, 但使用基于model生成的样本增强训练数据(MBVE) 或 只是用虚拟生成的数据(World Models)来training in the dream Embedding Planning Loops into Policies: 选择何时plan何时使用$\pi_\theta(a|s)$, 例如I2A. Part 3: Intro to Policy OptimizationDDPG Deep Deterministic Policy Gradient在该文档中对DDPG介绍的很简单, 基本上就是按照链式求导法则讲的. 因为这个系列文档中没有Actor Critic的内容, 所以实际上先讲了下Actor Critic的原理. Critic原文中The Q-Learning Side of DDPG实际上讲的是AC框架中Critic, Critic的作用是来估计$Q(s,a)$, 我们用参数$\phi$来估计$Q$, 即有$Q_\phi(s,a)$, 加上Bellman最优方程: Q^*(s,a) = \mathop{E}\limits_{s' \sim P} \left[r(s,a) + \gamma \max_{a'} Q^*(s', a')\right]实际上是一种自举(bootstrapping)的方法, 往后多看一步对$Q^*(s,a)$的估计更准确一些, 使用两者的差作为训练信号, 及TD-error $\delta$ \delta = Q_{\phi}(s,a) - \left(r + \gamma (1 - d) \max_{a'} Q_{\phi}(s',a') \right)其中$d$表示该episode是否结束. 以最小化TD-error为目标, 使用MSE作为metric, 得到最终的Loss函数: L(\phi, {\mathcal D}) = \mathop{E}\limits_{(s,a,r,s',d) \sim {\mathcal D}}\left[\delta ^2 \right]这就是Q-learning以及AC框架中Critic的Loss形式. 在DQN中为了稳定训练引入Replay Buffer和Target Networks.这个地方文档中在You Should Know里写了为什么可以用old experiences, 就是无论多过时的$(s,a,r,s’,d)$都应该遵循Bellman方程, 但实际上因为前面还有个求期望, 所以Replay Buffer也不能太大, 要不然old policy的$(s,a,r,s’,d)$的分布可能和现在的policy的$(s,a,r,s’,d)$分布是不一致的. —&gt; 这一点之前的理解是有问题的, 如果policy产生了变化, 即对应$s_1$的action可能不再是$a_1$而是$a_2$, 但是对于$Q(s,a)$来说是针对两个不同的值并不会产生混淆, 但由于近期之内都不会需要$Q(s_1,a_1)$了, 会让训练速度变慢, 所以Replay Buffer不能太大. 而Replay Buffer不能太小的原因是没法很好的拟合均值操作, 容易过拟合. 第二个问题为什么要使用Target Network, 文档里说的很简单, 就是说$r + \gamma (1 - d) \max_{a’} Q_{\phi}(s’,a’)$和$\phi$有关, 不用的话训练不稳定. 对此我产生一个问题是使用semi-gradient更新确实有问题, 但我可以对整个Loss重新求梯度做为更新的参数啊, 为啥非得引入target network呢? 对此, Sutton的书里说如果Td-target是静态的可以保证收敛, 但如果是动态的不能保证收敛. Actor加下来说的The Policy Learning Side of DDPG, 实际上就是AC中的Actor. 可以看到, 在$\delta$中, 有一个求$\max$的操作, 显然对于连续的情况无法高效的实现, 在DDPG中的选择是直接训练得到一个确定性的policy使得$\mu_\theta(s)$即为最优的action, 这样就可以直接应用链式求导法则来求$\theta$的梯度即可. 由于DDPG是确定性的, 无法像SPG一样天然的包含探索, 所以需要加入噪声, 原论文中说加入time-correlated OU noise, 但更新的研究证明加入mean-zero Gaussian noise就非常好了. 伪代码算法整体的伪代码如下所示: Key Notes DDPG和DQN一样, 是off policy的, 需要Replay Buffer DDPG因为用到$\frac{\partial Q(s,a)}{\partial a}$ 所以要求$a$是连续的 DDPG的主要算法思想即链式求导法则 需要注意的是训练Critic和训练Actor使用的样本是不一样的, 训练Critic时使用的是Replay Buffer中的$(s,a,r,s’,d)$元组, 而训练Actor的时候只使用$s$, 通过$\max{Q(s,\mu_\theta(s))}$来更新Policy. Soft Actor-CriticQuestions [√] 为什么Policy Optimization大多数用on policy的, 而value based的方法大部分是off policy的?和师兄讨论后的理解:Policy Optimization 的方法要优化现在的Policy, 自然不能使用以前Policy产生的$(s,a,r,s’,d)$元组, 因为分布不一样了.Value Based的方法可以使用以前Policy产生的样本的原因在于参见DDPG中关于Critic的描述 [] 学到$V^*(s)$后如何转化为policy]]></content>
      <categories>
        <category>RL</category>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>RL</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[有趣的实现/算法]]></title>
    <url>%2F2019%2F01%2F16%2F2019-01-16-interesting%2F</url>
    <content type="text"><![CDATA[next_permutation实现具体的算法原理可以参考这篇Blog, 如果失效可以查看这个本地备份, 知道了next_permutation的实现原理之后可以很容易得到prev_permutation的算法.在Leetcode上发现了一个很有趣的实现:123456void nextPermutation(vector&lt;int&gt;&amp; nums) &#123; auto i = is_sorted_until(nums.rbegin(), nums.rend()); if (i != nums.rend()) swap(*i, *upper_bound(nums.rbegin(), i, *i)); reverse(nums.rbegin(), i);&#125;]]></content>
      <categories>
        <category>算法和数据结构</category>
      </categories>
      <tags>
        <tag>算法和数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[很好的题目之股票买卖合集]]></title>
    <url>%2F2018%2F12%2F31%2F2019-01-01-BestTimetoBuyandSellStock%2F</url>
    <content type="text"><![CDATA[最近做了LeetCode股票买卖的这个系列的题目, 感觉很有意思, 想当年这个还是我读研的面试题, 当时太过紧张在买卖两次的时候就卡住了, 重新做了下, 发现了一种新的解法, 记录一下权当作回忆吧. 题目分别是: Leetcode 121.Best Time to Buy and Sell Stock 买卖一次 Leetcode 122.Best Time to Buy and Sell Stock II 买卖无数次 Leetcode 123.Best Time to Buy and Sell Stock III 买卖两次 Leetcode 123.Best Time to Buy and Sell Stock IV 最多买卖K次 买卖一次首先第一种情况很简单, 就是定义$f[i]$为在第i天卖出的最大获利, 那我只要知道在第i天及以前所能达到的最小的买入价格就好了, 这个显然可以边计算$f[i]$边维护出来, 当然实际做的时候其实并不需要把所有的$f[i]$全存起来, 只需要维护前i个中最大的就好了. 买卖无数次然后看第二种情况, 可以买卖无数次, 股票价格在不断波动, 那可以买卖无数次的情况就是所有的可以获利的区间都买了, 显然我只要求出所有的上涨的区间即可. 更简单的是一个上涨区间比如$[2,3,4]$, 那完全可以拆分成$[2,3]$, $[3,4]$两笔交易, 这样只要每次比较相邻的两个数就可以了. 买卖两次这种情况可以继承买卖一次的思路, 买卖两次而且必须先卖出再买入, 那就说明两笔交易之间有一个分割点把所有日期的股票分成两段, 需要在前半段完成一次交易, 在后半段完成一次交易. 在这个点之前必须完成第一次交易的卖出, 在这个点之后才能完成第二次交易的买入, 所以定义$f[i]$为在第i天卖出的最大获利, $s[i]$为在第i天买入的最大获利, 根据是否允许在买入的同时卖出看最后求$f[i]+s[i]$还是$f[i]+s[i+1]$就好了. 最多买卖K次到这种情况就比较有意思了, 首先说一下比较常规的DP解法.可以用一个状态机来表示现在所处的状态, 如下图所示: 可以写出状态表示, $f[i][j][0]$ 表示在第i天及以前第j次买入的最大获利, $f[i][j][1]$ 表示在第i天及以前第j次卖出的最大获利, 状态表示写出来了那就比较容易写出状态转移来了. 最开始的时候其实没想到这种状态表示, 想的是用$f[i][j]$表示前i天完成j次交易的最大获利, 这样虽然也能写出来, 但时间复杂度就比较高了. 想出来了之后发现其实这种表示方法并不少见, 关键就是状态还可以再细分成买入还是卖出, 进一步拆分之后就容易表示了. 第二种方法是一个贪心的思路, 和印象中的一道题很像(APIO/CTSC 2007数据备份), 都是带反悔的贪心.首先考虑对于第一天价格最低, 第n天价格最高的一个区间, 买卖次数越多最大获利越大. 如下图所示: 原因是我们可以找下降的区间把一笔交易拆成两笔, 那么最终总的收益就是原先一笔交易的获利+下降区间的绝对值(看图很容易理解), 同理我们可以继续往下拆下去, 找出一个下降区间就可以把原先的一笔交易拆成两笔, 找出两个就可以把一个交易拆成三笔, 并且拆的顺序是按下降多少依次进行. 那除这种情况还有什么情况呢? 这种情况的特点是”第一天价格最低, 第n天价格最高的一个区间”, 也就是说如果次数比较少的时候会优先选择整一段, 次数比较多的时候才考虑内部的子段, 那剩下的情况就是本来两个区间就是独立的, 不是一个从另一个中拆出来的. 考虑最简单的情况, 这个区间由两端上升区间组成, $(v1,p1)$和$(v2,p2)$, 只有下面四种情况: 可以发现第一种情况就是上面说的”第一天价格最低, 第n天价格最高的一个区间”, 其他情况的话绝对不会选$(v1,p2)$, 因为有比他更优的选择, 两个区间$(v1,p1)$, $(v2,p2)$ 分开优先取大的即可. 还需要考虑的是在第一种情况内部包含了包好了多个第二种情况的子区间, 那拆的时候实际上不是找单调递减最多的一个区间来把原区间拆成两个, 而是只要这个区间减少的最多就可以了, 如下图所示: 这里实现起来技巧性非常强, 下面这段代码给了一个非常优雅的实现, 值得认真体会:1234567891011121314151617181920212223242526272829303132333435363738int maxProfit(int k, vector&lt;int&gt; &amp;prices) &#123; int n = (int)prices.size(), ret = 0, v, p = 0; priority_queue&lt;int&gt; profits; stack&lt;pair&lt;int, int&gt; &gt; vp_pairs; while (p &lt; n) &#123; // find next valley/peak pair v = p; while (v &lt; n - 1 &amp;&amp; prices[v + 1] &lt;= prices[v]) v++; p = v + 1; while (p &lt; n &amp;&amp; prices[p] &gt;= prices[p - 1]) p++; // save profit of 1 transaction at last v/p pair, if current v is lower than last v while (!vp_pairs.empty() &amp;&amp; prices[v] &lt; prices[TOP.first]) &#123; profits.push(prices[TOP.second - 1] - prices[TOP.first]); vp_pairs.pop(); &#125; // save profit difference between 1 transaction (last v and current p) and // 2 transactions (last v/p + current v/p), if current v is higher than // last v and current p is higher than last p while (!vp_pairs.empty() &amp;&amp; prices[p - 1] &gt;= prices[TOP.second - 1]) &#123; profits.push(prices[TOP.second - 1] - prices[v]); v = TOP.first; vp_pairs.pop(); &#125; // 这里包含了 上面说的包含子区间不符合第一种情况 , 把这个子区间替换掉. vp_pairs.push(pair&lt;int, int&gt;(v, p)); &#125; // save profits of the rest v/p pairs while (!vp_pairs.empty()) &#123; profits.push(prices[TOP.second - 1] - prices[TOP.first]); vp_pairs.pop(); &#125; // sum up first k highest profits for (int i = 0; i &lt; k &amp;&amp; !profits.empty(); i++) &#123; ret += profits.top(); profits.pop(); &#125; return ret;&#125; 参考1 拆分的思路2 优雅的实现]]></content>
      <categories>
        <category>算法和数据结构</category>
      </categories>
      <tags>
        <tag>算法和数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[各种树结构]]></title>
    <url>%2F2018%2F12%2F31%2F2018-12-31-Tree%2F</url>
    <content type="text"><![CDATA[树的存储 树的前序、中序、后序遍历 二叉树 普通树转为二叉树 堆 题目 完全二叉树 题目 哈夫曼树 二叉排序树 并查集 树状数组 模板 题目 trie 模板 题目 树的存储树的前序、中序、后序遍历 遍历 洛谷1030 求先序排列： 给出一棵二叉树的中序与后序排列。求出它的先序排列。 Leetcode 145 Binary Tree Postorder Traversal 非递归求二叉树的后序遍历. 如果先序遍历时先访问右儿子再访问左儿子, 则先序遍历和正常的后序遍历结果一样, 有几种实现非常优雅, 非递归实现时重点关注遇到NULL该怎么办, 思路就比较清晰了, 因为如果不是NULL, 那肯定是访问左儿子. Leetcode 232 Implement Queue using Stacks 非递归实现很容易, 可以练习下非递归实现 二叉树 Leetcode 687.Longest Univalue Path 很简单的一道题, 但需要注意的是过程量和结果要分别维护 Leetcode 226.Invert Binary Tree 简单题, 交换左右儿子 Leetcode 617.Merge Two Binary Trees 简单题, 遍历下就好了 Leetcode 669.Trim a Binary Search Tree 熟悉BST的结构 Leetcode 971.Flip Binary Tree To Match Preorder Traversal Leetcode 114.Flatten Binary Tree to Linked List 二叉树转为链表, 有非递归的简单解法 普通树转为二叉树堆题目 Leetcode 23.Merge k Sorted Lists Leetcode 218.The Skyline Problem 扫描线算法 完全二叉树题目 Leetcode 958.Check Completeness of a Binary Tree 考察对完全二叉树的理解 哈夫曼树 luogu P1090 合并果子 NOIP2004 (luogu_1090_fruit.cpp) 二叉排序树 Leetcode 530. Minimum Absolute Difference in BST/783.Minimum Distance Between BST Nodes 两个题一模一样, 简单题, 中序遍历 并查集 洛谷P1525 关押罪犯 | 带权并查集或分点 | 中等 P2024 食物链 | 带权并查集 | 中上 POJ 2524 统计集合数量(poj_2524_Religions.cpp) 银河英雄传说 洛谷 1196(luogu_1196_galaxy.cpp) Leetcode 399.Evaluate Division 类似带权并查集, 也可以用bfs/dfs做 树状数组模板算法思想如下图所示: 具体过程可以参考Blog, 如原链接失效, 可以访问这个本地备份 . 123456789101112131415161718192021222324252627282930313233343536373839404142class BinaryIndexedTree &#123;public: BinaryIndexedTree(vector&lt;int&gt; nums) &#123; N = nums.size(); bit = new int[N + 1]; raw = new int[N + 1]; bit[0] = 0; copy(nums.begin(), nums.end(), bit + 1); copy(nums.begin(), nums.end(), raw + 1); for (int i = 1; i &lt; N; i++) &#123; int j = i + (i &amp; -i); if (j &lt;= N) bit[j] += bit[i]; &#125; &#125; void update(int i, int val) &#123; i++; int d = val - raw[i]; raw[i] = val; while (i &lt;= N) &#123; bit[i] += d; i += (i &amp; -i); &#125; &#125; int rangeSum(int i, int j) &#123; return preSum(j) - preSum(i - 1); &#125; int preSum(int i) &#123; int res = 0; i++; while (i) &#123; res += bit[i]; i -= (i &amp; -i); &#125; return res; &#125;private: int N; int *bit, *raw;&#125;; 题目 洛谷 3374 (luogu_3374_treearray.cpp) Leetcode 307 Range Sum Query - Mutable 模板题 trie模板1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class TrieNode &#123;public: bool eow = false; int id = 0; TrieNode *children[26] = &#123;&#125;;&#125;;class Trie &#123;public: TrieNode *root; Trie() &#123; root = new TrieNode(); &#125; void insert(const string &amp;s, int id = 0) &#123; TrieNode *curr = root; for (int i = 0; i &lt; s.size(); i++) &#123; int index = s[i] - 'a'; if (!curr-&gt;children[index]) curr-&gt;children[index] = new TrieNode(); curr = curr-&gt;children[index]; &#125; curr-&gt;eow = true; curr-&gt;id = id; &#125; int find(const string &amp;s) &#123; TrieNode *curr = root; for (int i = 0; i &lt; s.size(); i++) &#123; int index = s[i] - 'a'; if (curr &amp;&amp; curr-&gt;children[index]) curr = curr-&gt;children[index]; else return -1; &#125; return curr-&gt;eow ? curr-&gt;id : -1; &#125; void displayUtil(TrieNode *curr, string &amp;s) &#123; if (curr-&gt;eow) cout &lt;&lt; s &lt;&lt; endl; for (int i = 0; i &lt; 26; i++) &#123; if (curr-&gt;children[i]) &#123; s.push_back(i + 'a'); displayUtil(curr-&gt;children[i], s); s.pop_back(); &#125; &#125; &#125; void displayTrie() &#123; string s; displayUtil(root, s); &#125;&#125;; 题目 Leetcode 211.Add and Search Word leetcode 720.Longest Word in Dictionary 先排序然后检索前缀是否在里面, 用HashMap做检索的话时间复杂度是$O(nklogn)+O(nk)$, 其中n是单词表中单词的个数, k是单词的平均长度. 然而和前缀有关, 所以可以用Trie做, 插入时间复杂度$O(nk)$, 检查时间复杂度是$O(nk)$ leetcode 208.Implement Trie (Prefix Tree) leetcode 212.Word Search II 因为有前缀, 所以可以用Tire优化]]></content>
      <categories>
        <category>算法和数据结构</category>
      </categories>
      <tags>
        <tag>算法和数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[STL]]></title>
    <url>%2F2018%2F12%2F31%2F2018-12-31-STL%2F</url>
    <content type="text"><![CDATA[容器 vector、deque、list set 题目 map stack、queue priority_queue 迭代器 算法 sort、stable_sort、 partial_sort、nth_element、partition copy、copy_backward、remove_copy find、find_if、find_first_of、count/count_if swap、swap_ranges、min、max transform/for_each fill/fill_n rotate、next_permutation、prev_permutation binary_search、equal_range 容器vector、deque、listset题目 Leetcode 347.Top K Frequent Elements 计数 mapstack、queuepriority_queue迭代器算法sort、stable_sort、partial_sort、nth_element、partitioncopy、copy_backward、remove_copyfind、find_if、find_first_of、count/count_ifswap、swap_ranges、min、maxtransform/for_eachfill/fill_nrotate、next_permutation、prev_permutationbinary_search、equal_range]]></content>
      <categories>
        <category>算法和数据结构</category>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>算法和数据结构</tag>
        <tag>C/C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用算法题目分类]]></title>
    <url>%2F2018%2F12%2F31%2F2018-12-31-AlgorithmCatalog%2F</url>
    <content type="text"><![CDATA[模板 基础 模拟 高精度 加 减 乘 除 排序 简单排序 插入排序 选择排序 冒泡排序 高级排序 堆排序 快速排序 归并排序 非基于比较的排序 计数排序 桶排序 树 LCA问题 倍增算法 tarjan算法 图结构 图的概念 图的存储 邻接矩阵 邻接表 边表 图的遍历 DFS BFS 图的传递闭包 最小生成树 kruskal prim 最短路算法 无权最短路（BFS） floyd算法（floyd找最小环） Dijkstra算法 Bellman-Ford算法 SPFA（Shortest Path Faster Algorithm） 差分约束系统 拓扑排序 Kahn拓扑排序算法 基于DFS的拓扑排序算法 欧拉路径 Fleury算法 Hierholzer算法 图的强连通分量 Kosaraju算法（双DFS) Tarjan算法 Gabow算法 Hash 康托展开 搜索 BFS及优化 判重 双向广搜 DFS及优化 可行性剪枝 最优性剪枝 迭代加深搜索 分治法 二分法 字符串处理 KMP算法 数学相关 同余相关 整除的基本性质 欧几里德算法 Stein算法 扩展欧几里德算法 中国剩余定理 排列组合 排列数 组合数即扩展公式 二项式定理 素数相关 算术基本定理 筛法求素数 欧拉函数、欧拉定理、费马小定理 素数测试 快速幂 矩阵快速幂 ST算法 贪心法 带反悔的贪心 贪心法的证明 检索 Two-Pointer 技巧题 待归类 模板 SPFA poj2387_spfa.cpp 基础 洛谷1085 不高兴的津津：求最大数 洛谷1008 三连击：模运算及标志数组的使用 洛谷1423 小玉在游泳：while循环 洛谷1055 ISBN号码：字符串处理或者模运算，ASCII表 洛谷1047 校门外的树：标志数组的使用 洛谷1428 小鱼比可爱：双重循环 洛谷1017 进制转换，进一步熟悉循环的使用 P1003 铺地毯，介绍二维数组 P1424 小鱼的航程(改进版) P1427 小鱼的数字游戏(数组和循环的简单应用) P1914 小书童——密码(模运算和循环的简单应用) P1200 [USACO1.1]你的飞碟在这儿Your Ride Is He(模运算和循环的简单应用) P1540 机器翻译（队列的简单应用） P1067 多项式输出 模拟 字符串处理 P1125 笨小猴 字符串处理+100以内的质数判断 P1017 进制转换 负进制数 P1160 队列安排 教链表 P1996 约瑟夫问题 P1425 小鱼的游泳时间 100 P1089 津津的储蓄计划 0 P1046 陶陶摘苹果 P1075 质因数分解 P1028 数的计算 P1149 火柴棒等式 P1308 统计单词数 P1036 选数 P1015 回文数 P1308 统计单词数题目 P1217 [USACO1.5]回文质数 Prime Palindromes P1618 三连击（升级版） 模拟 P1079 Vigenère 密码 简单模拟 P1035 级数求和 简单模拟 玛雅历 2017算法课第一次作业 A poj1008 取余运算 Leetcode 957.Prison Cells After N Days 模拟+取余 高精度加 P1601 A+B Problem（高精） P1015 回文数 减 P2142 高精度减法 乘 P1009 阶乘之和 除排序 P1068 分数线划定 简单排序 简单排序插入排序选择排序冒泡排序高级排序堆排序快速排序归并排序非基于比较的排序计数排序桶排序树LCA问题LCA和RMQ问题 LCA 洛谷3397 POJ1330 Nearest Common Ancestors | LCA | 简单 CF609E Minimum spanning tree for each edge | LCA | 中上 POJ 1986Distance Queries | LCA | 较难 倍增算法倍增LCA的一个讲的比较好的资料 tarjan算法Tarjan LCA的一个讲的比较好的资料 图结构图的概念图的存储邻接矩阵邻接表边表图的遍历 洛谷 P1330 封锁阳光大学(染色) Leetcode 959.Regions Cut By Slashes 见图好好想想 DFS DFS 输出所有的哈密顿回路 HDU 2181 (hdu_2181_Hamiltonian.cpp) Leetcode 685.Redundant Connection II 在图上找环, 并考察dfs树的理解 BFS 统计无向图的联通块个数，POJ 2524(poj_2524_Religions_bfs.cpp) 仙岛求药 在棋盘上BFS求最短路 图的传递闭包 图的传递闭包，高精加和高精乘，洛谷1037，NOIP2002年普及组(luogu_1037_transClosure.cpp) 最小生成树 洛谷 P2330 [SCOI2005] 繁忙的都市（最小生成树模板题） kruskal 洛谷 P1546 最短网络 Agri-Net 模板题 (luogu_1546_kruskal.cpp、luogu_1546_prim.cpp) Conscription (POJ No.3723) POJ 2349 Arctic 很巧妙的的把问题转化为最小生成树问题 (poj_2349_Arctic.cpp) http://bailian.openjudge.cn/practice/2349/ prim最短路算法无权最短路（BFS） 洛谷2296 寻找道路：BFS求最短路，需要先把图反向 floyd算法（floyd找最小环） 洛谷 P1119 灾后重建（Floyd的理解） Dijkstra算法Bellman-Ford算法SPFA（Shortest Path Faster Algorithm） POJ2387 Til the Cows Come Home 最短路模板题(poj2387_spfa.cpp) P1073 最优贸易 （图论，经典题，前后各做一次spfa） 洛谷3371 【模板】单源最短路径 要求分别使用SPFA，bellman-ford，Dijkstra（朴素及堆优化）算法实现 洛谷 P1339 [USACO09OCT]热浪Heat Wave 最短路模板题 洛谷 P1462 通往奥格瑞玛的道路（二分+最短路） 洛谷 P1346 电车（建图有点意思，难度不大） Roadblocks (POJ No.3255) 次短路 P1027 Car的旅行路线 最短路 Poj 1860 Currency Exchange 判环 差分约束系统 Layout (POJ No.3169) 拓扑排序 拓扑排序算法的实现，poj 2367，(poj_2367_topSort_kahn.cpp) P2019 脑力达人之拓扑序列路径总数 Kahn拓扑排序算法基于DFS的拓扑排序算法欧拉路径 P2731 骑马修栅栏 | 欧拉回路 | 简单 P1341 无序字母对(欧拉回路) Fleury算法Hierholzer算法图的强连通分量 POJ2186 Popular Cows （牛仰慕） 求强连通分量缩点 Kosaraju算法（双DFS)Tarjan算法Gabow算法Hash康托展开搜索 棋盘问题 类似八皇后 POJ1321 P1034 矩形覆盖 NOIP 2002 P1092 虫食算 noip2004提高组第4题 P1312 Mayan游戏 BFS及优化 P1074 靶形数独 NOIP2009 提高组 第四题 （可加入位运算优化） 判重双向广搜DFS及优化 P1219 八皇后 可行性剪枝最优性剪枝迭代加深搜索分治法 Leetcode 974 Subarray Sums Divisible by K 类似逆序对 Leetcode 315 Count of Smaller Numbers After Self 类似逆序对 二分法 洛谷 P1462 通往奥格瑞玛的道路（二分+最短路） P1182 数列分段Section II (二分答案) P1314 聪明的质监员 二分问题 P1083 借教室 前缀和+二分 字符串处理KMP算法数学相关同余相关整除的基本性质欧几里德算法 P1029 最大公约数和最小公倍数问题 Stein算法扩展欧几里德算法中国剩余定理排列组合排列数组合数即扩展公式二项式定理 P1313 计算系数 杨辉三角 素数相关算术基本定理筛法求素数 P3383 【模板】线性筛素数 欧拉函数、欧拉定理、费马小定理素数测试快速幂 P1226 取余运算||快速幂 矩阵快速幂ST算法贪心法 完成P3602 Koishi Loves Segments 完成P2094 运输 POJ2376 洛谷P1970 花匠 洛谷P1223 排队接水 P3620 [APIO2007]数据备份 贪心 P1080 国王游戏 带反悔的贪心 APIO/CTSC 2007数据备份 Leetcode 123.Best Time to Buy and Sell Stock IV 贪心法的证明检索有很多题目可以规约成从某个集合中检索某个元素, 常用的方法有hash, Trie, 平衡树例如 leetcode 1.Two Sum 两个数a,b和为s, 已知a和s, 确认b是否在集合中 leetcode 720.Longest Word in Dictionary 先排序然后检索前缀是否在里面, 用HashMap做检索的话时间复杂度是$O(nklogn)+O(nk)$, 其中n是单词表中单词的个数, k是单词的平均长度. 然而和前缀有关, 所以可以用Trie做, 插入时间复杂度$O(nk)$, 检查时间复杂度是$O(nk)$ Leetcode 963 Minimum Area Rectangle II 使用HashMap检索 Leetcode 966 Vowel Spellchecker 使用HashMap检索或者Trie检索都可以 Two-Pointer Leetcode 3.Longest Substring Without Repeating Characters Leetcode 395.Longest Substring with At Least K Repeating Characters 枚举不同的字母有几个, 最多26个, 给定不同字母个数, 找到Longest Substring with At Least K Repeating Characters是$O(n)$的, 所以总的时间复杂度是$O(n)$的. 分治的方法最坏情况下时间复杂度是$O(n^2)$ Leetcode 4.Median of Two Sorted Arrays 始终维护在两个指针左边的有n/2个, 可以扩展到求第k个, 只要维护在两个指针左边的有k个即可. 同时有另外一种思路, 直接找第k个元素, 每次取两个有序数列的第k/2个比较, 会发现有一个肯定不包含第k个元素, 可以直接去掉. 详情参见链接 Leetcode 336.Palindrome Pairs 给一堆字符串, 找成对字符串连接可以构成回文串的, 实际上就是检索反转字符串是否也存在, 不过需要注意各种情况, 比如一个字符串为空, 两个字符串不等长, 两个字符串等长等, 既可以用Trie也可以用HashMap. Leetcode 904.Fruit Into Baskets 与之前一道最长无重复字母子区间类似 技巧题 Leetcode 42 Trapping Rain Water Leetcode 407 Trapping Rain Water BFS+堆, 但建模是重点 Leetcode 238 Product of Array Except Self Leetcode 233 Number of Digit One 从1到n的整数中1出现的次数, 仔细诸位统计即可 Leetcode 976 Largest Perimeter Triangle 认真分析 Leetcode 961 N-Repeated Element in Size 2N Array 有O(1)的算法, 仔细想想, 发现该题目中只要两个数相同即为答案, 随机找两个数是答案的概率为25%. 待归类 POJ 1191 火车进站 P2758 编辑距离 P1772 [ZJOI2006]物流运输 P1334 瑞瑞的木板 P1007 独木桥 P1311 选择客栈 P1541 乌龟棋 P1315 观光公交 P1006 传纸条 NOIP2008 P3623 [APIO2008]免费道路 P1155 双栈排序 NOIP2008]]></content>
      <categories>
        <category>算法和数据结构</category>
      </categories>
      <tags>
        <tag>算法和数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用算法-动态规划]]></title>
    <url>%2F2018%2F12%2F31%2F2018-12-31-AlgorithmDP%2F</url>
    <content type="text"><![CDATA[线型动态规划 隐含少数状态枚举 区间型动态规划 背包型动态规划 树型动态规划 动态规划的优化 滚动数组 状态压缩 线型动态规划 POJ 3903 最长上升子序列，如果要过这道题目必须使用二分优化(poj_3903_LDS_nlogn.cpp、poj_3903_LDS_nn.cpp实现的是最长下降子序列) 最长公共子序列 POJ1458 luogu3402 (poj_1458_LCS.cpp) P1854 花店橱窗布置 P1091 合唱队形 最短回文串，POJ1159 将一个字符串变为回文串最少插入多少个字符 需要使用滚动数组优化(poj_1159_Palindrome.cpp) 棋盘分割 poj 1191 (poj_1191_chessboard.cpp) POJ 3280 给出一个字符串，要求将其修改成一个回文字符串，给出修改某种字母（添加或删除）的价值，求最小使其成为回文字符串的价值 P1280 尼克的任务 (线性DP) 关键路径 青蛙过河(NOIP2005) 洛谷1052 状态压缩的线性dp P1018 乘积最大 和有点区别 高精度+线性DP Leetcode 股票买卖合集 Leetcode 10.Regular Expression Matching 带*和.的字符串匹配, 不算很难 Leetcode 516.Longest Palindromic Subsequence 和LCS类似,找一个字符串中的最长回文子序列, Leetcode 5 Longest Palindromic Substring 找一个字符串中的最长回文子串, 和上一题类似, 看似有$O(n)$的算法, 实际上真的有, 不过不好像, $O(n^2)$的很好想. 隐含少数状态枚举 Leetcode 964 Least Operators to Express Number 区间型动态规划 P1880 石子合并 P1040 加分二叉树 整数划分（Poj上3181的Dollar Dayz是另一种整数划分） 凸多边形的三角剖分 多边形（IOI98） P1005 矩阵取数游戏 区间动规 codevs 3546 矩阵链乘法 背包型动态规划 P1048 采药 (经典的01背包问题) P1060 开心的金明 裸的01背包问题 P2066 机器分配（背包问题） P1858 多人背包（k优解） 树型动态规划 P1040 加分二叉树 P2014 选课 树上的背包 普通树、多叉树转二叉树均可 P2016 战略游戏 P1352 没有上司的舞会/Leetcode 337. House Robber III 动态规划的优化滚动数组状态压缩]]></content>
      <categories>
        <category>算法和数据结构</category>
      </categories>
      <tags>
        <tag>算法和数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性数据结构]]></title>
    <url>%2F2018%2F12%2F31%2F2018-12-31-linear_DS%2F</url>
    <content type="text"><![CDATA[链表链表不算是抽象数据结构, 不过因为比较常用, 有些操作也比较典型, 所以也单独拿出来说下. Leetcode 2.Add Two Numbers 非常简单, 实现两个链表表示的数的加法, 而且头节点是个位, 不用对齐. Leetcode 206 Reverse Linked List 栈 P1739 表达式括号匹配 / Leetcode 20.Valid Parentheses 逆波兰式求解 中缀表达式求解 Leetcode 946 Validate Stack Sequences 判断压栈和出栈顺序是否相同 Leetcode 155 Min Stack类似单调队列 队列 Leetcode 225 Implement Stack using Queues 单调队列 Leetcode 962 Maximum Width Ramp 使用单调队列可以优化到$O(N)$ Leetcode 496 Next Greater Element I Leetcode 503 Next Greater Element II]]></content>
      <categories>
        <category>算法和数据结构</category>
      </categories>
      <tags>
        <tag>算法和数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用算法和数据结构的模板]]></title>
    <url>%2F2018%2F12%2F27%2F2018-12-27-AlgorithmTemplate%2F</url>
    <content type="text"><![CDATA[前言本篇blog主要用于整理做题过程中经常用到的算法和数据结构, 不断完善中. 树结构Trie相关题目: Leetcode720 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class TrieNode &#123;public: bool eow = false; int id = 0; TrieNode *children[26] = &#123;&#125;;&#125;;class Trie &#123;public: TrieNode *root; Trie() &#123; root = new TrieNode(); &#125; void insert(const string &amp;s, int id = 0) &#123; TrieNode *curr = root; for (int i = 0; i &lt; s.size(); i++) &#123; int index = s[i] - 'a'; if (!curr-&gt;children[index]) curr-&gt;children[index] = new TrieNode(); curr = curr-&gt;children[index]; &#125; curr-&gt;eow = true; curr-&gt;id = id; &#125; int find(const string &amp;s) &#123; TrieNode *curr = root; for (int i = 0; i &lt; s.size(); i++) &#123; int index = s[i] - 'a'; if (curr &amp;&amp; curr-&gt;children[index]) curr = curr-&gt;children[index]; else return -1; &#125; return curr-&gt;eow ? curr-&gt;id : -1; &#125; void displayUtil(TrieNode *curr, string &amp;s) &#123; if (curr-&gt;eow) cout &lt;&lt; s &lt;&lt; endl; for (int i = 0; i &lt; 26; i++) &#123; if (curr-&gt;children[i]) &#123; s.push_back(i + 'a'); displayUtil(curr-&gt;children[i], s); s.pop_back(); &#125; &#125; &#125; void displayTrie() &#123; string s; displayUtil(root, s); &#125;&#125;;]]></content>
      <categories>
        <category>算法和数据结构</category>
      </categories>
      <tags>
        <tag>算法和数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++ 中清空stringstream]]></title>
    <url>%2F2018%2F12%2F27%2F2018-12-27-CppGrammar%2F</url>
    <content type="text"><![CDATA[使用stringstream时的清空操作在C++中可以使用stringstream来很方便的进行类型转换，字符串连接等，不过注意重复使用同一个stringstream对象时要先清空,但clear方法实际上是初始化所有标志位,并不会对rdbuf()的内容进行修改, 即 Sets the stream error state flags by assigning them the value of state. By default, assigns std::ios_base::goodbit which has the effect of clearing all error state flags. If rdbuf() is a null pointer (i.e. there is no associated stream buffer), then state | badbit is assigned. May throw an exception. 如果要清空内容, 则要使用.str(&quot;&quot;)方法。]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[信息安全]]></title>
    <url>%2F2018%2F09%2F19%2F2018-06-18-xxaq%2F</url>
    <content type="text"><![CDATA[第一章信息安全概念的历史变化和含义通信保密（COMSEC）：60-70年代 信息保密信息安全（INFOSEC）：80-90年代 机密性、完整性、可用性、不可否认性 等信息保障（IA）： 90-00年代 PDRR赛博安全（Cyber Security）：00-? Cyber-Physical-System安全通信技术(加密)→计算机系统安全→以互联网为基础网络安全→网络（空间）安全 导致信息安全问题的原因有哪些?计算机在网络通信环境下进行信息交换所面临的安全威胁有哪些?安全的信息交换应满足哪些性质?信息安全的目标三个基本方面是：– 机密性(Confidentiality) 即保证信息为授权者享用而不泄漏给未经授权者。– 完整性(Integrity) 数据完整性，未被未授权篡改或者损坏 系统完整性，系统未被非授权操纵，按既定的功能运行– 可用性 (Availability) 即保证信息和信息系统随时为授权者提供服务，而不要出现非授权者滥用却对授权者拒绝服务的情况。其他方面：– 信息的不可否认性 Non-repudiation ：要求无论发送方还是接收方都不能抵赖所进行的传输– 鉴别 Authentication 鉴别就是确认实体是它所声明的。适用于用户、进程、系统、信息等– 审计 Accountability 确保实体的活动可被跟踪– 可靠性 Reliability 特定行为和结果的一致性 ISO 7498-2－1989确立了基于OSI参考模型的七层协议之上的信息安全体系结构，其中五大类安全服务分别是么?鉴别 访问控制 保密性 完整性 抗否认 1999年9月我国制定并颁布了《计算机信息系统安全保护等级划分准则》,该准则将计算机信息系统分别划分为哪几个等级?用户自主保护级系统审计保护级安全标记保护级结构化保护级访问验证保护级 P2DR安全模型• 策略：是模型的核心，具体的实施过程中，策略意味着网络安全要达到的目标。• 防护：安全规章、安全配置、安全措施• 检测：异常监视、模式发现• 响应：报告、记录、反应、恢复P2DR代表的分别是Policy(策略)，Protection(防护)、Detection(检测)和Response(响应)的首字母。按照P2DR的观点，一个良好的完整的动态安全体系，不仅需要恰当的防护(比如操作系统访问控制，防火墙、加密等)，而且需要动态的检测机制(比如入侵检测、漏洞扫描等)，在发现问题时还需要及时做出响应，这样的一个体系需要在统一的安全策略指导下进行实施，由此形成一个完备的，闭环的动态自适应安全体系。P2DR模型是建立在基于时间的安全理论基础之上的。该理论的基本思想是信息安全相关的所有活动，无论是攻击行为.防护行为，检测行为还是响应行为，都要消耗时间，因而可以用时间尺度来衡量一个体系的能力和安全性。网络安全定义：Pt &gt; Dt + Rt 第二章密码算法的分类? 按照保密的内容分 : 受限制的（ restricted) 算法 : 算法的保密性基于保持算法的秘密。 基于密钥（ key-based) 的算法 : 算法的保密性基于对密钥的保密。 基于密钥的算法，按照密钥的特点分类： 对称密码算法（ symmetric cipher) ：又称传统密码算法（ conventional cipher) ，就是加密密钥和解密密钥相同，或实质上等同，即从一个易于推出另一个。又称秘密密钥算法或单密钥算法。 非对称密钥算法（ asymmetric cipher): 加密密钥和解密密钥不相同，从一个很难推出另一个。又称公开密钥算法（ public-key cipher) 。公开密钥算法用一个密钥进行加密 , 而用另一个进行解密。其中的加密密钥可以公开 , 又称公开密钥（ public key) ，简称公钥 . 解密密钥必须保密 , 又称私人密钥（ private key) 私钥 . 简称私钥。 按照明文的处理方法： 分组密码（ block cipher): 将明文分成固定长度的组，用同一密钥和算法对每一块加密，输出也是固定长度的密文。序列密码是手工和机械密码时代的主流 流密码（ stream cipher): 又称序列密码 . 序列密码每次加密一位或一字节的明文，也可以称为流密码。 古典密码算法和现代对称分组密码算法的基本手段是什么? 有何本质差别?代替 (substitution) :明文中的每一个字符被替换成密文中的另一个字符。接收者对密文做反向替换就可以恢复出明文。置换（permutation) 又称换位（ transposition cipher) ：明文的字母保持相同，但顺序被打乱了。 代替加密的原理和实现 单表代换密码：Caesar Cipher： only have 26 possible ciphers移位（ shift ）密码、乘数（ multiplicative) 密码 仿射（ affine ) 密码、多项式（ Polynomial) 密码 密钥短语（ Key Word) 密码 多表代换密码： 可以用矩阵变换方便地描述多字母代换密码，有时又称起为矩阵变换密码。维吉尼亚（ Vigenere) 密码博福特（ Beaufort ）密码滚动密钥 (running-key) 密码弗纳姆 (Vernam) 密码 置换加密的原理和实现密码分析的基本类型有哪些? 唯密文攻击： Oscar 具有密文串 y. 已知明文攻击 : Oscar 具有明文串 x 和相应的密文 y. 选择明文攻击： Oscar 可获得对加密机的暂时访问， 因此他能选择明文串 x 并构造出相应的密文串 y 。 选择密文攻击 :Oscar 可暂时接近密码机 , 可选择密文串 y ，并构造出相应的明文 x.这一切的目的在于破译出密钥或密文 对称密码分析的两个基本方法 系统分析法（统计分析法） 穷举法 密码算法的安全性含义? 无条件安全（Unconditionally secure）无论破译者有多少密文,他也无法解出对应的明文,即使他解出了,他也无法验证结果的正确性.One time padding 计算上安全（Computationally secure） 破译的代价超出信息本身的价值 破译的时间超出了信息的有效期. 现代对称分组密码的两个基本的设计原则和方法. 混乱 (confusion) ：使得密文的统计特性与密钥的取值之间的关系尽量复杂 扩散（ Diffusion): 明文的统计结构被扩散消失到密文的长程统计特性 ,使得明文和密文之间的统计关系尽量复杂 典型算法的名称DESTriple DESIDEARC5RC6AES 分组密码的操作模式,各有何特点? 电子密码本ECB (electronic codebook mode):适合于传输短信息 简单和有效 可以并行实现 不能隐藏明文的模式信息, 相同明文 -&gt; 相同密文, 同样信息多次出现造成泄漏 对明文的主动攻击是可能的, 信息块可被替换、重排、删除、重放 误差传递：密文块损坏-&gt;仅对应明文块损坏 密码分组链接CBC (cipher block chaining) 没有已知的并行实现算法 能隐藏明文的模式信息– 需要共同的初始化向量IV– 相同明文 不同密文– 初始化向量IV可以用来改变第一块 对明文的主动攻击是不容易的,信息块不容易被替换、重排、删除、重放– 误差传递：密文块损坏两明文块损坏 安全性好于ECB 适合于传输长度大于64位的报文，还可以进行用户鉴别,是大多系统的标准如 SSL、IPSec 密码反馈CFB (cipher feedback) 分组密码-&gt;流密码 没有已知的并行实现算法 隐藏了明文模式 需要共同的移位寄存器初始值IV 对于不同的消息，IV必须唯一 误差传递：一个单元损坏影响多个单元 输出反馈OFB (output feedback) OFB:分组密码流密码 没有已知的并行实现算法 隐藏了明文模式 需要共同的移位寄存器初始值IV 误差传递：一个单元损坏只影响对应单元 对明文的主动攻击是可能的, 信息块可被替换、重排、删除、重放 安全性较CFB差 计数器模式CTR(Counter ) efficiency,can do parallel encryptions, in advance of need, good for bursty high speed links random access to encrypted data blocks provable security (good as other modes) but must ensure never reuse key/counter values, otherwise could break (cf OFB)RC4 RC4是Ron Rivest 1987年为RSA公司设计的一种流密码。– 可变密钥长度– 面向字节操作 主要设计思想 - 流密码可用一类伪随机数发生器实现– 密钥做为随机数种子– 产生密钥流 keystream (不重复,或极大周期)– XOR（plaintext，keystream） 数据长度不满足分组长度整数倍时的处理办法？双重DES和三重DES的原理和安全性分析DES不能成为群对于 DES和所有56比特密钥，给定任意两个密钥K1和K2，都能找到一个密钥K3,使得E K2 (E K1 (P)) = E K3 (P) 是不可能的。中间相遇(meet-in-the-middle)攻击困难 *International Data Encryption Algorithm*RC5 适用于软件或者硬件实现 运算速度快 能适应于不同字长的程序（一个字的bit数是RC5的一个参数；） 加密的轮数可变（轮数是RC5的第二个参数） 密钥长度是可变的（密钥长度是RC5的第三个参数） 对内存要求低 依赖于数据的循环移位（增强抗攻击能力） 先进对称分组加密算法的特点 可变的密钥长度: RC5 混合的运算 IDEA 数据相关的圈数 RC5 密钥相关的圈数 CAST-128 密钥相关的S盒: Blowfish 冗长密钥调度算法： Blowfish 可变的F：CAST-128 可变长明文/密文块长度 可变圈数 每圈操作作用于全部数据 DES和AES分别属于Feistel结构和SP网络结构 Feistel结构： 给明文分组(L,R) 对R进行加密 密文=加密后的R+L（即LR的位置交换） SP网络结构轮输入首先被一个由子密钥控制的可逆函数S作用，然后再对所得结果用置换（或可逆线性变换）P作用。S和P分别被称为混乱层和扩散层，主要起混乱和扩散作用。AES 优点 没有发现弱密钥或补密钥 能有效抵抗目前已知的攻击算法– 线性攻击– 差分攻击 加密的位置： 链路方式 Data exposed in sending node Data exposed in intermediate node The packet head almost in secure. The traffic pattern is secure to some dgree. Require one key per host to node and node to node A degree of host authentication. 在较低的网络层次上实现 端到端的方式：-The source host encrypts the data.The destination host decrypts it.-The packet head is in the clear.-The user data are secure.-The traffic pattern is insecure.-Require one key per user pair-A degree of user authentication.-在较高的网络层次上实现 总结 两种方式共同使用有更高的安全性 The amount of traffic can be observed. Traffic padding is useful. 源与目的地需有共享密钥 *IBE CPK CPK是“Combined Public Key”的缩写，即中文名为组合公钥，是一种加密算法，以很小的资源，生成大规模密钥。 IBE identity-based encryption和签名方案。首先假设存在一个可信的密钥生成中心（trusted key generation center，KGG）；用户选择他的名字（或者是，网络地址，所在街道地址门牌号，电话号码）作为公钥，相应的私钥由KGG计算出来分配给每个加入网络中的用户。 单项陷门函数 严格单向函数： 一个单射函数f: X-&gt;Y称为是严格单向函数，如果下述条件成立：存在一个有效的方法，对所有的x可计算f（x），但不存在一个有效的办法由y= f（x）计算x。 算术基本定理：任意大于1的整数a都能被因式分解为素因子幂的形式 中国剩余定理 费马小定理： p素数,a是整数且不能被p整除,则: a^(p-1) = 1 mod p Euler数: Euler数(n)定义为小于n且与n互素的正整数个数 Euler定理: 若a与n为互素的正整数,则: a\phi(n) = 1 mod n 素性测试: 确定一个给定的大数是否是素数。目前还没有简单有效的方法解决该问题, Miller-Rabin素数测试算法(利用Fermat定理) 素数定理。n附近的素数分布情况为：平均每ln(n)个整数中有一个素数。 公钥密码的提出和应用范围 对称密码算法的不足 （1）密钥管理量的困难 （2）密钥必须通过某一信道协商， （3）数字签名的问题 公钥密码的基本思想和要求 加密与解密由不同的密钥完成 知道加密算法,从加密密钥得到解密密钥在计算上是不可行的 两个密钥中任何一个都可以用作加密而另一个用作解密(不是必须的) 产生一对密钥是计算可行的 已知公钥和明文，产生密文是计算可行的 接收方利用私钥来解密密文是计算可行的 对于攻击者，利用公钥来推断私钥是计算不可行的 已知公钥和密文，恢复明文是计算不可行的 (可选)加密和解密的顺序可交换 三种重要的公钥算法的名称及其依据 MH公钥算法 背包问题背包问题描述：给定重量分别为a1 ,a2 ,…an 的n个物品,装入一个背包中,要求重量等于一个给定值。那么，究竟是那些物品？做法：明文为X，S为密文奥妙在于有两类背包，一类可以在线性时间内求解，另一类则不能，把易解的背包问题修改成难解的背包问题：公开密钥使用难解的背包问题；私钥使用易解的背包问题 是第一个推广的公钥加密算法 安全性基于背包问题 在实践过程中，大多数的背包方案都已被破解，或者证明存在缺陷 它表示了如何将NP完全问题用于公开密钥算法 大整数分解问题（The Integer Factorization Problem, RSA体制） –加/解密、密钥交换、数字签名 公开密钥 {e, n} n（两素数p和q的乘积）（推荐p,q等长）e（与(p-1)(q-1)互素） ed = 1(mod(p-1)(q-1)) 私有密钥 {d, p, q} d（e^-1 mod(p-1)(q-1) ) 加密 c=m e mod n 解密 m=c d mod n 对RSA的选择密文攻击:不要用RSA对陌生人的随机文件签名，签名前先使用一个散列函数 对RSA的公共模攻击: 不要让一群用户共享一个模n re1+se2=1 对RSA的小加密指数攻击: 加密前将消息与随机值混合，并保证m与n有相同的长度 对RSA的小解密指数攻击: 使用较小的d会产生穷尽解密攻击的可能 注意：应选择一个大的d值 定时攻击: 基于加密程序运行时间的攻击, 一种全新的攻击手段, 选择密文的攻击,适用于攻击其他公钥算法; 对策 使用恒定的幂运算时间 加一个随机延迟 盲化 为了抵抗现有的整数分解算法，对RSA模n的素因子p和q还有如下要求：(1)|p-q|很大，通常 p和q的长度相同；(2)p-1 和q-1分别含有大素因子p1和q1(3)P1-1和q1-1分别含有大素因子p2和q2(4)p+1和q+1分别含有大素因子p3和q3 强素数 离散对数问题y=g^x mod p 已知g,x,p,计算y是容易的 已知y,g,p,计算x是困难的。− 有限域的乘法群上的离散对数问题(The Discrete Logarithm Problem, ElGamal体制）− 定义在有限域的椭圆曲线上的离散对数问题（The Elliptic Curve Discrete Logarithm Problem, 类比的 ElGamal体制） R=kQ中的k *Diffie-Hellman密钥交换算法 密钥交换方案:不能用于交换任意信息,允许两个用户可以安全地建立一个秘密信息，用于后续的通讯过程,该秘密信息仅为两个参与者知道 算法的安全性依赖于有限域上计算离散对数的难度 中间人攻击：O无法计算出a^(XaXb) mod p, O永远必须实时截获并冒充转发,否则会被发现 *EIGamal算法 加/解密、密钥交换、数字签名既可以用于加密，也可以用于签名，其安全性依赖于有限域上计算离散对数的难度 攻击ElGamal加密算法等价于解离散对数问题 要使用不同的随机数k来加密不同的信息 对称及非对称算法的优缺点 对称密码算法– 运算速度快、密钥短、多种用途（随机数产生、Hash函数）、历史悠久– 密钥管理困难（分发、更换）– 对称的，通信方是平等的（不能为发送者提供保护） 非对称密码算法– 只需保管私钥、可以相当长的时间保持不变、需要的数目较小– 运算速度慢、密钥尺寸大、历史短– 非对称的，通信方是不平等的，因为加密消息和验证签名的人不能解密同一信息和产生同样的签名 消息鉴别的目的第一，验证信息的发送者是真正的，而不是冒充的，此为信源识别；第二，验证信息的完整性，在传送或存储过程中未被篡改，重放或延迟等。 鉴别函数 消息加密函数:用完整信息的密文作为对信息的鉴别。 对称密钥加密提供保密，提供鉴别差错控制，内部差错控制 外部差错控制 非对称密钥加密 zheng 公钥加密 提供保密，不提供鉴别 私钥加密 提供鉴别和签名 公钥加密私钥加密过的 保密、鉴别与签名 消息鉴别码MAC:公开函数+密钥产生一个固定长度的值作为鉴别标识使用一个密钥生成一个固定大小的小数据块，并加入到消息中，称MAC （Message Authentication Code）， 或密码校验和（cryptographic checksum） 接收者可以确信消息M未被改变。 接收者可以确信消息来自所声称的发送者； 如果消息中包含顺序码（如HDLC,X.25,TCP），则接收者可以保证消息的正常顺序；基于DES的MAC 散列函数:是一个公开的函数，它将任意长的信息映射成一个固定长度的信息。 散列函数的特性1、H可以作用于一个任意长度的数据块；2、H产生一个固定长度的输出；3、对任意给定的x ,H(x) 计算相对容易，无论是软件还是硬件实现。4、对任意给定码h，找到x满足H(x)=h具有计算不可行性；（单向性）5、对任意给定的数据块x，找到满足H(y)=H(x)的yx具有计算不可行性。6、找到任意数据对(x,y)，满足H(x) = H(y)是计算不可行的。（生日攻击： 对长度为m位的散列码，共有2^m 个可能的散列码，若要使任意的x,y 有H(x)=H(y)的概率为0.5,只需k=2^(m/2)） 输入长度可变：H可应用于任意大小的数据块 输出长度固定：H产生定长的输出 效率：对任意的x，计算H(x)容易 抗原像攻击（单向性）：任意给定h,找到y满足H(y)=h在计算上是不可行的 抗第二原像攻击（抗弱碰撞性）：对任意给定的分块x,找到y≠x且H(x)=H(y)的y在计算上是不可行的 抗碰撞攻击（抗强碰撞性）：找到任何满足H(x)=H(y)的偶对(x,y)在计算上是不可行的 伪随机性：H的输出满足伪随机性测试标准 带秘密密钥的Hash函数:消息的散列值由只有通信双方知道的秘密密钥K来控制。此时，散列值称作MAC。 不带秘密密钥的Hash函数：消息的散列值的产生无需使用密钥。此时，散列值称作MDC。 散列函数的构造方法 基于数学难题的构造方法：计算速度慢，不实用 利用对称密码体制来设计Hash : 分组链接 Block Chaining 用对称加密算法构造hash函数 且许多这样的hash函数被证明不安全(与E的安全性无关) 直接设计–把原始消息M分成一些固定长度的块Yi–最后一块padding并使其包含消息M长度–设定初始值CV0–压缩函数f, CVi=f(CVi-1,Yi-1)–最后一个CVi为hash值 经典的散列算法 MD5 :+SHA-1使用little-endian 128bits SHA-1 :+SHA-1使用big-endian 160bits SHA-2 : – SHA-256（add -224 version) – SHA-384 – SHA-512 RIPEMD-160 :little-endian +RIPEMD为128位 +更新后成为RIPEMD-160 +对密码分析的抵抗力好于SHA-1 HMAC : 无需修改地使用现有的散列函数 当出现新的散列函数时,要能轻易地替换 保持散列函数的原有性能不会导致算法性能的降低 使用和处理密钥的方式简单 对鉴别机制的安全强度容易分析,与hash函数有同等的安全性 SHA-3 ， 最 早 称 为 Keccak ， 数字签名：特点、分类+传统签名的基本特点:能与被签的文件在物理上不可分割签名者不能否认自己的签名签名不能被伪造容易被验证+数字签名是传统签名的数字化,基本要求:能与所签文件“绑定”签名者不能否认自己的签名签名不能被伪造容易被自动验证 必须能够验证作者及其签名的日期时间； 必须能够认证签名时刻的内容； 签名必须能够由第三方验证，以解决争议； 签名必须是依赖于被签名信息的一个位串模式； 签名必须使用某些对发送者是唯一的信息，以防止双方的伪造与否认； 必须相对容易生成该数字签名； 必须相对容易识别和验证该数字签名； 伪造该数字签名在计算复杂性意义上具有不可行性，既包括对一个已有的数字签名构造新的消息，也包括对一个给定消息伪造一个数字签名； 在存储器中保存一个数字签名副本是现实可行的。 对签名方案的攻击模型 唯密钥攻击(Key-only attack) 已知消息攻击(know message attack) 选择消息攻击对签名方案的攻击目的 完全破译(total break) 选择性伪造(selective forgery) 存在性伪造(existential forgery) 数字签名分类 以验证方式分 直接数字签名direct digital signature直接数字签名的缺点 验证模式依赖于发送方的保密密钥；– 发送方要抵赖发送某一消息时，可能会声称其私有密钥丢失或被窃，从而他人伪造了他的签名。– 通常需要采用与私有密钥安全性相关的行政管理控制手段来制止或至少是削弱这种情况，但威胁在某种程度上依然存在。– 改进的方式例如可以要求被签名的信息包含一个时间戳（日期与时间），并要求将已暴露的密钥报告给一个授权中心。 X的某些私有密钥确实在时间T被窃取，敌方可以伪造X的签名及早于或等于时间T的时间戳。 仲裁数字签名arbitrated digital signature 引入仲裁者– 通常的做法是所有从发送方X到接收方Y的签名消息首先送到仲裁者A，A将消息及其签名进行一系列测试，以检查其来源和内容，然后将消息加上日期并与已被仲裁者验证通过的指示一起发给Y。 仲裁者在这一类签名模式中扮演敏感和关键的角色。所有的参与者必须极大地相信这一仲裁机制工作正常。（trusted system）+以计算能力分无条件安全的数字签名计算上安全的数字签名+以可签名次数分一次性的数字签名： 如果一个签名方案仅给一则消息签名时是安全的，则签名方案是一次性签名方案。当然可以进行若干次验证， Lamport 数字签名方案， Lamport方案缺陷:签名信息比较长多次性的数字签名+具有特殊性质的数字签名 普通数字签名算法– RSA– EIGamal/ Schnorr方案– DSS/DSA 不可否认的数字签名算法没有签名者的合作，签名就不能得到验证。从而防止了由她签署的电子文档资料没有经过她的同意而被复制和分发的可能性。适用于知识产权产品分发控制。带来的问题：签名者在认为对其不利时拒绝合作，从而否认他曾签署的文件。 如果要阻止她主观否认，一个不可否认签名与一个否认协议(Disavowal Protocol)结合：签名者执行否认协议可以向法庭或公众证明一个伪造的签名确实是假的；如果签名者拒绝参与执行否认协议，就表明签名事实上是真的由其签署的。由三部分组成：签名算法、验证协议、否认协议。 群签名算法只有群成员能代表所在的群签名，接收者能验证签名所在的群,但不知道签名者，需要时,可借助于群成员或者可信机构找到签名者。应用: 投标，群数字签名方案由三个算法组成：签名算法、验证算法和识别算法 盲签名算法消息内容对签名者不可见签名被接收者泄漏后,签名者无法追踪签名 应用: 电子货币,电子选举 盲签名过程:消息盲变换签名接收者逆盲变换 *协议 协议的分类 仲裁协议 裁决协议：包括两个低级的子协议：一个是非仲裁子协议，执行协议的各方每次想要完成的，另一个是裁决子协议，仅在例外的情况下，即有争议的时候才执行，这种特殊的仲裁者叫裁决人。 自动执行协议： 协议本身就保证了公平性，不需要仲裁者来完成协议，也不需要裁决者来解决争端。 对协议的攻击 攻击目标：– 攻击协议使用的密码算法和密码技术 – 攻击协议本身 攻击方式：– 被动攻击：与协议无关的人能窃听协议的一部分或全部。– 主动攻击：改变协议以便对自己有利。假冒、删除、代替、重放 密钥分配– 基于对称密码体制的密钥分配– 基于公开密码体制的秘密密钥分配 PKI基本概念PKI（Public Key Infrastructure）公钥基础设施是提供公钥加密和数字签名服务的系统或平台，目的是为了管理密钥和证书。一个机构通过采用PKI 框架管理密钥和证书可以建立一个安全的网络环境。PKI 主要包括四个部分：X.509 格式的证书（X.509 V3）和证书废止列表CRL（X.509 V2）；CA 操作协议；CA 管理协议；CA 政策制定。一个典型、完整、有效的PKI 应用系统至少应具有以下五个部分； 1） 认证中心CA CA 是PKI 的核心，CA 负责管理PKI 结构下的所有用户（包括各种应用程序）的证书，把用户的公钥和用户的其他信息捆绑在一起，在网上验证用户的身份，CA 还要负责用户证书的黑名单登记和黑名单发布，后面有CA 的详细描述。 2） X.500 目录服务器 X.500 目录服务器用于发布用户的证书和黑名单信息，用户可通过标准的LDAP 协议查询自己或其他人的证书和下载黑名单信息。 3） 具有高强度密码算法(SSL)的安全WWW服务器 Secure socket layer(SSL)协议最初由Netscape 企业发展，现已成为网络用来鉴别网站和网页浏览者身份，以及在浏览器使用者及网页服务器之间进行加密通讯的全球化标准。 4） Web（安全通信平台） Web 有Web Client 端和Web Server 端两部分，分别安装在客户端和服务器端，通过具有高强度密码算法的SSL 协议保证客户端和服务器端数据的机密性、完整性、身份验证。 5） 自开发安全应用系统 自开发安全应用系统是指各行业自开发的各种具体应用系统，例如银行、证券的应用系统等。完整的PKI 包括认证政策的制定（包括遵循的技术标准、各CA 之间的上下级或同级关系、安全策略、安全程度、服务对象、管理原则和框架等）、认证规则、运作制度的制定、所涉及的各方法律关系内容以及技术的实现等。完整的PKI系统必须具有权威认证机构(CA)、数字证书库、密钥备份及恢复系统、证书作废系统、应用接口（API）等基本构成部分，构建PKI也将围绕着这五大系统来着手构建。 Diffie-Hellman密钥交换协议及中间人攻击及改进办法鉴别的概念和两种情形:实体(身份)鉴别和数据原发鉴别 实体鉴别（身份鉴别）： 实体鉴别就是确认实体是它所声明的。某一实体确信与之打交道的实体正是所需要的实体。只是简单地鉴别实体本身的身份，不会和实体想要进行何种活动相联系。在实体鉴别中，身份由参与某次通信连接或会话的远程参与者提交。这种服务在连接建立或在数据传送阶段的某些时刻提供，使用这种服务可以确信(仅仅在使用时间内): 一个实体此时没有试图冒充别的实体，或没有试图将先前的连接作非授权地重演。 实现身份鉴别的途经三种途径之一或他们的组合（1）所知（Knowledge）:密码、口令（2）所有（Possesses):身份证、护照、信用卡、钥匙（3）个人特征：指纹、笔迹、声纹、手型、血型、视网膜、 虹膜、DNA以及个人动作方面的一些特征实体鉴别可以分为本地和远程两类。 本地多用户鉴别：实体在本地环境的初始化鉴别（就是说，作为实体个人，和设备物理接触，不和网络中的其他设备通信）。– 需要用户进行明确的操作 远程用户鉴别：连接远程设备、实体和环境的实体鉴别。– 通常将本地鉴别结果传送到远程。 实体鉴别可以是单向的也可以是双向的。 单向鉴别是指通信双方中只有一方向另一方进行鉴别。 双向鉴别是指通信双方相互进行鉴别。 *实体鉴别实现安全目标的方式① 作为访问控制服务的一种必要支持，访问控制服务的执行依赖于确知的身份（访问控制服务直接对达到机密性、完整性、可用性及合法使用目标提供支持）；② 作为提供数据起源认证的一种可能方法（当它与数据完整性机制结合起来使用时）；③ 作为对责任原则的一种直接支持，例如，在审计追踪过程中做记录时，提供与某一活动相联系的确知身份。 *对身份鉴别系统的要求（1）验证者正确识别合法申请者的概率极大化。（2）不具有可传递性（Transferability)（3）攻击者伪装成申请者欺骗验证者成功的概率要小到可以忽略的程度（4）计算有效性（5）通信有效性（6）秘密参数能安全存储 （7）交互识别 （8）第三方的实时参与 （9）第三方的可信赖性 （10）可证明的安全性 鉴别机制: 非密码的鉴别机制 口令机制 一次性口令机制：动态口令卡 询问—应答机制 基于地址的机制 基于个人特征的机制 个人鉴别令牌 基于密码算法的鉴别强鉴别（strong authentication):通过密码学的询问-应答(challenge-response)协议实现的身份鉴别，询问-应答协议的思想是一个实体向另一个实体证明他知道有关的秘密知识，但不向验证者提供秘密本身。这通过对一个时变的询问提供应答来实现，应答通常依赖于实体的秘密和询问。询问通常是一个实体选择的一个数（随机和秘密地）。 采用对称密码的鉴别机制 基于对称密码算法的鉴别依靠一定协议下的数据加密处理。 通信双方共享一个密钥（通常存储在硬件中），该密钥 在询问—应答协议中处理或加密信息交换。 无可信第三方参与的鉴别 – 单向鉴别：使用该机制时，两实体中只有一方被鉴别。 – 双向鉴别：两通信实体使用此机制进行相互鉴别。 有可信第三方参与的鉴别 采用公开密码算法的机制 采用密码校验函数的机制 零知识证明技术 重放攻击的形式和对抗措施 常见的消息重放 攻击形式有：① 简单重放：攻击者简单复制一条消息，以后在重新发送它；② 可被日志记录的复制品：攻击者可以在一个合法有效的时间窗内重放一个带时间戳的消息；③ 不能被检测到的复制品：这种情况可能出现，原因是原始信息已经被拦截，无法到达目的地，而只有重放的信息到达目的地。④ 反向重放，不做修改。向消息发送者重放。当采用传统对称加密方式时，这种攻击是可能的。因为消息发送者不能简单地识别发送的消息和收到的消息在内容上的区别。 对策：① 针对同一验证者的重放：非重复值 A. 序列号：计数的策略：对付重放攻击的一种方法是在认证交换中使用一个序数来给每一个消息报文编号。仅当收到的消息序数顺序合法时才接受之。但这种方法的困难是要求双方必须保持上次消息的序号。 B. 时间戳： A接受一个新消息仅当该消息包含一个时间戳，该时间戳在A看来，是足够接近A所知道的当前时间；这种方法要求不同参与者之间的时钟需要同步。 一旦时钟同步失败要么协议不能正常服务，影响可用性(availability)，造成拒绝服务(DOS) 要么放大时钟窗口，造成攻击的机会 时间窗大小的选择应根据消息的时效性来确定 C. 验证者发送随机值（如询问）：不可预测、不重复。A期望从B获得一个消息：首先发给B一个随机值(challenge)；B收到这个值之后，对它作某种变换，并送回去； A收到B的response，希望包含这个随机值；在有的协议中，这个challenge也称为nonce。可能明文传输，也可能密文传输；这个条件可以是知道某个口令，也可能是其他的事情。变换例子：用密钥加密，说明B知道这个密钥;简单运算，比如增一，说明B知道这个随机值。询问/应答方法不适应非连接性的应用，因为它要求在传输开始之前先有握手的额外开销，这就抵消了无连接通信的主要特点。② 针对不同验证者的重放：验证者的标识符 鉴别和交换协议的核心问题鉴别和密钥交换协议的核心问题有两个：– 保密性– 时效性 典型鉴别协议 *询问握手鉴别协议CHAP（challenge-Handshake Authentication Protocol） 优点– 协议简单、易于实现– 虽为单向鉴别协议，但可以通过在另一方的配置请求，实现对通信实体的双向鉴别– 通过不断地改变鉴别标识符和提问消息的值来防止重放(playback)攻击。– 利用周期性的提问防止通信双方在长期会话过程中被攻击。 缺点– CHAP鉴别基于共享密钥，给密钥管理带来巨大不便，不适合于大规模用户鉴别– CHAP要求提供明文的密钥形式。 *S/Key 协议： S/key protocol主要用于防止重放攻击 三个组成部分 客户端程序：为端用户提供登录程序，并在得到服务器质询值时，获取用户私钥，并调用口令计算器形成本次鉴别口令，然后发送给服务器程序 口令计算器：负责产生本次口令 服务器程序：验证用户口令 整个过程中，用户的私钥不会暴露在网络上 过程① 用户登录② 客户端向服务器发出登录请求③ 服务器向客户端发出S/KEY质询④ 客户端要求用户键入私钥⑤ 私钥与服务器发出的质询问被送入计算器⑥ 计算器产生本次口令⑦ 客户端将口令传给服务器，服务器对之进行验证 Needham/Schroeder Protocol [1978]假定攻击方C已经掌握A和B之间通信的一个老的会话密钥。C可以在第3步冒充A利用老的会话密钥欺骗B。除非B记住所有以前使用的与A通信的会话密钥，否则B无法判断这是一个重放攻击。如果C可以中途阻止第4步的握手信息，则可以冒充A在第5步响应。从这一点起，C就可以向B发送伪造的消息而对B来说认为是用认证的会话密钥与A进行的正常通信。 Denning Protocol [1982]改进版Needham/Schroeder Protocol，抵御重放攻击，但必须依靠各时钟均可通过网络同步。如果发送者的时钟比接收者的时钟要快，攻击者就可以从发送者窃听消息，并在以后当时间戳对接收者来说成为当前时重放给接收者。这种重放将会得到意想不到的后果。（称为抑制重放攻击）。一种克服抑制重放攻击的方法是强制各方定期检查自己的时钟是否与KDC的时钟同步。另一种避免同步开销的方法是采用临时数握手协议。 KEHN92 *一个基于临时值握手协议：WOO92a *一个基于临时值握手协议：WOO92b Kerberos– 基于口令的鉴别协议– 利用对称密码技术建立起来的鉴别协议– 可伸缩性——可适用于分布式网络环境– 环境特点User-to-server authentication X.509鉴别服务 个体的公钥信息– 算法– 参数– 密钥 签发人唯一标识符 个体唯一标识符 扩展域 签名 • 与Kerberos协议相比，X.509鉴别交换协议有一个很大的优点：X.509不需要物理上安全的在线服务器，因为一个证书包含了一个认证授权机构的签名。公钥证书可通过使用一个不可信的目录服务被离线地分配。• X.509双向交换拥有Kerberos的缺陷，即依赖于时戳，而X.509三向交换克服了这一缺陷。 Web安全：SQL注入攻击的原理和抵御机制、浏览器的同源策略(Same Origin Policy)、跨站脚本(XSS)攻击和跨站请求伪造(CSRF)的攻击原理和效果 SSL协议：X.509证书的构成及扩展1.1. 版本号.标识证书的版本（版本1、版本2或是版本3）。 1.2. 序列号标识证书的唯一整数，由证书颁发者分配的本证书的唯一标识符。 1.3. 签名用于签证书的算法标识，由对象标识符加上相关的参数组成，用于说明本证书所用的数字签名算法。例如，SHA-1和RSA的对象标识符就用来说明该数字签名是利用RSA对SHA-1杂凑加密。 1.4. 颁发者证书颁发者的可识别名（DN）。 1.5. 有效期证书有效期的时间段。本字段由”Not Before”和”Not After”两项组成，它们分别由UTC时间或一般的时间表示（在RFC2459中有详细的时间表示规则）。 1.6. 主体证书拥有者的可识别名，这个字段必须是非空的，除非你在证书扩展中有别名。 1.7. 主体公钥信息主体的公钥（以及算法标识符）。 1.8. 颁发者唯一标识符标识符—证书颁发者的唯一标识符，仅在版本2和版本3中有要求，属于可选项。 1.9. 主体唯一标识符证书拥有者的唯一标识符，仅在版本2和版本3中有要求，属于可选项。 2、X.509证书扩展部分可选的标准和专用的扩展（仅在版本2和版本3中使用），扩展部分的元素都有这样的结构： Extension ::= SEQUENCE { extnID OBJECT IDENTIFIER, critical BOOLEAN DEFAULT FALSE, extnValue OCTET STRING }extnID：表示一个扩展元素的OID critical：表示这个扩展元素是否极重要 extnValue：表示这个扩展元素的值，字符串类型。 扩展部分包括： 2.1. 发行者密钥标识符证书所含密钥的唯一标识符，用来区分同一证书拥有者的多对密钥。 2.2. 密钥使用一个比特串，指明（限定）证书的公钥可以完成的功能或服务，如：证书签名、数据加密等。 如果某一证书将 KeyUsage 扩展标记为“极重要”，而且设置为“keyCertSign”，则在 SSL 通信期间该证书出现时将被拒绝，因为该证书扩展表示相关私钥应只用于签写证书，而不应该用于 SSL。 2.3. CRL分布点指明CRL的分布地点。 2.4. 私钥的使用期指明证书中与公钥相联系的私钥的使用期限，它也有Not Before和Not After组成。若此项不存在时，公私钥的使用期是一样的。 2.5. 证书策略由对象标识符和限定符组成，这些对象标识符说明证书的颁发和使用策略有关。 2.6. 策略映射表明两个CA域之间的一个或多个策略对象标识符的等价关系，仅在CA证书里存在。 2.7. 主体别名指出证书拥有者的别名，如电子邮件地址、IP地址等，别名是和DN绑定在一起的。 2.8. 颁发者别名指出证书颁发者的别名，如电子邮件地址、IP地址等，但颁发者的DN必须出现在证书的颁发者字段。 2.9. 主体目录属性指出证书拥有者的一系列属性。可以使用这一项来传递访问控制信息。 SSL/TLS协议的密码套件(Cipher-suites)1个authentication (认证)算法1个encryption(加密)算法1个message authentication code (消息认证码 简称MAC)算法1 个key exchange(密钥交换)算法 记录协议和握手协议的多种典型构成 SSL记录协议– 建立在可靠的传输协议(如TCP)之上– 它提供连接安全性，有两个特点• 保密性，使用了对称加密算法• 完整性，使用HMAC算法– 用来封装高层的协议• SSL握手协议– 客户和服务器之间相互鉴别– 协商加密算法和密钥– 它提供连接安全性，有三个特点• 身份鉴别，至少对一方实现鉴别，也可以是双向鉴别• 协商得到的共享密钥是安全的，中间人不能够知道• 协商过程是可靠的 软件安全：恶意软件的分类病毒程序和其它恶意内容后门(Backdoor or Trapdoor)逻辑炸弹(Logic Bomb)特洛伊木马(Trojan Horse)僵尸(Zombie)病毒(Viruses)邮件病毒（Email Virus）蠕虫（Worms） 缓冲区溢出、整数溢出和C格式化字符串溢出漏洞• 缓冲区：程序运行期间，在内存中分配的一个连续的区域，用于保存包括字符数组在内的各种数据类型。• 溢出：所填充的数据超出了原有缓冲区的边界，并非法占据了另一段内存区域。• 缓冲区溢出：由于填充数据越界而导致程序原有流程的改变，黑客借此精心构造填充数据，让程序转而执行特殊的代码，最终获得系统的控制权。 硬件安全：硬件安全等级(FIPS 140-2)和典型攻击]]></content>
      <categories>
        <category>课程</category>
        <category>信息安全</category>
      </categories>
      <tags>
        <tag>信息安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入浅出强化学习 原理入门]]></title>
    <url>%2F2018%2F09%2F19%2F2018-11-13-RL%2F</url>
    <content type="text"><![CDATA[基本 迭代计算平均这是下面很多迭代式更新公式的主要根据 S_n = \frac{1}{N}\sum_{i=1}^{n}{a_i}=S_{n-1}+\frac{1}{N}(a_n-S_{n-1})重要性抽样Model-based DP方法v_{\pi}(s)=\sum_{a\in A}{\pi(a|s)}(R_{s}^{a}+\gamma\sum_{s'\in S}{P_{ss'}^{a}v_{\pi}(s')})是一个bootstrpd的方法，因为有${P_{ss’}^{a}}$项，所以是model-based的。 Model-free的基于表格方法MC方法因为是model-free的, 上面的${P_{ss’}^{a}}$项未知, 所以从定义出发 v_{\pi}(s)=E_{\pi}[G_t|S_t=s] \\ =E_{\pi}[\sum_{k=0}^{\infty}{\gamma^kR_{t+k+1}|S_t=s}]对$E_{\pi}$使用MC方法采样代替。 更新公式为: v_k(s)=\frac{1}{k}\sum_{j=1}^k{G_j}(s) \\ =v_{k-1}(s)+\frac{1}{k}(G_k(s)-v_{k-1}(s))实际中使用固定的$\alpha$代替$\frac{1}{k}$, 得到 v_k(s) \leftarrow v(s) + \alpha (G_k(s)-v(s))使用$\leftarrow$表示更新操作. 类似的有: Q(s,a) \leftarrow Q(s,a)+\alpha(G_t-Q(s,a))TD方法Q(s,a) \leftarrow Q(s,a)+\alpha[r+\gamma Q(s',a')-Q(s,a)]$TD(\lambda)$方法Q(s,a) \leftarrow Q(s,a)+\alpha[G_t^\lambda-Q(s,a)]其中$G_t^\lambda$为: G_t^\lambda=(1-\lambda)[G_t^1+\lambda G_t^2 +\lambda^2 G_t^3+...+\lambda^{n-1} G_t^n其中$G_t^n$表示向后看几步. 本质上$TD(\lambda)$方法是TD方法和MC方法的折衷, 因为MC方法虽然是无偏估计, 但因为需要一个回合结束才能计算, 这个过程中随机性很大, 所以方差非常大. 而TD方法由于计算TD-Target时使用了$Q(s,a)$本身, 所以是有偏估计, 但好处是方差较小, 落实到具体的事件中, 就是较容易训练. $TD(\lambda)$方法采用多步估计值组合, 比TD方法扩大了方差但估计更准确. Model-free的基于值函数逼近的方法]]></content>
      <categories>
        <category>RL</category>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>RL</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python升级和常用包的安装脚本]]></title>
    <url>%2F2018%2F06%2F15%2F2018-06-15-updatePython_install%2F</url>
    <content type="text"><![CDATA[升级python3.5并设置python3.6为默认注意：如果使用python3.6替换掉python3.5，并将软链指向python3.6会有很多软件出问题比如nvidia-setting等，所以不建议更换，如果需要不同的环境可以使用pyenv，挺好用的。 添加python的PPA12sudo add-apt-repository ppa:deadsnakes/ppasudo apt-get update 安装python3.61sudo apg-get install -y python3.6 将python3.6置为python3的默认选项12sudo update-alternatives --install /usr/bin/python3 python /usr/bin/python3.6 5sudo update-alternatives --install /usr/bin/python3 python /usr/bin/python3.5 1 数字越大优先级越高，使用sudo update-alternatives --config python3可以切换。 安装常用python包安装pip和pip3并升级123sudo apt-get install -y python3-pip python-pip sudo python3 -m pip install pip --upgradesudo python -m pip install pip --upgrade 更换pypi源为清华的源12345mkdir ~/.config/pipcat &gt;&gt; ~/.config/pip/pip.conf &lt;&lt; EOF[global]index-url = https://pypi.tuna.tsinghua.edu.cn/simpleEOF 安装python3的各种常用包12345for i in 'xgboost' 'bs4' 'coursera-dl' 'jupyter' 'jupyterlab' 'ipython' 'keras' 'matplotlib' 'seaborn' 'pandas' 'pep8' 'pylint' 'pysc2' 'sklearn' dosudo pip3 install $isudo pip2 install $idone 依赖于CUDA9.0和cuDNN7.0的包安装tensorflow-gpu12sudo pip3 install tensorflow-gpusudo pip2 install tensorflow-gpu 安装pytorch安装pytorch0.4 cuda9.0 python3.5 版本，安装前需要去官网看下，进行相应的修改。 12sudo pip3 install http://download.pytorch.org/whl/cu90/torch-0.4.0-cp35-cp35m-linux_x86_64.whl sudo pip3 install torchvision 安装mxnet安装mxnet cuda9.0版本，运行前请去更新mxnet安装指导更新本段代码。 123sudo pip3 install mxnet-cu90sudo apt-get install graphvizsudo pip install graphviz 总结整个的脚本可以从这里下载。]]></content>
      <categories>
        <category>软件安装和使用</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>软件安装和使用</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在ubuntu16.04上安装nvida显卡驱动，CUDA9.0 和 cuDNN7.0]]></title>
    <url>%2F2018%2F06%2F15%2F2018-06-15-installCUDAcnDNN%2F</url>
    <content type="text"><![CDATA[使用PPA安装nvidia驱动添加nvidia驱动的PPA该PPA的详细信息 12sudo add-apt-repository ppa:graphics-driverssudo apt-get update 清除旧的安装1sudo apt-get purge nvidia* 禁用开源驱动nouveau首先查看是否已用了nouveau （不应该有输出） 1lsmod | grep nouveau 然后把 nouveau 驱动加入黑名单 12345678910sudo cat &gt;&gt; /etc/modprobe.d/blacklist-nouveau.conf &lt;&lt; EOFblacklist nouveaublacklist lbm-nouveauoptions nouveau modeset=0alias nouveau offalias lbm-nouveau offEOFsudo update-initramfs -usudo reboot 此时电脑会重启一次 安装驱动1234567# 首先需要关闭lightdm服务sudo service lightdm stop# 安装最新版本驱动，可以去上面那个PPA的网站查一下支持的最新版本sudo apt-get install nvidia-396# 更新模块sudo update-initramfs -usudo reboot 此时电脑会重启一次，重启后可以通过lsmod | grep nvidia来查看nvidia的模块有没有被正确载入，也可以通过nvidia-smi来显示显卡一些信息。 安装CUDA下载CUDA9.0并安装以下脚本会下载CUDA9.0的run文件和3个patch，并自动安装。注意在安装的时候询问是否要装驱动选no，有的资料说讯问是否要装openGL时选no，否则会循环登陆，不过我按的之后没有问我是否要装openGL。另外询问安装samples的位置的时候，将samples目录设置为/usr/local/cuda_samples，后面才可以编译一个sample测试一下是否正确安装。 1234567891011mkdir cuda9.0cd cuda9.0wget https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda_9.0.176_384.81_linux-runwget https://developer.nvidia.com/compute/cuda/9.0/Prod/patches/1/cuda_9.0.176.1_linux-runwget https://developer.nvidia.com/compute/cuda/9.0/Prod/patches/2/cuda_9.0.176.2_linux-runwget https://developer.nvidia.com/compute/cuda/9.0/Prod/patches/3/cuda_9.0.176.3_linux-runchmod +x *sudo ./cuda_9.0.176_384.81_linux-runsudo ./cuda_9.0.176.1_linux-runsudo ./cuda_9.0.176.2_linux-runsudo ./cuda_9.0.176.3_linux-run 设置全局环境变量123456sudo cat &gt;&gt; /etc/profile &lt;&lt;EOFCUDA_HOME="/usr/local/CUDA-9.0"PATH="$CUDA_HOME/bin:$PATH"LD_LIBRARY_PATH="$CUDA_HOME/lib64"EOFsource /etc/profile 如果只想对当前用户生效，将上面的内容加入到~/.bashrc中即可。 检查cuda是否安装好可以通过查看cuda版本nvcc --version来查看是否正确安装。不过需要先sudo apt-get install nvidia-cuda-toolkit安装这个包，不过这个包比较大 1G左右，可以不安装。更好的是编译一个sample来测试，还可以看到很详细的显卡的信息。 123cd /usr/local/cuda_samples/NVIDIA_CUDA-9.0_Samples/1_Utilities/deviceQuerysudo make./deviceQuery 安装cuDNN下载安装cuDNN的安装很简单，从官网下载下来之后复制到cuda的安装目录就可以了。不过cuDNN的下载需要先注册nvidia的会员，假设已下好并命名为本目录下cudnn.tgz。 123tar -xf cudnn.tgzsudo cp -R cuda/include/* /usr/local/cuda-9.0/includesudo cp -R cuda/lib64/* /usr/local/cuda-9.0/lib64 更新动态链接库1234sudo cat &gt;&gt; /etc/ld.so.conf.d/cuda.conf &lt;&lt;EOF/usr/local/cuda-9.0/lib64EOFsudo ldconfig -v 到此处就大功告成了，可以正常安装tensorflow-gpu，pytorch，maxnet了。 一些有用的命令 显卡设置 nvidia-settings 安装驱动有问题时可以试试自动载入nvidia的模块 sudo modprobe nvidia 总结最后，将整个代码放在这个文件中。]]></content>
      <categories>
        <category>软件安装和使用</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>软件安装和使用</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git使用中遇到的一些问题及解决]]></title>
    <url>%2F2018%2F06%2F15%2F2018-06-15-gitUsage%2F</url>
    <content type="text"><![CDATA[git的中文乱码问题12345678git config --global core.quotepath false # 显示 status 编码git config --global gui.encoding utf-8 # 图形界面编码git config --global i18n.commit.encoding utf-8 # 提交信息编码git config --global i18n.logoutputencoding utf-8 # 输出 log 编码cat &gt;&gt; ~/.bashrc &lt;&lt;EOFexport LESSCHARSET=utf-8EOF. ~/.bashrc 最后一条命令是因为git log默认使用less分页，所以需要bash对less命令进行 utf-8 编码。参考这个链接 在每次git Push 时不用重复输入密码有两种方法： 使用 SSH 方式进行推送，您需要配置 SSH 公钥后进行操作，详情请阅读 SSH 公钥配置文档需要注意的是，在~\.ssh下的公钥和私钥文件应使用id_rsa.pub和id_rsa文件命名，否则ssh不能自动识别。或者可以在配置文件/etc/ssh/ssh_config中显式的写出IdentityFile ~/.ssh/id_rsa。具体设置步骤为一、本地生成密钥对；ssh-keygen -t rsa -b 4096 -C &quot;your_email@youremail.com&quot;二、设置github上的公钥；网页中填写，填写完成后使用ssh -T git@github.com测试三、修改git的remote url为git协议 git remote -v更详细的介绍可以参见coding.net,git使用ssh密钥，和这个 对于 Https 协议: 首先在全局配置保存你的密码， ~/.git-credentials （没有就创建）内添加 https://{username}:{passwd}@github.com, 然后执行配置 Git 命令存储认证命令： git config --global credential.helper store执行后在 ~/.gitconfig 文件会多出下面配置项: credential.helper = store, 在后面添加--file ~/.git-credential 即可。详情请参考凭证存储 。]]></content>
      <categories>
        <category>软件安装和使用</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>软件安装和使用</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow变量初始化]]></title>
    <url>%2F2018%2F06%2F03%2F2018-06-03-tensorflow%E5%8F%98%E9%87%8F%E5%88%9D%E5%A7%8B%E5%8C%96%2F</url>
    <content type="text"><![CDATA[tensorflow变量初始化的方式使用tf.Variable()创建或tf.get_variable()创建，但初始化有所不同。如下图所示代码：1234567init = tf.truncated_normal([3, 1], stddev=0.1)va = tf.Variable(init)vb = tf.Variable(init)###############init = tf.random_normal_initializer(0., 0.3)w1 = tf.get_variable('w1', [3,], initializer=init)w2 = tf.get_variable('w2', [3,], initializer=init) 使用第一种方法创建出来的va和vb变量的值是相同的，使用第二种方法创建出来的变量w1和w2的值是不相同的。具体的这两种方法及还有没有其他的方法还需要再看下。]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DQN]]></title>
    <url>%2F2018%2F06%2F03%2F2018-06-03-DQN%2F</url>
    <content type="text"><![CDATA[DQN算法伪代码 Replay buffer一个很重要的问题是buffer中的样本不是独立同分布的。随机抽样是为了解决样本之间不独立的问题，打破样本之间的相关性。其实更准确的说应该是为了是每一个batch都是在state分布上的平均抽样才能达到比较好的效果，要不然神经网络可能会为了适应某个样本调偏了原来的。（问题在与处理数据偏斜的意义何在？神经网络其实不管样本的分布，他只管准确的表达出某个样本对应的情况，其实是认为所有样本是均匀分布的，所以得把不均匀分布的样本处理成均匀分布的样本） Target Network为什么要用Target Network？两个网络能不能共享一部分参数？ 不能, 就是为了没有同样参数才设置了Target Network]]></content>
      <categories>
        <category>RL</category>
      </categories>
      <tags>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ssh -L端口映射]]></title>
    <url>%2F2018%2F03%2F30%2F2018-03-30-ssh%E7%AB%AF%E5%8F%A3%E6%98%A0%E5%B0%84%2F</url>
    <content type="text"><![CDATA[ssh的一些参数含义 -1：强制使用ssh协议版本1； -2：强制使用ssh协议版本2； -4：强制使用IPv4地址； -6：强制使用IPv6地址； -A：开启认证代理连接转发功能； -a：关闭认证代理连接转发功能； -b：使用本机指定地址作为对应连接的源ip地址； -C：请求压缩所有数据； -F：指定ssh指令的配置文件； -f：后台执行ssh指令； -g：允许远程主机连接主机的转发端口； -i：指定身份文件； -l：指定连接远程服务器登录用户名； -N：不执行远程指令； -o：指定配置选项； -p：指定远程服务器上的端口； -q：静默模式； -X：开启X11转发功能； -x：关闭X11转发功能； -y：开启信任X11转发功能。 /etc/ssh/ssh_config 是客户端配置文件/ect/ssh/sshd_config 是服务器端配置文件 -L端口转发这次只用到了-L选项，对这个搞得比较清楚了，所以详细总结一下这个选项。本文从这篇blog中学到很多，最终总结也只是其中一部分，还盗用了两个图，如有侵权，告知即删。 首先直接看下man中的用法说明： -L [bind_address:]port:host:hostport-L [bind_address:]port:remote_socket-L local_socket:host:hostport-L local_socket:remote_socket Specifies that connections to the given TCP port or Unix socket on the local (client) host are to be forwarded to the given host and port, or Unix socket, on the remote side. This works by allocating a socket to listen to either a TCP port on the local side, optionally bound to the specified bind_address, or to a Unix socket. Whenever a connection is made to the local port or socket, the connection is forwarded over the secure channel, and a connection is made to either host port hostport, or the Unix socket remote_socket, from the remote machine. Port forwardings can also be specified in the configuration file. Only the superuser can forward privileged ports. IPv6 addresses can be specified by enclosing the address in square brackets. By default, the local port is bound in accordance with the GatewayPorts setting. However, an explicit bind_address may be used to bind the connection to a specific address. The bind_address of “localhost” indicates that the listening port be bound for local use only, while an empty address or ‘*’ indicates that the port should be available from all interfaces. (其实这里面说的非常清楚了，不光把我这次踩得坑直接点出来了，还说了很多不知道的用法，所以以后遇到这种问题，先看官方的文档，解决不了再去找，要不然实在太浪费时间了。) 功能简单的说就是把远程主机的端口映射到本地主机上的某个端口。从上面的用法中可以发现几个值得注意的地方 两台主机网络拓扑如下图所示： 主机A想要访问主机B上的某个服务，但不想输入B的地址或者是将localhost硬编码在code中，那只能借助]]></content>
      <categories>
        <category>软件安装和使用</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>软件安装和使用</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy、python random、tensorflow随机种子设置]]></title>
    <url>%2F2018%2F03%2F30%2F2018-03-30-numpy%E9%9A%8F%E6%9C%BA%E7%A7%8D%E5%AD%90%2F</url>
    <content type="text"><![CDATA[numpy、python randomnumpy可以使用np.random.seed(2)来设置初始化种子 每次设置seed之后都会重新初始化随机数发生器，计数器会置零 设置seed是全局的，不同文件也是通用的 python内置的random也是一样的 tensorflowtf的random seed分为两个层次，graph level 和 operation level 都不设置—&gt; 每次都不一样 只设置operation的random seed，对于每个Session，operation random会重新初始化，结果是可复现的。 只设置graph-level，即使用tf.set_random_seed()设置，对于每个Session，random状态也会初始化，同时operation如果没有特别指定的话，由系统自动根据graph level的seed指定，不同Session之间也是可复现的。 同时指定graph level 和operation level的random seed，也是可复现的，不过和只指定其中一个生成的随机数是不一样。 总的来说，对于某个operation，任意指定graph level 或operation level中的random seed，生成的随机数就是确定的，但生成的随机数列是不一样的。特别注意：op声明的时候就应该已经设置了graph level的random seedtf.set_random_seed(1)，否则对于该op相当于没有设置random seed。例如下面的代码：123456789101112131415161718import tensorflow as tfa = tf.truncated_normal([3], stddev=0.1))#相当于op c没有设置random seedtf.set_random_seed(1)a = tf.truncated_normal([3], stddev=0.1)#相当于op a的random seed为1+hash(a)tf.set_random_seed(2)b = tf.truncated_normal([3], stddev=0.1)#相当于op b的random seed为2+hash(b)print("Session 1")with tf.Session() as sess1: print(sess1.run(a)) print(sess1.run(b)) print(sess1.run(c))print("Session 2")with tf.Session() as sess2: print(sess2.run(a)) print(sess2.run(b)) print(sess2.run(c))]]></content>
      <categories>
        <category>类库</category>
        <category>numpy</category>
      </categories>
      <tags>
        <tag>numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[把blog从windows上迁移到Ubuntu]]></title>
    <url>%2F2018%2F03%2F30%2F2017-11-16-nodejs%2F</url>
    <content type="text"><![CDATA[Ubuntu上安装Node.js使用包管理安装Node.js本blog系统使用了hexo的方案，hexo是基于Node.js开发的，所以需要先在ubuntu上安装Node.js，安装源里面的非常简单，只要两句话就可以了。12sudo apt-get install nodejssudo apt-get install npm 这样安装了之后有几个问题，第一就是这两个版本都太老了，后面有些包会弹Warning，所以需要先升级。 升级可以使用这里面说的方法。 npm中有一个模块叫做“n”，专门用来管理node.js版本的。更新到最新的稳定版只需要在命令行中打下如下代码： npm install -g n n stablen后面也可以跟具体的版本号：n v6.2.0npm升级npm -g install npm@next 总结起来就是123npm install -g nn stablenpm -g install npm@next 但这样升级只有有点小问题，就是系统中同时存在nodejs和node，并且版本还不一样，之前的时候是没有node的，不过也不影响使用，所以也没管它。 另外，由于npm的源比较慢，所以可以使用淘宝提供的进行代替，npm install -g cnpm --registry=https://registry.npm.taobao.org这样就可以了。之后再安装东西就是cnpm install xxx。 使用二进制包安装node.js又重装了系统，所有又要重新配置一下，还忘记上次写了这个笔记的，直接从官网下载了linux上的二进制包，折腾了下才弄好。 其实这个help写的已经很清楚了，总结一下就是： 下载包，解压，放到相放的地方 添加环境变量，加入PATH中。即在~/.profile末尾添加 123# Nodejsexport NODEJS_HOME=/usr/local/lib/nodejs/node-v8.9.4/binexport PATH=$NODEJS_HOME/bin:$PATH 然后. ~/.profile使其生效。 测试是否正确安装node -v npm version 安装hexo按照官方文档里的安装就可以了，其实就一句话：1cnpm install -g hexo-cli 我记的当时还装了一个用来提供索引的包，忘记叫什么了。但在我的blog目录下，有一个叫做package.json的文件，印象中是用来记录依赖关系的，所以还在该目录下运行了npm install . -g，然后hexo s就可以啦！索引也可以正常使用。]]></content>
      <categories>
        <category>软件安装和使用</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>软件安装和使用</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[英语积累]]></title>
    <url>%2F2018%2F02%2F02%2F2018-02-02-english%2F</url>
    <content type="text"><![CDATA[Deterministic Policy Gradient (DPG) and Deep DPG (DDPG) are widely applied policy gradient methods in single-agent reinforcement learning. cz: 是不是应该是are applied in? qc: 原文没有问题。这里不涉及apply的搭配问题，因为不是做的动词，而是形容词。 Nevertheless, we ﬁnd that simple MLP networks cannot give full play to the centralized critic. cz: give a full play to? qc: 原文没有问题。不用加“a” The centralized critic takes as input the observations and actions of all agents. cz: as input应该放在句末吧？ qc: 原文没有问题。如果宾语（the observations and actions of all agents）太长，可以考虑把“as input”放在前面。 In this paper, we enhance the centralized critic with moresophisticated and interpretable network structures. qc: 这里不用修改。提示一下，在论文中指称自己的论文或者本文的作者，可以用“present”这个单词。比如：本文：the present paper; 在本文中：in the present paper; 本文作者：the present author/ the present resercher. we propose an attention critic, resulting in the ATT-MADDPG model as shown in Figure 2. cz: result in是导致的意思吧？ 原文没有问题。result in 愿意是”..的结果是。。“可以灵活使用，既可以表示因果关系，也可以表示某个过程的产出 Although it is a critical component in those models, current implementations only use simple MLP networks, which cannot give full play to the important critic. cz: Although好像没啥用，可以去掉？ qc: 原文没有问题。在语义上表示让步，在语法上有连接作用。 the training environment (ENV ) can be seen as stationary in spite of the changing policies of other agents. cz: 是否应该用名词？ qc: 原文没有问题。可以用形容词。 We refer the readers to the original papers (especially MADDPG) for details. cz: 可以这么用么？ qc: 可以这样用。 To make the illustration easy to read cz: 被动？ qc: 原文没有问题。be+品质形容词+to do可以表达被动意义。]]></content>
      <categories>
        <category>英语</category>
      </categories>
      <tags>
        <tag>英语</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DB课堂总结]]></title>
    <url>%2F2018%2F01%2F01%2F2018-01-01-DB%2F</url>
    <content type="text"><![CDATA[存储和文件结构存储磁盘时间寻道时间**+旋转延迟+传输时间 SSD写SSD的时间比读SSD的时间大很多 文件组织固定长的record变长recordSlotted Page Structurerecord的组织形式 heap sequentialoverflow block hashing multitable clustering file organization Lec3 索引Ordered IndicesPrimary index (clustering index)Secondary index (non-clustering index)ISAM(Indexed Sequential Access Method)静态结构，leaf节点按照顺序在磁盘中中顺序存放 B+ Tree 所有叶子结点的深度相同 除根节点和叶子结点外有|n/2|到n个孩子 叶子节点有|(n-1)/2| ~ n-1个值如果根节点不是也自己点，至少有两个孩子，如果根节点是叶子节点，有0~n-1个值叶子结点到第一层内部节点，是Copy Up内部节点的调整，是Push UpHash动态Dynamic Hash地址表不断扩大hash值的位数，地址表二倍的增长Linear Hashing Lec4 操作实现OverviewCost计算： 忽略CPU代价 最终结果的写代价不算，操作符号实现过程中的输出是算的，实现结束后，有可能是流水线，就不算。 忽略对index块的读取代价表示： B(R):R关系占用的块数 T(R):R中的元组个数 V(R,A):R中属性A的值的个数 M:可用的内存块数Cluster概念辨析： Clustered file Organization：不同关系的元组可以被放在一起存储 Clustered relation：每个块只存储相同关系的元组 Clustering index：按照index来顺序存储要处理的操作 选择selection 投影projection 连接join 集合求差 set-difference union集合求并 聚合函数 aggregation（group by sum min等） 排序 sort 去重 duplicate eliminationOne Pass scanOne Block in Memory一个一个读进来处理了再丢出去，做选择、投影、集合并（bag多重结果聚合）都可以Cost=B(R)Multiple Blocks in Memory 去重 Cost=B(R) 某一个元组可以放入主存，set union（set 单重集合）、difference、intersection、prodectCost=B(R)+B(S)、M=Min(B(R),B(S))+2Loop两个元组都无法放入主存，join，set操作 tuple based Nested loop join占用两块bufferCost=T(s)+T(R)*T(S) page oriented nested loop join占用两块bufferCost=B(s)+B(R)*B(S) block nested loop join占满bufferCost= B(s)+B(s)/M*B(R) s在外层循环，r在内层循环，所以小的放外层有点优化Sort 单纯的二路归并排序：Cost=2N(|log_2 N|+1) 最内层使用其他排序，可以利用所有的内存块，然后进行多路归并Cost=2N*(|log_{B-1}|N/B||+1) 计算了最后的输出 2-Phase Sort限制|log_{B-1}|N/B||为1，即N&lt;=(B-1)*B时。此时代价为3B(R)（忽略了最后输出的代价） Sort JoinCost: 5(B(R)+B(S)) ;sort用4(B(R)+B(s))（包括最后输出），最后join使用类似归并排序的方法，两个关系只扫描一次,需要sqrt(max(B(R),B(S))) 取消sort结果的输出，直接对很多个小的有序列表进行merge join则效率提高为3(B(R)+B(S)),需要M&gt;sqrt(B(R)+B(s)) 集合并交差、去重操作与上面的类似。HashHash join Cost=3(B(R)+B(S))所需buffer B(R)/(M-1)sqrt(min(B(R),B(S)))比sortjoin需要的buffer少Hash Join和Sort Join比较 有充足buffer的时候，hash join和sort join的cost 都是3(B(R)+B(S))，但此时所需的buffer数目是不一致的， sqrt(min(B(R), B(S)) &lt; sqrt(B(R) + B(S)) hash可以高度并行 hash的性能依赖于hash函数的质量 sort join可以应用于非等值连接 如果已经有序了sort join更有利 sort join的结果也是有序的 sort join对数据偏斜不敏感Indexselection的代价是T(R)/V(R,X)或B(R)/V(R,X)index join的代价是B(R)+T(R)T(S)/V(S,X)或B(R)+T(R)B(S)/V(S,X)看S是不是clusted的Lec5 查询优化启发式规则： 尽早做断言 尽可能避免笛卡儿积 尽早做投影 把子查询转化为join操作 使用左深树Lec7 并发控制基于锁的协议两种锁 排他锁X（exclusive）：可以应用与读写 共享锁S（shared）：只能应用与读操作锁的相容性两阶段锁协议 Phase 1：Growing Phase 不断获得锁 Phase 2：Shrinking Phase 不断释放锁可以按照Lock point（获取最后一个点的时间）串行化能保证可串行化，但可能导致级联回滚，无法避免死锁strict 两阶段锁x锁必须在commit/abort的时候才能释放rigorous 两阶段锁s，x锁都必须在commit/abort的时候才能释放锁提升协议P1. 获取s，x锁或s锁提升为x锁P2. 释放s，x锁或者讲x锁降低为s锁允许更多的并发事物，实现简单锁机制的实现所表，使用数据项的hash来索引，并维护每个事务所拥有的锁的列表Increment Lock自增自减锁即两个自增自减操作不区分先后基于图的锁协议将先后关系表示为一个有向无环图Tree 协议 只允许x锁 只能获取已拥有的锁的数据的子孙节点的锁，如果该事物还没有锁，则可以获取任意没有锁的数据 可以在任何时间释放锁 释放锁之后该事物无法再获取这个数据的锁优点：可以保证冲突可串行化，并且没有死锁，可以更早的释放锁缺点：不能保证可恢复性和不级联回滚，可能需要获取他们不会访问数据的锁多粒度Intention锁模型IS（Intention Shared）：意向共享锁IX：（Intention Exclusive）SIX（shared and intention exclusive）死锁处理死锁预防 一次获取所有资源 wait-die 老的抢年轻的 wound-wait 年轻的抢老的 Timeout，没有了死锁，但可能被饿死死锁检测等待图有了环则出现了死锁死锁恢复乐观协议基于验证的协议 执行并写在临时文件上Start 检验Validation 写入或者回滚Finish 机遇时间戳的协议多版本协议Lec9 并行和分布式数据库Speedup和Scaleup限制Speedup和Scaleup的因素 并行数据库结构 shared memory数据共享很快，但可扩展性不高，总线成为瓶颈 shared disk连接到磁盘系统的传输网络成为瓶颈 shared nothing HierarchicalIO并行Range、Hash、Round robin Partition主要的三个功能 扫描全表 等值查询 range查询：处理数据偏斜，排序或者构造直方图（可以抽样）操作并行选择排序连接：Fragment and Replicate join (n vs 1 ; n vs m)实际上就是用硬件实现循环group by 和聚集函数去重投影plan并行和优化 Inter query Intra Query Intra operation Inter operation分布式SemiJoin主拷贝Primary Copy 设定一个Primary site基于时间戳的并发控制可以应用于分布式系统，全局唯一的时间戳的实现将成为一个问题。使用local unique timestamp+site identifier构成global unique identifierlocal unique timestamp随接本地时钟和收到的事物的时间戳确定两阶段提交协议2 PC阻塞： coordinator挂了]]></content>
      <categories>
        <category>课程</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ML课程总结]]></title>
    <url>%2F2017%2F12%2F24%2F2017-12-24-MLsummary%2F</url>
    <content type="text"><![CDATA[强化学习bellman方程V_T^\pi(x) = \sum_{a \in A} \pi(x,a) \sum_{x' \in X} (\frac{1}{T}R_{s,s'}^{a}+\frac{T-1}{T}P_{s,s'}^aV_{T-1}^\pi(x')Q_T^\* = \sum_{x' \in X} (\frac{1}{T}R_{s,s'}^{a}+\frac{T-1}{T}P_{s,s'}^aV_{T-1}^\pi(x')bellman最优方程V_T^\pi(x)(s) = \max_{a \in A} \sum_{x' \in X} (\frac{1}{T}R_{s,s'}^{a}+\frac{T-1}{T}P_{s,s'}^aV_{T-1}^\pi(x') \\ =\max_{a \in A}Q^\*_{T-1}(x,a) Q_T^\*(x,a) = \sum_{x' \in X} (\frac{1}{T}R_{s,s'}^{a}+\frac{T-1}{T}P_{s,s'}^a' \max_{a' \in A}Q^\*_{T-1}(x,a')]]></content>
      <categories>
        <category>课程</category>
        <category>ML</category>
      </categories>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MultiBoosting A Technique for Combining Boosting and Wagging 阅读笔记]]></title>
    <url>%2F2017%2F11%2F28%2F2017-11-28-MultiBoosting%2F</url>
    <content type="text"><![CDATA[AbstractIntroductionAdaBoost and baggingbagging在二项分布的伯努利试验中，如果试验次数$n$很大，二项分布的概率$p$很小，且乘积$λ= np$比较适中，则事件出现的次数的概率可以用泊松分布来逼近。事实上，二项分布可以看作泊松分布在离散时间上的对应物。有放回取样，形成的概率分布是离散泊松分布 wagging将有放回取样改为分布不同权重，以利用所有的训练样本。权重是连续泊松分布的随机变量，然后再将权重的和标准化为n。 Adaboost调整权值比按比例取样更有效。 Bias and varianceBias和Variance分解对分类任务不很容易。在x相同则y相同的假设下，noise为0。 Previous bias/variance analyses of decision committee performanceBagging可以降低varance，可以认为是因为bagging本身就是取的很多个小模型的均值。因此，不能降低Bias也就可以理解了。AdaBoost可以认为是一个逻辑回归的过程，只降低bias，但实际上AdaBoost既可以降Varance，又可以降Bias，至于为什么，没有一个一致的结论。 Alternative accounts of decision committee performanceMultiBoostingbagging主要降低variance，AdaBoost既降低variance也降低bias（论文），主要关注降低bias（周志华西瓜书）。bagging在降低variance方面比AdaBoost更有效。most of the effect of each approach is obtained by the first few committee members bagging无法使用所有的训练样本来训练，所以采用了wagging。bagging可以并行，AdaBoost必须串行。 算法流程输入：$S$：有$m$个样本的训练集BaseLearn：基学习算法$T$：迭代次数$I_i$：第$i$次迭代中，bagging的基学习器的个数 算法流程：1234S&apos;=S，权重都为1k=1for t = 1 to T: if I EvaluationAdaboost和bagging算法性能的提升在前几次迭代中就完成了，后面的迭代提升很小。总共进行100次迭代，大致分成10段，每次boosting的基准被10，可能提前结束，也可能推迟结束。最后将训练的着100次进行加权，整个过程是Boosting的过程，但中间有重新开始的动作，所以也是几个Adaboost组成了一个bagging。在bagging内部，每个Adaboost的权重是相同的。 Data setsStatistics employedError ratesWagging and bagging restarts in AdaBoostBias/variance analysis of performanceComparison of MultiBoost t = 100 against AdaBoost t = 10On the optimal number of sub-committeesThe application of bagging and wagging in MultiBoostSummaryAppendix A: Bias and variance measures for classification learningAppendix B: A bias/variance estimation methodAppendix C: Detailed results]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>ML</category>
      </categories>
      <tags>
        <tag>论文阅读笔记</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Using Iterated Bagging to Debias Regressions 阅读笔记]]></title>
    <url>%2F2017%2F11%2F28%2F2017-11-28-IteratedBagging%2F</url>
    <content type="text"><![CDATA[Abstractbagging 可以显著的降低variance，但无法降低bias。第一步bagging，然后进行boosting。实验使用了tree和nearest neighbor regression来测试。 Introduction回归问题的泛化误差=随机噪声+预测bias+预测variance通常决策树越小，bias越大。 The basic ideaIterated bagging-more details令$y=f(x)+\epsilon$，其中$\epsilon$表示固有的均值为0的噪声，则 y_n'=\epsilon+f(x_n)-f_R(x_n,T)即表示预测的偏差bias，但是在$f_R(x_n,T)$有较大方差的时候，就会对结果造成干扰，就无法表示预测的bias了。所以应该找一个低方差的回归算法。但还有一个问题，应该使用一个残差的无偏估计，这里作者认为留一验证所得的残差是无偏的，而使用bagging中的袋外样本进行的对残差的估计恰好是无偏的。所以算法框架为： 在每个阶段首先执行bagging，即重复训练$K$个predictor，在该阶段使用的$y$记作$y^i$,可以发现$y^1$即为最开始的y值。记第$k$个predictor的输出为$\hat{y}_{n,k}$为第$k$个predictor对第$n$个样本点的输出。记作$f_R^i(x,T)$ 生成下一阶段的新训练集$T_{(i)}$，$x$不变，$y$使用$y^{i+1}=y^{i}-avg_k\{\hat{y}_k\}$计算得到，$avg_k$的含义是不适用该样本点作为训练集的predictor预测的结果的平均。当某个阶段的MSE大于以前所有阶段MSE的1.1倍时，则停止， Bias-variance results on synthetic dataRecalling the bias-variance decomposition 偏差方差分解偏差方差分解：predictor的误差可以表示为： PE^* = E(\epsilon^2)+E(f(x)-\overline{f}(x))^2+E(f_R(x)-\overline{f}(x))^2其中，$f(x)$是真实值，$\overline{f}(x)$ 是预测值的均值，$f_R(x)$是预测值。第一项表示噪声，第二项表示bias，第三项表示variance。 Breiman证明了如果在独立的训练集上做无限次取样，则最终的方差为0，但偏差不变。 Bias and variance for synthetic dataEmpirical resultsComparison to SVRMsSome heuristic theory 一些启发式理论Nearest neighbor bagging and debiasingUsing small trees as predictorsDebiasing applied to classificationConnection with Friedman’s workDiscussion]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>ML</category>
      </categories>
      <tags>
        <tag>论文阅读笔记</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[中国文化的现代化与世界化]]></title>
    <url>%2F2017%2F11%2F27%2F2017-11-27-%E4%B8%AD%E5%9B%BD%E6%96%87%E5%8C%96%E7%9A%84%E7%8E%B0%E4%BB%A3%E5%8C%96%E4%B8%8E%E4%B8%96%E7%95%8C%E5%8C%96%2F</url>
    <content type="text"><![CDATA[从中国哲学论中国五千年文化独特之价值研究中国文化独特价值之价值中国文化独特的价值简述天人合德的宇宙本体哲学中外何用的理想政治哲学诚明合能的人身修炼哲学知行合体的社会实践哲学结论中国哲学与中国文化哲学与文化的界说及两者的关系中国哲学的优良传统中国儒家哲学的特质中国文化的特质中国人很讲究人格伦理价值的充实和发扬中国整个社会要求沟通个人的人格和家庭的价值爱好和平与忍耐的美德伦理、艺术与宗教的会通中国文化遭遇的危机 列强暴力的侵略。现在我们要避免的不是外力的压迫，而是内在文化创造或接受外来文化时所带有的自卑崇洋心理。 另一种认为西方文化毫无价值，而中国样样都好，要关起门来，不肯挖掘中国文化的现代意义，不将中国文化创造的意义和理想实现出来。各位要知道中国文化并未完成，过去的成就只是开始，只是一个未完成的作品而已。了解这点，我们就不会固步自封，麻木不仁，中国文化才能开放地自行地去创造，并要求理想的实现。 理想和实际的分离。如何振兴中国文化 加强研讨中国文化的兴趣 培养科学思考的习惯 建立伦理生活的标准 发挥仁民爱物的精神知识分子对社会与文化的责任 中国哲学的特性西洋早期研究中国哲学的错误先儒家时期的原型观念儒家的传统道家的传统中国佛学的传统在现代中国的马克思主义中国哲学的四个特性作为内在的人文主义的中国哲学作为具体理性主义的中国哲学作为生机的自然主义之中国哲学作为自我修养是实效主义的中国哲学从本体诠释学看中西文化异同左和右，激进和保守，由于其本身的片面性，不可避免的要走向各自的反面。我们在说左和右的时候，实际上假定了一个没有说出来的前提，即有一个没有说出来的历史中心。革命政党的任务是试图从社会的边缘进入社会的中心，这不可能是永恒的工作中信。因为当进入中心以后，立场变了，立足点变了，任何一方的偏执都是不利的。全球化全球世界主义秩序不受约束的资本主义市场依然有许多马克思提出的破坏性后果，包括居于主导地位的增长的伦理，普遍的商品化以及经济上的两极分化。 修复破坏了的团结, 即重构个人生活与集体⽣活的关系, 修复呗极端利己主义破坏了的社会关系和家庭关系; 从左翼和自由主义的解放政治转入吉登斯提出的生活政治, 关注⼈类在⼀个开放地全球化的世界中如何生活的问题 结合反思社会的出现,推行积极的新人的信任，提倡⼀种能动性政治, 使个人与集体、国家与公民建立起积极的信任关系, 以解决贫困与社会排斥的问题。 在全球化和反思的社会秩序中,克服自由民主制度的缺陷, 建立一种对话⺠主的社会制度; 为建立一种积极的和反思的福利国家做好准备, 井将其与解决全球贫围联系起。 通过对话决包括战争、 价值冲突和性暴⼒等在内的吾种暴力问题。 局限性： 没有从经济层面去了解社会主义的问题 吉登斯对于中国改革成就和社会主义理论和实践的重要意义缺乏深入了解。 对资本主义和社会主义发展中的矛盾做出了错误的理解。 吉登斯不仅从一个社会学家特有的角度密切关注当代西方社会政治生活的变化，而且在批判的同时，积极提供现实的解决路径。 吉登斯的学术路径：第一阶段： 分析和梳理欧洲传统思想，特别是三大古典社会理论家即马克思、韦伯记忆迪尔凯姆的著作，开始逐步创造自己的分析概念和分析框架。第二阶段： 运用自己的理论来分析社会历史以及当代西方社会生活。 二元困境和结构化新社会运动中的两大流派： 以法兰克福学派为代表的批判理论，强烈的悲观主义 经验主义和实证主义，固守价值中立，逐步改造 这种二元对立对于人们认识纷繁的世界是方便的，但由于这种便利性，也容易简单化，使整个社会陷入非彼即此的逻辑中，为了理论上的逻辑性，夸大个别因素的解释能力，造成方法论之间的排斥。 吉登斯对主题理解力的分析实际上反驳了启蒙运动以来形成的进步/落后，文明/野蛮，现代/传统的二分法定式，强调了社会的连续性以及主题的能动性。时空关系，我们在考察社会系统、社会互动、社会转变时必须把他们定位在时空关系中。时空延伸、时空交界、跨社会系统 公民社会的复兴实际上是危险的而不是解放的。因为它可能会促进原教旨主义的高涨，与增长的潜在暴力结合在一起。福利国家总是民主国家，而且这种联系绝不是巧合。促进福利制度发展的主要因素之一是当局促进国家稳定的愿望国家内部和解的时代也是战争产业化的时代：随着武器的机械化和大规模生产，战争的本质也改变了。]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用rsync从windows同步数据到ubuntu]]></title>
    <url>%2F2017%2F11%2F13%2F2017-11-13-rsync%2F</url>
    <content type="text"><![CDATA[rsync简介 rsync命令是一个远程数据同步工具，可通过LAN/WAN快速同步多台主机间的文件。rsync使用所谓的“rsync算法”来使本地和远程两个主机之间的文件达到同步，这个算法只传送两个文件的不同部分，而不是每次都整份传送，因此速度相当快。 rsync是一个功能非常强大的工具，其命令也有很多功能特色选项，我们下面就对它的选项一一进行分析说明。 需求描述由于之前的移动硬盘挂了，里面还有大量资料没有备份，虽然说里面的资料都不是很要紧，但还是有点心痛。感慨移动硬盘太脆弱了，就是读写的时候从5厘米的平台上掉下来了然后就挂了，遂产生定期自动多盘备份的想法。实验室主机有一块500G的硬盘基本上是空的，可以用作备份盘。主机上装的是Ubuntu，主力机是windows 10，之前折腾过samba，想要将ubuntu的文件夹直接映射成windows的网络磁盘，但一直不行，这次使用rsync的方案进行备份和传文件。所以总的方案是，ubuntu作为server，windows作为clinet。不错也找到了windows作为server的方案，本次没有用到就没实验，如果需要可以参考这个、这个，看起来很详细。 ubuntu的server端配置主要是配置两个文件rsyncd.conf和rsyncd.secrets，这两个文件并不是初始建好的，需要自己找个位置建，启动的之后指定位置即可，在本例中，我将其放在了/etc/rsyncd/中。 rsyncd.conf文件网上有很多版本，不过我试了几个都有问题，最后这个是我实际可以的版本。12345678910111213141516171819uid = chaogid = chaomax connections = 4use chroot = nohosts allow = *[mbackup]path = /home/chao/hdd/mbackupread only = falselist = trueauth users = rsyncsecrets file = /etc/rsyncd/rsyncd.secrets[userdata]path = /home/chao/hdd/chaoread only = falselist = trueauth users = rsyncsecrets file = /etc/rsyncd/rsyncd.secrets 前面的是全局设置，后面的每个方括号代表一个同步节点。需要注意的是，这里面的auth users不要和系统中的用户名相同，否则会提示认证错误。全局配置中的uid和gid是说的使用系统中的哪个组的哪个用户。（怀疑之前samba配置有问题可能也是因为这个问题） rsyncd.secrets文件本文件保存rsync使用的用户名和密码，使用明文，用户名:密码的形式，但要记住需要把权限改为600。 运行服务如果需要开机运行，需要将下面命令添加到rc.local中。rsync --daemon --config=/etc/rsyncd/rsyncd.conf windows端的clinet端配置先去这里下载一个cwRsync，下载后解压把bin放到path里面即可。需要注意的是，rsync可以使用--password-file指定存储了密码的文件，但要求该文件的读写属性为600，windows的文件权限没法设置啊，cycwin中的chmod设置了也没用（或许是我的操作有问题，并未深究）。另一种解决方案是设置一个名为SET RSYNC_PASSWORD的环境变量，如果在bat里面，可以SET RSYNC_PASSWORD=xxxx这样写。 注意事项 本地路径如果是上传整个文件夹的，本地路径最后不加/；如本地路径写为abc，此时，服务器上会在根目录下创建一个abc文件夹，并且不会删除本节点根目录下的其他文件夹；如果写为abc/则是讲本地abc目录与服务器的本节点根目录完全匹配，会删掉其他文件，当然是在加了--delete之后。 使用--exclude-from=只能在最开始的时候使用，无法删除已有的文件和目录，即使带了--delete选项。--include-from=这个选项好像木有作用。同时，使用--include-from=和--exclude-from=后面跟的文件和目录列表使用相对路径，和上一条对应，如果本地路径为一个目录名不带/，例如abc，那么列表里就要写成abc/a，如果带/，则列表里写成a。 如果不带--delete参数，则执行的是增量式的备份，新文件上传，有修改也上传，修改文件名也上传，但本地删除的服务器端不删除。 更多命令，可以参考这个网站]]></content>
      <categories>
        <category>软件安装和使用</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>软件安装和使用</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SAMBA介绍和配置]]></title>
    <url>%2F2017%2F10%2F15%2F2017-10-15-samba%2F</url>
    <content type="text"><![CDATA[历史和由来 windows上网络共享文件系统叫做Common Internet File System, CIFS;linux上有 Network File System, NFS; 两者进行转化产生了Server Message Block (SMB)，即现在要用的samba。 NFS架构在RPC Server上，samba使用NetBIOS(Network Basic Input/Output System)协议，原生的NetBIOS用于局域网，无法跨越路由，但有NetBIOS over TCP/IP技术可以让他跨越路由。Windows上有NetBEUI（NetBIOS Extened User Interface），是NetBIOS的改良版本。 SAMBA 使用的 daemonsSAMBA使用两个服务： nmbd ：这个 daemon 是用来管理工作组啦、NetBIOS name 啦等等的解析。 主要利用 UDP 协议开启 port 137, 138 来负责名称解析的任务； smbd ：这个 daemon 的主要功能就是用来管理 SAMBA 主机分享的目录、档案与打印机等等。 主要利用可靠的 TCP 协议来传输数据，开放的端口为 139 及 445(不一定存在) 。 联机模式的介绍 (peer/peer, domain model) peer/peer (Workgroup model)：所有 PC 均可以在自己的计算机上面管理自己的账号与密码，同时每一部计算机也都具有独力可以执行各项软件的能力 domain model：将所有的账号与密码都放置在一部主控计算机 (Primary Domain Controller, PDC) 上面。 SAMBA 服务器的基础设定Samba 所需套件及其套件结构组件 samba：这个套件主要包含了 SAMBA 的主要 daemon 档案 (smbd 及 nmbd)、 SAMBA 的文件档 (document)、以及其他与 SAMBA 相关的 logrotate 配置文件及开机默认选项档案等； samba-common：这个套件则主要提供了 SAMBA 的主要配置文件 (smb.conf) 、 smb.conf 语法检验的测试程序 (testparm)等等； samba-client：这个套件则提供了当 Linux 做为 SAMBA Client 端时，所需要的工具指令，例如挂载 SAMBA 文件格式的执行档 smbmount 等等。 相关文件 /etc/samba/smb.conf：smb.conf 是 Samba 的主要配置文件名，如果你的 distribution 的 SAMBA 配置文件不在 /etc/samba/ 目录当中， 那么你应该使用 locate 或 find 等方式将他找出来就好啦。基本上，咱们的 Samba 就仅有这个配置文件而已， 且这个配置文件本身就是很详细的说明文件了，请用 vi 去查阅他吧！这个档案主要在设定工作组、 NetBIOS 名称以及分享的目录等相关设定，我们后续要介绍的都是这个档案而已啦！ /etc/samba/lmhosts：这个档案的主要目的在对应 NetBIOS name 与该主机名的 IP ，事实上他有点像是 /etc/hosts 的功能！只不过这个 lmhosts 对应的主机名是 NetBIOS name 喔！不要跟 /etc/hosts 搞混了！由于目前 SAMBA 的功能越来越强大，所以通常只要您一启动 SAMBA 时，他就能自己捉到 LAN 里面的相关计算机的 NetBIOS name 对应 IP 的信息，因此这个档案通常可以不用设定了！ /etc/samba/smbpasswd：这个档案预设并不存在啦！他是 SAMBA 默认的用户密码对应表。当我们设定的 SAMBA 服务器是较为严密的，需要用户输入账号与密码后才能登入的状态时，用户的密码默认就是放置在这里咯 (当然啰，您可以自行在 smb.conf 里面设定密码放置的地方及密码文件名， 不过我们这里都以预设的状态来说明) 。比较需要注意的是，这个档案因为包含了用户的密码， 当然权限方面要较为注意啦！这个档案的拥有者需要是 root ，且权限设定为 600 才行喔！ /etc/samba/smbusers：由于 Windows 与 Unix-like 在管理员与访客的账号名称不一致，分别为 administrator 及 root， 为了对应这两者之间的账号关系，可以使用这个档案来设定。不过这个档案的使用必须要经由 smb.conf 内的『 username map 』设定项目来启动才行。 testparm：这个指令主要在检验 samba 配置文件 smb.conf 的语法正确与否，当你编辑过 smb.conf 时，请务必使用这个指令来检查一次， 避免因为打字错误引起的困扰啊！ smbd, nmbd：前几个小节曾经提过的两个主要 daemon 就在这里！ smbstatus：列出目前的 SMB server 的状态，也是很有用途的一个指令啦！ smbpasswd：如果您的 SAMBA 设定的较为严格，需要规定用户的账号与密码，那么那个密码档案的建立就需要使用 smbpasswd 来建置才可以的喔！所以这个指令与建立 SAMBA 的密码有关咯！ smbclient：当你的 Linux 主机想要藉由『网络上的芳邻』的功能来查看别台计算机所分享出来的目录与装置时，就可以使用 smbclient 来查看啦！这个指令也可以使用在自己的 SAMBA 主机上面，用来查看是否设定成功哩！ smbmount：在 Windows 上面我们可以设定『网络驱动器机』来连接到自己的主机上面，同样的，在 Linux 上面，我们可以透过 smbmount 来将远程主机分享的档案与目录挂载到自己的 Linux 主机上面哪！不过，其实我们也可以直接使用 mount 这个指令来进行同样的功能就是了。 nmblookup：有点类似 nslookup 啦！重点在查出 NetBIOS name 就是了。 smbtree：这玩意就有点像 Windows 系统的网络上的芳邻显示的结果，可以显示类似『靠近我的计算机』之类的数据， 能够查到工作组与计算机名称的树状目录分布图，有趣吧！ /usr/share/doc/samba-&lt;版本&gt;：这个目录包含了 SAMBA 的所有相关的技术手册喔！也就是说，当您安装好了 SAMBA 之后，您的系统里面就已经含有相当丰富而完整的 SAMBA 使用手册了！值得高兴吧！ ^_^，所以，赶紧自行参考喔！ 设置 在 smb.conf 当中设定好工作组、NetBIOS 主机名、密码使用状态等等与主机相关的信息； 在 smb.conf 内设定好预计要分享的目录或装置以及可供使用的用户数据； 根据步骤 2 的设定，在 Linux 文件系统当中建立好分享出去的档案或装置的权限； 根据步骤 2 的设定，以 smbpasswd 建立起用户的账号及密码 启动 Samba 的 smbd, nmbd 服务，开始运转 smb.conf设置访问控制和用户后台之前饰演了很多次都不行，其中的关键应该是在于passdb backend和security这一项配置，之前看的blog一般都说没有说要配置passdb backend（用户后台）这一项。samba有三种用户后台：smbpasswd, tdbsam和ldapsam. smbpasswd：该方式是使用smb工具smbpasswd给系统用户（真实用户或者虚拟用户）设置一个Samba 密码，客户端就用此密码访问Samba资源。smbpasswd在/etc/samba中，有时需要手工创建该文件。 tdbsam：使用数据库文件创建用户数据库。数据库文件在/etc/samba/passdb.tdb中，可使用smbpasswd –a创建Samba用户，要创建的Samba用户必须先是系统用户。也可使用pdbedit创建Samba账户。pdbedit参数很多，列出几个主要的： pdbedit –a username：新建Samba账户。 pdbedit –x username：删除Samba账户。 pdbedit –L：列出Samba用户列表，读取passdb.tdb数据库文件。 pdbedit –Lv：列出Samba用户列表详细信息。 pdbedit –c “[D]” –u username：暂停该Samba用户账号。 pdbedit –c “[]” –u username：恢复该Samba用户账号。 ldapsam：基于LDAP账户管理方式验证用户。首先要建立LDAP服务，设置passdb backend = ldapsam:ldap://LDAP Server [global]中的参数的含义 workgroup = 工作组的名称：注意，主机群要相同； netbios name = 主机的 NetBIOS 名称啊，每部主机均不同； server string = 主机的简易说明，这个随便写即可。 display charset = 自己服务器上面的显示编码， 例如你在终端机时所查阅的编码信息。一般来说，与底下的 unix charset 会相同。 unix charset = 在 Linux 服务器上面所使用的编码，一般来说就是 i18n 的编码啰！ 所以你必须要参考 /etc/sysconfig/i18n 内的『默认』编码。 dos charset = 就是 Windows 客户端的编码了！ log file = 登录档放置的档案，文件名可能会使用变量处理； max log size = 登录档最大仅能到多少 Kbytes ，若大于该数字，则会被 rotate 掉。 security = user, server, domain, share 四选一，这四个设定值分别代表：user ：使用 SAMBA 本身的密码数据库，密码数据库与底下的 smb passwd file 有关；share：分享的数据不需要密码即可分享；server, domain：使用外部主机的密码，亦即 SAMBA 是客户端之意，如果设定这个项目， 你还得要提供『password server = IP』的设定值才行； encrypt passwords = Yes 代表密码要加密，注意那个 passwords 要有 s 才对！ smb passwd file = 密码放置的档案，通常是 /etc/samba/smbpasswd 。 分享目录参数在目录参数部分，主要有底下这几个常见的参数喔： [分享名称] ：这个分享名称很重要，他是一个『代号』而已。 举例来说，你在 Windows 当中使用『共享』来分享网芳时，假设你将『D:\game』分享出来， 系统不是还会要你输入一个『在网络上面的名称』吗？假设你输入『My_Games』这个名称好了， 那么未来大家在网芳看到的这个文件夹 (D:\game) 名称其实是『\\你的IP\My_Games』啦！ comment ：这个目录的说明！ path ：在网芳中显示的名称 [分享名称] 中，所实际进入的 Linux 文件系统。 也就是说，在网芳当中看到的是 [分享] 的名称，而实际操作的文件系统则是在 path 里头所设定的。 read only：是否只读？ public ：是否让所有可以登入的用户看到这个项目？ writable ：是否可以写入？这里需要注意一下喔！那个 read only 与 writable 不是两个蛮相似的设定值吗？如果 writable 在这里设定为 no ，亦即不可写入，那跟 read only 不就互相抵触了！那个才是正确的设定？答案是：最后出现的那个设定值为主要的设定！ create mode 与 directory mode 都与权限有关的咯！ valid users = 用户，这个项目可以指定能够进入到此资源的特定使用者。 变量%S：取代目前的设定项目值，所谓的『设定项目值』就是在 [分享] 里面的内容！ 举例来说，例如底下的设定范例： [homes] valid users = %S ….因为 valid users 是允许的登入者，设定为 %S 表示任何可登入的使用者都能够登入的意思～今天如果 dmtsai 这个使用者登入之后，那个 [homes] 就会自动的变成了 [dmtsai] 了！这样可以明白了吗？ %S 的用意就是在替换掉目前 [ ] 里面的内容啦！ %m：代表 Client 端的 NetBIOS 主机名喔！ %M：代表 Client 端的 Internet 主机名喔！就是 HOSTNAME。 %L：代表 SAMBA 主机的 NetBIOS 主机名。 %H：代表用户的家目录。 %U：代表目前登入的使用者的使用者名称 %g：代表登入的使用者的组名。 %h：代表目前这部 SAMBA 主机的 HOSTNAME 喔！注意是 hostname 不是 NetBIOS name 喔！ %I：代表 Client 的 IP 咯。 %T：代表目前的日期与时间 安装和配置安装并创建密码很容易，使用如下脚本很容易装好。需要注意的是，用户需要是linux中已有的一个用户，如果没有的话需要先新建一个用户。 123sudo apt-get install samba -y# 设置密码，需要注意的是没有任何提示，并且是明文输入sudo smbpasswd -a -s zczhang 创建配置文件并重启服务这篇blog 第24章 配置Samba服务器写的很好，按这个配置基本上没问题。后面是我实验了很多版本，最终这个版本的配置文件久经考验是可以的。（如果也不行，可以考虑用webmine进行配置） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152sudo mv /etc/samba/smb.conf /etc/samba/smb.conf.baksudo cat &gt; /etc/samba/smb.conf &lt;&lt; EOF[global] unix charset = utf8 dos charset = cp936 log file = /var/log/samba/%m.log security = user passdb backend = tdbsam smb passwd file = /etc/samba/smbpasswd encrypt passwords = yes read raw = no deny hosts = 1.1.1.1 load printers = no max log size = 500 log level = 3 write raw = no socket options = TCP_NODELAY debug level = 3 browseable = yes os level = 20 available = yes # winbind trusted domains only = yes # dns proxy = no # name resolve order = lmhosts bcast host # winbind use default domain = yes # winbind use default domain = yes # netbios name = vbirdserver # template shell = /bin/false # display charset = utf8 # wins support = true # server string = my samba server # winbind trusted domains only = yes # socket options = TCP_NODELAY SO_RCVBUF=8192 SO_SNDBUF=8192 # NetBIOS name 名称解析有关[usrxxxhome] user = usrxxx,@usrxxx path = /home/usrxxx write list = usrxxx,@usrxxx valid users = usrxxx,@usrxxx directory mode = 771 deny hosts = 1.1.1.1 create mode = 771 writeable = yes available = yes browseable = yesEOFsudo /etc/init.d/samba restart 按照上面的配置好了，应该就可以访问了，再把这个共享文件夹映射成网络磁盘就很方便了。 如果使用webmin进行配置，需要注意的是，对于某个文件共享，口令和访问控制里面的“限于可能的列表”不能勾选，如果勾选的话会在配置文件中加入only user = yes这个配置项，然后就无法验证了，很奇怪，按理说应该也是可以的。怕是不勾选可以允许匿名访问，但我用手机实验了一下，发现还是需要输用户名和密码，而让qc实验了一下发现他电脑和我之前的一样，无法连接，所以之前怕不是windows有问题吧。。。 在ipv6中使用samba服务samba是为局域网而诞生的，除非在边界路由上将samba服务用到的几个端口做了端口转发，要不然过一个路由器就无法链接了。当然，这是针对ipv4的情况。ipv6有唯一的地址，所以路由之后也是可以访问的，不过使用windows资源管理器来进行访问需要做点特殊的处理：将一个ipv6地址\\2001:da8:201:1146:2be4:946c:af92:356f替换成\\2001-da8-201-1146-2be4-946c-af92-356f.ipv6_literal.net放到文件浏览器的地址栏就可以了。 最后，将安装和配置的整个的脚本放在这里，大家可以根据自己的实际情况修改使用。]]></content>
      <categories>
        <category>软件安装和使用</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>软件安装和使用</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu双网卡（内网+外网）]]></title>
    <url>%2F2017%2F10%2F10%2F2017-10-10-doubleNetCard%2F</url>
    <content type="text"><![CDATA[缘起实验室的台式机有公网ip，而平时主要使用笔记本，平时拷文件使用scp有些慢，并且也不想每次钻桌子底插usb，所以就想加快笔记本和台式机的网络速度。 分析 连接公网的有线不支持dhcp，意味着没法直接买一个交换机来插上。同时我的笔记本也没有rj45接口，需要一个usb的网卡适配器，每次拔插usb也不爽，所以该方案放弃。 有线连接路由器，然后路由器接台式机和笔记本，笔记本用无线。但该方案不知道怎么保留公网ip，不知道路由器外ip能不能设置成公网ip然后配置一个路由表项？ 实验室还有一个无线路由，平时我笔记本用这个来上网，所以有一个看起来可行的方法，即买一个无线网卡插在台式机上，然后外网访问用有线网卡，和笔记本之间的通信无线，由于两个无线网卡接在同一个AP上，少绕了几个圈子，应该快一点。看起来不错的样子，遂行动。 还是使用台式机装一个有线网卡一个无线网卡的方式，不过使用无线网卡作为AP，开一个无线网，然后将台式机配置好路由，这个方法肯定是可行的，但ubuntu的网络配置我不会，而且买的小米随身wifi在ubuntu上也不原生支持AP模式，配置有点麻烦，遂放弃。后来发现方案3不行简单看了一下，但没有弄，参考这个。 之后又有了一种新的方案，就是在台式机上再装一块有线网卡，然后买一个千兆的USB网卡，网线连接surface和台式机，这样他们交换数据就是千兆的了，已经基本上达到机械硬盘的上限了。同时，把台式机配置成路由器，笔记本把台式机作为网关，实现对外网的访问。最终采用了这种方案。 ubuntu双网卡（内网+外网）的配置配置还是比较简单的。可以直接参考这篇文章，下面做一下备份，放置遗失。 环境ubuntu 14.04 server拥有两块网卡的服务器外网IP:210.44.185.75 外网网关:210.44.185.10内网IP:10.6.0.248 内网网关:10.6.0.254 需求服务器能通过网卡1来连接外网的某台主机；同时要求局域网内网段为 10.6.1. , 10.6.4. , 10.6.15.* 的三个网段要能通过网卡2连接服务器。 明确一台双网卡电脑拥有两个网关是不可能的，因为默认网关（default gateway）只能是一个！ 解决方案：1.配置网络1sudo vim /etc/network/interfaces 配置如下： 1234567891011auto eth0iface eth0 inet staticaddress 210.44.185.75netmask 255.255.255.0gateway 210.44.185.10dns-nameservers 115.27.254.2 114.114.114.114auto eth1iface eth1 inet staticaddress 10.6.0.248netmask 255.255.255.0 只设置外网IP的网关，不要设置内网IP的网关。 2. 重启网络1sudo /etc/init.d/networking restart 3.设置路由这时我们的第一条需求已经实现了，但是由于没有设置内网网关，第二条需求还实现不了，我们需要分别给这三个网段设置路由。注意：一块网卡只能设置一个网关，多个网关会发生冲突而无法成功配置。操作如下： 1234sudo route add -net 10.6.0.0/24 gw 10.6.0.254 dev eth1sudo route add -net 10.6.1.0/24 dev eth1sudo route add -net 10.6.4.0/24 dev eth1sudo route add -net 10.6.15.0/24 dev eth1 最后使用 ip route或者route 查看路由设置情况： 1234567ttop5@ubuntu:~$ ip routedefault via 210.44.185.10 dev eth0 metric 10010.6.0.0/24 dev eth1 proto kernel scope link src 10.6.0.24810.6.1.0/24 dev eth1 scope link10.6.4.0/24 dev eth1 scope link10.6.15.0/24 dev eth1 scope link210.44.185.0/24 dev eth0 proto kernel scope link src 210.44.185.75 如有多余的配置，可使用下面的命令进行删除，祝你好运！😀 1sudo route del -net *.*.*.*/* dev eth* 到此为止，我们就设置完毕了，内外网应该都可以访问了,不过由于路由是手动添加进去的，所以系统重启之后路由就丢失了．不过我可以将设置的命令保存为一个脚本，然后在 /etc/rc.local中调用执行或者直接添加到该文件中。 悲剧原文中没有添加dns信息，ping的时候会出现无法解析的情况。添加dns，解决这个问题之后发现可以ping通外网，但台式机和笔记本直接互相无法ping通，笔记本也无法ping通台式机的公网ip，遂怀疑是不是无线网里面不让通信。使用手机实验，连接同一个AP时，和笔记本也无法互相ping通，也无法ping通台式机的局域网ip和公网ip。但使用流量可以ping通台式机的公网ip。画一下图发现只要经过了无线网里面的通信都无法进行，使用笔记本ping公网ip时，由有线网卡收到，但根据上面配置的路由表项，全部转发给内网的无线网卡，遂挂。所以确实是因为无线局域网里的设备无法通信，但还不知道为什么。求助HC之后，他说是不是配置了AP隔离，恍然大悟。 有线方案双网卡配置主要可以参考这篇blog。主要分为三个步骤： 配置双网卡一个内网一个外网，和上面的无线方案完全相同。 打开IP转发。修改/etc/sysctl.conf，取消这一行的注释： 12然后使之立即生效```sudo sysctl -p 在iptables添加一条NAT规则，一般都是这样配置，然后将其加入的rc.local中，使之开机启动。 1234iptables -Fiptables -P INPUT ACCEPTiptables -P FORWARD ACCEPTiptables -t nat -A POSTROUTING -s 192.168.121.0/24 ! -d 192.168.121.0/24 -o eth1 -j SNAT MASQUERADE 不过我配置了，然后也加入到rc.local了，但还是不行。可能是iptables没有激活，然后我用Webmin进行了配置，然后就可以了。然后查看iptables的规则可以使用iptables -n -vv -L。详细的iptables的使用，可以参考这篇iptables详解 整个过程，这篇blog说的很详细，他的网络拓扑图也画得很好，直接盗用下。 收获对路由器和交换机的认识加深了一点。其他的还是ubuntu上的网络操作还是迷迷糊糊，有点不值得。 教训 少折腾，能用就行 如果按照网上说的配置了但有了问题，立马去查看文档，详细的系统的学习一下，不要再头疼医头脚疼医脚，随时记笔记，保持头脑清醒。]]></content>
      <categories>
        <category>软件安装和使用</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>软件安装和使用</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TexLive的安装]]></title>
    <url>%2F2017%2F09%2F26%2F2017-09-26-TexInstall%2F</url>
    <content type="text"><![CDATA[安装linux下的安装linux下安装Texlive还是比较简单的，参照Quick install即可，分为下面几步： 清理环境 下载运行安装器，注意此处可以选择安装模式，下载服务器（不过我实验了一下好像没有用） 安装完成后记得设置环境变量 运行latex sample2e.tex实验一下看是否成功。更详细的可以看中文安装文档，还包含了各目录的说明等。 Windows下的安装直接去ctex下载安装即可。 中文支持方案1. 使用ctex宏包，latex编译；方案2. 使用XeLaTeX + UTF-8编码的源文件；方案一实验了可行，但方案二好像不行。]]></content>
      <categories>
        <category>软件安装和使用</category>
      </categories>
      <tags>
        <tag>软件安装和使用</tag>
        <tag>Tex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RL课程笔记·第五节·免模型控制]]></title>
    <url>%2F2017%2F09%2F25%2F2017-09-29-RL%E7%AC%94%E8%AE%B0Lec5%2F</url>
    <content type="text"><![CDATA[Introduction$\epsilon$ Greedy 探索GLIEGLIE Monte-Carlo 控制On-Policy Monte-Carlo ControlOn-Policy Temporal-Difference LearningOff-Policy LearningSummary]]></content>
      <categories>
        <category>课程</category>
        <category>RL</category>
      </categories>
      <tags>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RL课程笔记·第四节·免模型预测]]></title>
    <url>%2F2017%2F09%2F25%2F2017-09-25-RL%E7%AC%94%E8%AE%B0Lec4%2F</url>
    <content type="text"><![CDATA[Introduction蒙特卡洛学习Monte-Carlo Learning原理：大数定理 初访蒙特卡洛（First-Visit MC）Policy Evaluation: 只记开始状态的$G$ 每访蒙特卡洛（Every-Visit MC）Policy Evaluation: 每次遇到都会记下这个状态的估计递推求平均值的意义：向偏离方向靠近 $N \leftarrow N+1$$avg \leftarrow avg+\frac{1}{N}(X_i-avg)$将$\frac{1}{N}$换成一个常数$\alpha$得到$avg \leftarrow avg+\alpha (X_i-avg)$起到遗忘的作用。 12345678910111213141516import numpy as npimport matplotlib.pyplot as plx = np.random.randint(1,100,[1000])avga = [x[0]]for n in range(2,1001): p = avga[len(avga)-1] avga.append(p+(x[n-1]-p)/n)pl.plot(range(0,1000),avga)alpha = 100avgb = [x[0]]for n in range(2,1001): p = avgb[len(avgb)-1] avgb.append(p+(x[n-1]-p)/alpha)pl.plot(range(0,1000),avgb,'r')pl.show() Temporal-Difference Learningbootstrapping这里计算$V(S_t)$使用这个公式（最简单的形式$TD(0)$）： V(S_t) \leftarrow V(S_t)+\alpha (R_{t+1}+ \gamma V(S_{t+1})-V(S_t))其中，$R_{t+1}+ \gamma V(S_{t+1})$称为TD target，$\delta_t=R_{t+1}+ \gamma V(S_{t+1})-V(S_t)$称为TD error。 MC和TD对比 蒙特卡洛需要把complete episodes，not by bootstrapping，TD方法使用incomplete episodes，by bootstrapping TD可以在连续环境（没有终止状态）中进行，MC只能在有终止状态的环境中进行。 MC中的Return $G_t$是$V_{\pi}(S_t)$ 的非偏估计（unbiased estimate）。大多数情况下TD中的TD target $R_{t+1}+ \gamma V(S_{t+1})-V(S_t)$是实际$V_{\pi}(S_t)$的有偏估计。 TD Target比Return的方差更小，因为它只基于一步。 MC是高Variance，低bias的，其优点（1）很好的收敛（2）（even with function approximation）（3）对初始值不敏感（4）非常容易理解和使用。 TD低Variance，但有bias，其优点（1）通常更高效（2）TD(0)可以收敛到$V_{\pi}(S_t)$（3）（but not always with function approximation）（4）对初始值敏感 MC收敛到均方差最小，TD收敛到最可能的马尔科夫模型（solution of max likelihood Markov model） TD利用了Markov性质，MC没有利用，所以在Markov环境中TD方法更有效，在非Markov环境中，MC方法更有效 bootstrapping sampling MC × √ DP √ × TD √ √ $TD(\lambda)$MC是走到头，TD是走一步，所以有很多中间状态，即走$n$步，甚至步数不同时也是可以组合在一起的。把两个TD Target分开更新和合并到一起更新有区别么？]]></content>
      <categories>
        <category>课程</category>
        <category>RL</category>
      </categories>
      <tags>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RL课程笔记·第一节·RL简介]]></title>
    <url>%2F2017%2F09%2F23%2F2017-09-23-RL%E7%AC%94%E8%AE%B0Lec1%2F</url>
    <content type="text"><![CDATA[关于RLRL和监督学习，非监督学习是并列的 RL的特性 没有监督，通过reward信号来训练 反馈是延迟的而不是即时的 time非常重要，通常并不是独立同分布的 agent的action影响他之后环境 RL问题RL术语 Rewards $R_t$一个标量的反馈信号，表明在$t$步agent的表现，agent的目标就是最大化各步累计的reward。RL就是基于reward假设，即所有的目标都可以通过最大化期望累计reward来描述 序列决策 Sequential Decision Marking目标是做一系列action来最大化所有的未来reward，其中有个限制：action可能有长期的影响，reward可能被延迟，可能牺牲掉眼前利益可以获得更大的长期利益。 Agent and Environment Observation $O_t$ Action $A_t$ History $H_t$ $H_t = O_1,R_1,A_1,…,A_{t-1},O_t,R_t$ State $S_t$用来决定将要发生什么的信息，正式的，state是history的函数，$S_t=f(H_t)$ Environment State $S_t^e$Environment用来决定下阶段的发射的observation和reward Agent State $S_t^a$Agent用来决定下阶段的action，是RL算法中需要的信息，$S_t^a=f(H_t)$ Information State(Markov State)当前状态包含了历史中所有有用信息的状态，即Markov过程。$H_{1:t} \rightarrow S_t \rightarrow H_{t+1:\infty}$ 完全可观察的环境 Fully Observable Environment（MDP）agent可以直接观察环境，$O_t=S_t^a=S_t^e$，正式的，这就是马尔科夫决策过程（Markov decision process，MDP） 部分可观察环境 Partially Observable Environment（POMDP）agent间接的观察环境，此时$S_t^a \neq S_t^e$，称作部分可观察马尔科夫决策过程（Partially observable Markov decision process，POMDP）。agent必须自己构造$S_t^a$ Policy $A_t \pi$agent的行为函数，从状态s到动作action的映射，可以是确定式的，$a = \pi (s)$，也可以是随机形式的，$\pi(a|s) = P[A_t=a|S_t=s]$ Value function $V$描述这个state/action有多好，即预测将来的reward，从而做出一个好的action，V_{\pi}(s)=\mathbb{E}_{\pi}[R_{t+1}+\gamma R_{t+1}+\gamma^2 R_{t+2}+...| S_t=s] Modelmodel预测环境接下来干什么，$P_{ss’}^a$描述状态的变化，$R_s^a$描述未来所能获得的reward，有$P_{ss’}^a=\mathbb{P}[S_{t+1=s’| S_t=s,A_t=a}]$ $R_s^a=\mathbb{E}[R_{t+1}| S_t=s,A_t=a]$ Inside An RL Agent组成，一个RL agent必须包含Policy，Value function，Model中的一个或几个部分。 分类 Value Based Policy Based Actor Critic Model Free Model Based RL中的问题Learning 和 Planning RL中，环境初始是未知的，agent和环境进行交互，agent改进的是他的policy Planning中，环境（Model）初始是已知的，agent不和环境交互，也是改进的他的policy 探索和开发 Exploration and ExploitationRL 类似于不断试错然后学习，需要不断探索（Exploration），发现环境的更多信息，然后利用已知信息上开发（Exploitation）最大化reward的方法。 预测和控制 Prediction and Control预测是给你这Policy让你去评估将来reward，Control让你找到一个最优的Policy来得到一个最优的将来reward]]></content>
      <categories>
        <category>课程</category>
        <category>RL</category>
      </categories>
      <tags>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Traffic Engineering Based on Stochastic Model Predictive Control for Uncertain Traffic Change 论文阅读笔记]]></title>
    <url>%2F2017%2F09%2F19%2F2017-09-19-Traffic%20Engineering%20Based%20on%20Stochastic%20Model%20Predictive%20Control%20for%20Uncertain%20Traffic%20Change%2F</url>
    <content type="text"><![CDATA[概括在使用MPC(Model Predictive Control)做TE(Traffic Engineering)时，可能某次流量预测并不准确而按照这个准确的值进行调教反而导致了网络拥塞，因此Otoshi提出相应的改进方法，即将预测错误的概率考虑进去，用一个高斯分布来描述预测错误的概率，从而将问题转化为一个在发生拥塞的可能性小于给定阈值的约束下的减少网络变化的优化问题。同时，作者基于越远时间的预测可能越不准，而其阈值也应该更加宽松提出了Relax的操作。实验证明，论文中提出的方法对于减少由预测错误导致的网络拥塞问题有效果。 Meta-Data 发表期刊：2015 IFIP/IEEE International Symposium on Integrated Network Management (IM) 作者：Otoshi, Tatsuya; Ohsita, Yuichi; Murata, Masayuki; Takahashi, Yousuke; Ishibashi, Keisuke; Shiomoto, Kohei; Hashimoto, Tomoaki; 年份：2015 前提对route的更改，一次大的更改比多次小的更改代价更大。在考虑错误信息之后，确实是改的频繁了，但大的更改少了。但从表1中，加了relax的最大值比不加relax的反而大了，但改的不那么频繁了，所以。。到底是想干什么呢？ I. Introduction本篇论文的核心是TE method that is robust to prediction errors，SMP-TE can avoid the congestion under limited resources and the existence of prediction error。有两个目标： 减少因预测错误导致的拥塞 对链路的更改尽量小 把TE问题看作一个在发生拥塞的可能性小于给定阈值的约束下的减少网络变化的优化问题。考虑预测错误概率分布的MPC就叫做stochastic MPC(SMPC)，应用于TE，本文中称之为stochastic model predictive traffic engineering (SMP-TE)。 II. dynamic traffic engineeringTE的步骤： 获取traffic信息 在这些信息的基础上计算路由 应用这些路由结果 使用$x(t-1)$代替$x(t)$来计算分配方案，使得容易出现问题。解决方法之一是让t变小，从而一段时间之内的可以看作变化不大，但这样频繁的调整网络有代价。另一种方式是预测t时刻的流量来进行规划，但预测不一定对，所以本篇论文引入了预测错误的分布。 III. SMP-TE: Stochastic Model Predictive Traffic EngineeringA. Stochastic Model Predictive Control1) Model Predictive Control 要和目标$y(k)$尽可能的接近，所以引入代价函数$J_1=\sum_{k=t+1}^{t+h}{||y(k)-r_y(k)||^2}$ 还要尽可能的平缓，所以引入$J_2=\sum_{k=t+1}^{t+h}{||u(k)-u(k-1)||^2}$，感觉这里不太对，应该是$\hat{y}(k)-\hat{y}(k-1)$吧 所以总的代价为$J=(1-w)J_1+wJ_2$ 2) Probability Constraints使用上面说的引入预测来改进使用$x(t-1)$代替$x(t)$来计算分配方案的问题时，又引入了一个新的问题，即预测错了怎么办？一种方法是按最坏的情况配置冗余，还有一种方法是讲错误的发生作为一个随机变量，为了克服这个错误需要配置的冗余也变成一个随机变量，这样就引入一个soft bound，可以给定一个阈值和容量上限，这样最大的可通行流量就可以确定出来。 B. Applying SMPC to TE1) TE Model for SMPC 预测traffic rates 根据预测进行规划，使$J$最小 2) Formulation of the Optimization Problem求解一个给定阈值$p$和链路容量$C$，使其发生拥塞的概率小于$p$的条件下，使改变幅度尽量小的方案。 3) Relaxation of Future Probabilistic注意，代价函数是在多个调节轮次上定义的，虽然只用于调节当前的这一次。可以发现，预测显然是紧邻的下一个最靠谱也最重要，其他的预测就不那么靠谱也没有那么重要了，因为之后会有反馈修正，这样还是要求他们具有相同的拥塞的概率就不合适了，可以把这个概率$q$逐渐放宽。论文中使用了$q(k)=(1-p)exp(-\frac{k-t-1}{\tau})$来设定概率阈值，$\tau$是可以设定的决定松弛素的常数。当$q(k)&gt;0.5$时则不再上升，置为$0.5$ IV. EvaluationA. Simulation Environment1) network topology2) traffic使用Netflow协议收集，100个包取样一次，5分钟汇总一次。设置的目标利用率为95%。 3) prediction error model假设预测错误服从高斯分布。其均值为0，方差与多少步成正比$k\sigma ^2$，使用normalized mean squared error (NMSE)来计算$\sigma ^2$，$NMSE=\frac{\sigma_j^2}{V[x_j(t)]}$，这里取$NMSE=0.3$，所以令$\sigma_j^2=0.3V[x_j(t)]$，$j$是表示第几条路由。 4) Cost Function使用平均hop length作为代价函数，因为降低hop length可以降低传播时延（propagation delay），相对于传播时延，在排队延时（queuing delay ）可以忽略不计。平均hop length D定义为$D=\frac{1}{F}\sum_j{\sum_{i\in \vartheta }{R_{i,j}d_i}}$，使用正则化的hop length $\frac{D}{\max{X_jd_j}}$作为代价函数前面说了一种代价函数呀，为什么还要有一个，这两个一样么？。$w$最后选定0.5，因为实验中更改$w$从0到1，步长为0.1结果没啥变化。没啥变化不是说明这个方法不大靠谱么。。。。 5) Routing Calculation求解上面提到个约束，先将概率不等式转换为确定性的形式，$\forall k,\forall l,\hat y_l(k)+\Phi^{-1}(1-p)\sqrt{\sum_j{A_{l,j}(k)^2k\sigma_j^2}}\leq C_l(k)$，此处$A(k)=G · R(K)$,$\Phi^{-1}$是高斯分布的分位点函数（quantile function of the Gaussian distribution），通过这个变换之后，原问题就变成一个凸优化问题，叫做second-order cone programming (SOCP)，可以使用CPLEX包来求解该问题。 6) Compared Methods 和只预测一步的模型进行对比，也就是对应本模型中的$h=1,p=0.5$的情况，以证明多步预测是游泳的。 和基于MPC的TE方法进行对比，其基于多步预测，但并没有把预测错的的概率考虑进去，对应本模型$p=0.5$的情况，来证明本片论文中提出的方法是有效的，可以避免预测错的的影响而不造成明显的路由变化。 B. Effect of Stochastic Constraint第一步是比较一下看SMP-TE能不能减少因为预测错误导致的拥塞，使用99.9% delay来表示99.9%的包的延迟时间都小于此数值，使用M/M/1模型根据流量来计算延迟，公式为$-log(1-0.999)\frac{\bar L}{C_l-y_l}$,其中${\bar L}$表示包的平均长度。实验证明SMP-TE确实可以减少由预测错误导致的拥塞。 C. Multi-Step Prediction Effect来验证多步预测中可以让链路变化小的特性在SMP-TE中依然保持着。但引入预测错误之后，允许的阈值较小时需要预留较大冗余，也让两次之间的变化更大。 D. Probability Relaxation Effect不论是从变得多少来看，还是变得是否频繁来看，这种方法都不比其他方法好啊。。。。 E. ScalabilitySMP-TE的时间复杂度为$O(m^2n^6)$，$m$是链路数量，$n$是节点数量。 V. Conclusion]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>网络流量</category>
      </categories>
      <tags>
        <tag>论文阅读笔记</tag>
        <tag>网络流量</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Survey of Techniques for Internet Traffic Classification using ML论文阅读笔记]]></title>
    <url>%2F2017%2F09%2F15%2F2017-09-15-A%20Survey%20of%20Techniques%20for%20Internet%20Traffic%20Classification%20using%20Machine%20Learning%2F</url>
    <content type="text"><![CDATA[AbstractIntroduce传统方法根据TCP端口及内容推测。基于两个假设： 第三方可以获取到包内信息。 知道包格式但现在两个假设都不容易满足，所以想要根据其他特性来对流量类型进行聚类。 使用ML进行流量分类的步骤 定义特征（最大/小包的长度，典型包的长度，间隔时间等） 使用ML方法训练和识别 文章组织 II概括了IP流量分类对网络操作的重要性，并介绍了如何评价分类的准确性，并讨论论文传统的根据端口和负载进行流量分类的缺点。 III介绍了ML的背景知识和利用ML方法进行流量分类后操作网络的必要条件。 IV主要介绍了2004-2007年的主要工作，并分析了他们是否满足了III中的条件。 总结和展望 II. Application context for machine learning based ip traffic classificationA. The importance of IP traffic classification QoS和依照QoS进行的计价系统需要IP分类 合法的信息侦听（LI） B. Traffic classification metrics Positives, negatives, accuracy, precision and recall Byte and Flow accuracy C. Limitations of packet inspection for traffic classification Port based IP traffic classification: 用户不用公开默认的端口，不到70%的准确率 Payload based IP traffic classification： 负载大，需要维护庞大的协议列表，可能违法 D. Classification based on statistical traffic properties假设： 每种流量的持续时间，间隔，长度等统计特性都不一样，可以根据这个来进行分类 III. Background on machine learning and the application of machine learning in ip traffic classificationA. A review of classification with Machine LearningWitten and Frank [29] 把学习方法分为4种： Classification (or supervised learning)决策树、朴素贝叶斯ROC曲线，Neyman Pearson criterion（给定FP，最大化TP）stratified cross-validation Clustering (or unsupervised learning)k-means、 incremental clustering、probabilitybased clustering method评估：external criteria, internal criteria and relative criteria Association（联想） Numeric prediction 特征选择方法：filter method or wrapper method（例如Correlation-based Feature Selection (CFS) filter techniques with Greedy, Best-First or Genetic search[29] [40] [41] [42] [43]） B. The application of ML in IP traffic classification 特征包的长度，到达时间的标准差，流的长度（bytes或包个数），间隔时间的傅里叶变换等 When evaluating supervised ML schemes in an operational context it is worthwhile considering how the classifier will be supplied with adequate（充足的） supervised training examples, when it will be necessary to re-train, and how the user will detect a new type of applications.When evaluating unsupervised ML schemes in an operational context it is worthwhile considering how clusters will be labeled (mapped to specific applications), how labels will be updated as new applications are detected, and the optimal number of clusters (balancing accuracy, cost of labeling and label lookup, and computational complexity). C. Challenges for operational deployment 及时并且连续的分类Timely and continuous classification要从流的任意地方开始，用尽量少的包就可以确定流的类型，而不能等到流完了再给出类型。 Directional neutrality有些类型的流服务端到客户端和客户端到服务端的特性不一样，也无法预知哪个是服务端哪个是客户端，这种情况下要根据任意一个方向的流来判断这个流的类型。 Efficient use of memory and processors 可移植性和鲁棒性（Portability and Robustness） IV. A review of machine learning based ip traffic classification techniquesA. Clustering Approaches 在2004年，McGregor等使用EM算法对HTTP, FTP, SMTP, IMAP, NTP和DNS流量进行聚类[48]，虽然现在我们对流量属于哪个应用更感兴趣，但这种方法在初期我们对网络流量一无所知的时候作为一种预处理方法来分析网络流量的性质还是很有帮助的。 Zander[46]在2005年使用AutoClass[50]来对流量进行分类，使用EM和贝叶斯聚类来确定类别。特征的计算也是分为两个方向，基于整个流进行的，但是设置了超时时间为60秒。之后在[48]中使用intra-class homogeneit H改进了评估方法。一个类别的H定义为该类别中最大量流所占比例，所有类别的H定义为各个类别的平均。目标为最大化H以实现不同应用流量的分离。实验证明，这种方法可以实现各应用流量的分离，并且特征越多，H越高，最高可以达到85%~89%。这种方法按照应用的个数进行聚类和评估，每种应用作为一类。 TCP-based application identification using Simple KMeans在2006年，Bernaille[53]提出了一种使用K均值法进行聚类的方法，与前两种不同的是他可以利用TCP流的前几个包提前做出判断，基于前几个包包含了协商部分，这对于每个应用来说是独特的。训练过程是离线的，使用前p个包作为训练数据，使用欧氏距离来度量两个样本的距离。使用前5个包时，正确率超过80%。他的主要问题在于需要捕捉到前面的包。 Identifying Web and P2P traffic in the network coreErman [47]在2007解决网络核心（the network core）的流量分类问题，网络核心流的信息是受限的，他们的方案是基于单向流的，并且发现了对于TCP连接，server-to-client方向的流量包含了更多的信息。也是使用了K均值法来聚类，使用欧氏距离来度量差别。该工作的主要贡献在于尝试提出了使用单向的网络流量进行分类的可能性，为了验证，他在server-to-client和client-to-server及混合流量上都做了实验。开始时先对流量进行手动分类，然后进行聚类，把聚类得到的每个cluster中占比最大的一种流量作为该类（class）流量的标签。 B. Supervised Learning Approaches Statistical signature-based approach using NN, LDA and QDA algorithms在2005年，Roughan等[18]使用nearest neighbours (NN), linear discriminate analysis (LDA) and Quadratic Discriminant Analysis (QDA)机器学习方法将不同的网络应用映射到不同的QoS网络级别。作者列举了对整个流级别上可以利用的特征，并将它们分为5个类别。分别是：• Packet Level:包长度的均值，方差，均方差等。• Flow Level: 对每个方向的流计算持续时间，每个流的数据量，每个流包的数量（并求他们的均值和方差）• Connection Level:TCP窗口大小，吞吐量的分布，连接的对称性• Intra-flow/connection features:两个包之间的间隔时间• Multi-flow:并发的连接数他们做了3个级别的实验，分别是分成3类(Bulk data (FTP-data), Interactive (Telnet), and Streaming (RealMedia);)，四类（Interactive (Telnet), Bulk data (FTP-data), Streaming (RealMedia) and Transactional (DNS)）和七类（DNS, FTP-data, HTTPS, Kazaa, RealMedia, Telnet and WWW.）类数越多，错误率越高，三类时错误率2.5% ~3.4%，5类时错误率5.1% ~7.9%，三类时错误率9.4% ~12.6%。 Classification using Bayesian analysis techniques2005年Moore and Zuev提出了应用朴素贝叶斯分类的方法来对流量进行分类，实现了大概65%左右的准确率。使用NBKE（Naive Bayes Kernel Estimation）和FCBF（Fast Correlation-Based Filter )优化后可以达到95%。Recall对于不同应用差别较大，比如www98%，bulk data90%，而services traffic44%，P2P55%。[55]使用贝叶斯网络优化了该方法，并达到了当日流量分类正确率99%，8个月后流量分类正确率95%的好成绩。 Real-time traffic classification using Multiple Sub-Flows features在2006年，Nguyen and Armitage [56]实验了使用部分包进行分类。他们使用multiple subflows特征训练分类器，首先从原始双向数据中抽取出数段长度为N个包的subflows，使用它上的统计特性作为分类的依据。使用朴素贝叶斯作为分类器。当使用全部流量但前面的几个包丢掉了的时候，性能相当差。而使用这种方法，sub-flow设置为25个包的时候，即使从中间开始也可以达到95%的召回率和98%的准确率。但其实验是在基于Web, DNS, NTP, SMTP, SSH, Telnet, P2P 等多种应用流量中分离出基于UDP的第一人称射击游戏的流量。 Real-time traffic classification using Multiple Synthetic Sub-Flows Pairs在2006年，Nguyen and Armitage[54]扩展了之前的工作，将截取的包反转作为反向流量加入到训练数据中来解决directional neutrality的问题。 GA-based classification techniquesPark等[60]在2006年使用遗传算法来选择在[44]中提到的特征，并使用三种分类器（the Naive Bayesian classifier with Kernel Estimation (NBKE), Decision Tree J48 and the Reduced Error Pruning Tree (REPTree) ）分别进行了实验。 Simple statistical protocol fingerprint methodCrotti等在2007年提出了一种新的基于IP包（IP包长度，两个包之间像个时间和到达顺序）的方法称之为protocol fingerprints，其特征使用PDF表示，PDFi 表示第i个二元组Pi (Pi = {si, Δti})，其中si表示第i个包的大小，Δti表示两个包之间的长度。然后作者又定义了一个anomaly score来衡量两个PDF之间统计特性的差以确定这个流量属于哪一累，最终在HTTP、SMTP和POP3上取得了91%的正确率。其问题也主要是需要捕获到第一个包并且需要区分出方向，对包的丢失或者重排序也无能为力。 C. Hybrid Approaches Erman[62]在2007年提出了一种半监督（semi-supervised）的方法来解决标注数据的缺乏和未知类别的处理两个问题。首先将部分带标签和部分不带标签的训练数据进行聚类，然后对聚得的cluster使用最大似然估计确定对应的class，对于某个cluster中没有标注过的数据，则将之设置为与最近的cluster的类别种类。 D. Comparisons and Related Work Comparison of different clustering algorithms2006年Erman[45]在两个数据集（the University of Auckland的公开数据集和the University of Calgary的私有数据）上测试了三种聚类算法（K-Means, DBSCAN and AutoClass），实验验证AutoClass的overall accuracy最高，K均值法次之，DBSCAN最差。 Comparison of clustering vs. supervised techniquesErman还使用recall, precision和overall accuracy三个指标在University of Auckland (NLANR)提供的两个72数据上，比较了朴素贝叶斯分类器和AutoClass聚类的区别。结果出乎意料的AutoClass全面优于朴素贝叶斯，作者分析原因应该是训练数据太少。除了结果之外，还比较了训练时间，AutoClass需要的训练时间远远超过朴素贝叶斯（2070s vs. 0.06s）。 Comparison of different supervised ML algorithmsWilliams[65]在公开的NLANR数据集上对比了不同分类算法（Naive Bayes with Discretisation (NBD), Naive Bayes with Kernel Density Estimation (NBK) , C4.5 Decision Tree, Bayesian Network, and Naive Bayes Tree）的性能。具体的，他们使用了三组特征，分别是所有的22个特征（见表），使用correlation-based feature selection (CFS) 和 consistency based feature selection (CON) 算法选择出来的特征子集。在全特征集上，除了NBK准确率为80%，其他分类算法都在95%以上。而是用两种特征子集都没有让分类算法的正确率发生大的衰退，最大的衰退只有2.5%。从算法性能来看，C4.5不论在哪个特征集上，其分类速度都是最快的，其他依次是NBD, Bayesian Network, Naive Bayes Tree, NBK。而模型的构建时间Naive Bayes Tree最长，其他依次是C4.5, Bayesian Network, NBD, NBK。 ACAS: Classification using machine learning techniques on application signaturesHaffner[57]在2005年提出了一种使用ML方法构造应用指纹的思路，他分别实验了使用Naive Bayes, AdaBoost and Maximum Entropy三种方法对TCP单向流的前64bytes构造签名从而对流量进行分类，实验也取得了很好的效果，Recall可以达到94%，Precision可以达到99%。 Unsupervised approach for protocol inference using flow content在2006年，Ma[66]提出了另一种思路，即使用流的内容进行协议推理（protocol inference）而不是其统计信息进行分类。protocol 被定义为 ‘a pair of distributions on flows’ ，包括a byte sequence from the initiator to the responder 和a byte sequence from the responder to the initiator。它使用了product distribution, Markov processes, and common substring graphs (CSG)三种方法进行推理，取得了还不错的结果。 BLINC: Multilevel traffic classification in the dark:Karagiannis[15]开发了一种基于source host和传输层的行为进行分类的方法，利用包括源和目标地址及端口，传输协议，包的数量，大小等信息来描述source host的行为并对web, p2p, data transfer, network management traffic, mail, chat, media streaming, and gaming进行分类。作者总结出对于客户端其源端口和目的端口一般一样多，对于服务端一般源端口比目的端口少的特性。该方法提出了利用网络行为来进行区分节点角色和流量分类的新思路。 Pearson’s Chi-Square test and Naive Bayes classifierBonfiglio[67]使用皮尔森卡方检验和朴素贝叶斯来实时的从一个流中识别出Skype流来，卡方检验的方法只需要看消息的前几个bytes，朴素贝叶斯只需要看连续的30个包。 E. Challenges for operational deployment Timely and continuous classification大部分工作（as [14] [18] [46] [48] [64] [65]）需要统计整个流的信息，[53]和[61]只需要统计前几个包的信息，但无法处理前面的包丢失的情况，[56]则使用了一个滑动窗口，所以它不需要必须捕获到最开始的包。 Directional neutrality大部分工作都使用了单向流( [14] [48] [53] [46] [68])并假设能知道流的方向，这给流的分类带来了遍历，但当不知道方向时就损害了性能。[54]则探究了不使用额外的信息来构建分类模型的方法。 Efficient use of memory and processors[14] 和[55]研究了分类准确性的潜力，但其特征数量很大，对于时间和空间的消耗很大。Williams[65]就研究了训练时间和分类时间的权衡。但其他的时间和正确性的权衡研究的并不多。 Portability and Robustness目前没有工作严肃的讨论了稳定性和可移植性的问题，没有实验在丢包，坏包，延迟及抖动等情况下的表现。虽然聚类方法理论上具备识别新流量的潜能，但之后[62]简单的提到了这一点。 Qualitative summary V. Conclusion首先ML方法确实可以应用于流量分类。本综述介绍了使用不同的ML方法，例如AutoClass, Expectation Maximisation, Decision Tree, NaiveBayes进行离线分析和分类，证明最高可以到很高的正确率。以前的方法更注重静态的，统计的数据来进行离线分析，近来的工作更注重实用性。我们觉着基于ML的流量分类已经到了应用的临界点。但现在也还有很大提升空间，例如要注意不同时间点的区别，换一个时间点可能就没有这么高的正确率。并且可以组合多种分类模型，提高计算效率，提高鲁棒性等。还有一些针对新兴应用的流量分类也需要进一步研究。]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>网络流量</category>
      </categories>
      <tags>
        <tag>论文阅读笔记</tag>
        <tag>网络流量</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ML课程笔记·第一节]]></title>
    <url>%2F2017%2F09%2F11%2F2017-09-11-ML%E8%AF%BE%E5%A0%82%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[推荐书籍 课程书籍《统计学习方法》李航 《机器学习》周志华 《The elements of Statistical Learning》 作业与考核项目作业4次考核：作业60%+期末考试（闭卷笔试）40%最后一次课程时间随堂考试2017.12.25例行答疑：每周一13：30-14：30理科一号楼1422E 正则项由于模型复杂度高时容易导致过拟合，所以要加入一个正则项。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则项应越高。 监督学习的分类 生成方法（generative approach）学习P(X,Y)，然后通过P(Y|X)=P(X)P(X,Y)得到。例子：朴素贝叶斯方法、隐马尔科夫模型优点：1. 可以用于存在隐变量的情形（全概率公式）。 判别方法（discriminative approach）直接学习P(Y|X)例子：优点：直接 任务 分类问题classification评价：precision，recallF1 回归问题 标注问题隐马尔科夫，条件随机场]]></content>
      <categories>
        <category>课程</category>
        <category>ML</category>
      </categories>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Inter-data-center network traffic prediction with elephant flows 论文阅读笔记]]></title>
    <url>%2F2017%2F09%2F09%2F2017-09-09-Inter-data-center%20network%20traffic%20prediction%20with%20elephant%20flows%2F</url>
    <content type="text"><![CDATA[Inter-data-center network traffic prediction with elephant flows阅读笔记数据流$M$个大象流，1个总体流量，总共工$M+1$，加上进出两个方向，所以对于某个时间点其原始数据维度为$2M+2$，即$(ini,outi,eini1,eouti1,eini2,eouti2,…,einiM,eoutiM)$。然后将$k$步时间窗口内的每一个维度作为时间序列进行小波变换成$w+1$个组分，作为ANN的输入特征，此时共有$(w+1)(2M+2)$个维度（最终实验$w=1,M=5,k=$）。然后分别对某时间后的输入和输出流量分别进行预测。 IV. EXPERIMENTAL RESULTS 大象流5min取样一次，全部流量30s取样一次。 大象流占总流量的80%，取top-5个大象流应用。 一个输入层、一个隐藏层、一个输出层 ARIMA模型在短期预测（30s和1min）表现最好。]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>网络流量</category>
      </categories>
      <tags>
        <tag>论文阅读笔记</tag>
        <tag>网络流量</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Keras笔记]]></title>
    <url>%2F2017%2F04%2F30%2F2017-04-30-keras%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[新手指南 笼统的说，符号主义的计算首先定义各种变量，然后建立一个“计算图”，计算图规定了各个变量之间的计算关系。建立好的计算图需要编译以确定其内部细节，然而，此时的计算图还是一个“空壳子”，里面没有任何实际的数据，只有当你把需要运算的输入放进去后，才能在整个模型中形成数据流，从而形成输出值。 张量可以看作是向量、矩阵的自然推广，我们用张量来表示广泛的数据类型。张量的阶数有时候也称为维度，或者轴，轴这个词翻译自英文axis。譬如一个矩阵[[1,2],[3,4]]，是一个2阶张量，有两个维度或轴。 在Keras 0.x中，模型其实有两种，一种叫Sequential，称为序贯模型，也就是单输入单输出，一条路通到底，层与层之间只有相邻关系，跨层连接统统没有。这种模型编译速度快，操作上也比较简单。第二种模型称为Graph，即图模型，这个模型支持多输入多输出，层与层之间想怎么连怎么连，但是编译速度慢。可以看到，Sequential其实是Graph的一个特殊情况。在Keras1和Keras2中，图模型被移除，而增加了了“functional model API”，这个东西，更加强调了Sequential是特殊情况这一点。一般的模型就称为Model，然后如果你要用简单的Sequential，OK，那还有一个快捷方式Sequential。由于functional model API在使用时利用的是“函数式编程”的风格，我们这里将其译为函数式模型。总而言之，只要这个东西接收一个或一些张量作为输入，然后输出的也是一个或一些张量，那不管它是什么鬼，统统都称作“模型”。 第一种，遍历全部数据集算一次损失函数，然后算函数对各个参数的梯度，更新梯度。这种方法每更新一次参数都要把数据集里的所有样本都看一遍，计算量开销大，计算速度慢，不支持在线学习，这称为Batch gradient descent，批梯度下降。另一种，每看一个数据就算一下损失函数，然后求梯度更新参数，这个称为随机梯度下降，stochastic gradient descent。这个方法速度比较快，但是收敛性能不太好，可能在最优点附近晃来晃去，hit不到最优点。两次参数的更新也有可能互相抵消掉，造成目标函数震荡的比较剧烈。为了克服两种方法的缺点，现在一般采用的是一种折中手段，mini-batch gradient decent，小批的梯度下降，这种方法把数据分为若干个批，按批来更新参数，这样，一个批中的一组数据共同决定了本次梯度的方向，下降起来就不容易跑偏，减少了随机性。另一方面因为批的样本数与整个数据集相比小了很多，计算量也不是很大。基本上现在的梯度下降都是基于mini-batch的，所以Keras的模块中经常会出现batch_size，就是指这个。顺便说一句，Keras中用的优化器SGD是stochastic gradient descent的缩写，但不代表是一个样本就更新一回，还是基于mini-batch的。]]></content>
      <categories>
        <category>类库</category>
        <category>语言</category>
        <category>Python</category>
        <category>Keras</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Kreas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neural Relation Extraction with Selective Attention over Instances 阅读笔记]]></title>
    <url>%2F2017%2F04%2F21%2F2017-04-21-NREwithSelectiveAttentionoverInstances%2F</url>
    <content type="text"><![CDATA[Abstract 远程监督的关系提取（Distant supervised relation extraction）已经被广泛的使用来发现新的关系了，但主要问题是无可避免的引入了噪音，对关系提取的性能影响很大。 为了解决这个问题，我们提出了句子级别的attention-based模型(sentence-level attention-based model)，本文中使用CNN来编码句子语义，并在多个实例上应用了sentence-level attention，以期能够动态的减少噪音的权值。 实验结果显示，我们的方法可以保留所有信息并且有效的减少噪音的影响。 实验代码可以在https://github.com/thunlp/NRE找到. 1. Introduction 知识库knowledge bases (KBs) eg. Freebase (Bollacker et al., 2008), DBpedia (Auer et al., 2007) and YAGO (Suchanek et al., 2007)越来越多的被应用与NLP问题。 知识库KB主要是由三元组（e1,e2,relation）构成的，虽然这些关系库都很大了，但还远远不够大，所以想自动地提取关系。 (Mintz et al., 2009) 提出了distant supervision 通过对其KB和文本来自动的生成训练数据。 假设如果在KB中两个实体有某种关系，则认为在所有包含这两个实体的句子中也表达了这种关系。==&gt; 很容易导致错误标注。 (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) 使用多实例学习(multi-instance learning)减少错误标注的影响。这些方法的问题在于他们使用了NLP工具例如POS标注等，工具产生的误差会传导到分类中。 (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015)尝试在RE中使用DNN而不是人工提取特征，但这些工作是基于标注好的语句上的，没法大规模扩展到KBs上。 (Zeng et al., 2015) 之后将 DS multi-instance learning 应用到了CDNN中。 该方法假设包含两个实体的句子中至少有一个表示了他们的关系，然后只选择这个最可能的句子来作为实体对的训练和预测样本。 本文提出了一个句子级别的attention-based的CNN来做远程监督的关系提取（sentence-level attention-based convolutional neural network (CNN) for distant supervised relation extraction），如下图所示。图中$x_i$代表一个原始的句子，$X_i$代表表示这个句子的向量，$\alpha_i$代表第i个句子的权值，即Attention模型，$s$代表最终形成的训练样本。 创新点 与现有的NRE模型比较，我们的模型能够利用所有包含有效信息的句子。 为了减少远程学习中错误标注的问题，我们使用selective attention来降低噪音的影响。 在实验中，我们验证了selective attention 对NRE问题中的两种 CNN 模型都有效。 2. Related Work 关系抽取Relation extraction 和 远程监督 distant supervision 监督学习需要大量标注好的训练样本 ==&gt; (Mintz et al., 2009)提出远程监督（distant supervision），将文本和freebase对齐。==&gt; 出现了错误标注问题。 为了解决错误标注问题，(Riedel et al., 2010)提出了multi-instance single-label learing，(Hoffmann et al., 2011; Surdeanu et al., 2012) 提出了multi-instance multi-label learning。 (Zeng et al., 2015)将 multi-instance learning 和 CNN 和 DS 结合起来 多实例学习Multi-instance learning 基本原理就是考虑没有示例的可靠性（the reliability of the labels for each instance）。 最开始是为了解决训练数据的歧义标注问题ambiguously-labelled training data when predicting the activity of drugs (Dietterich et al., 1997) 之后(Bunescu and Mooney, 2007)将弱监督学习（weak supervision）和多实例（muti-instance）结合起来应用于关系提取。 但是这些基于特征的方法都依赖于NLP工具提取的特征，带来了错误的传递。 深度学习deep learning (Bengio, 2009)已经应用于很多其他的NLP问题 词性批注 part-of-speech tagging (Collobert et al., 2011), 语义分析 sentiment analysis (dos Santos and Gatti, 2014) 语法分析 parsing (Socher et al., 2013), 机器翻译 machine translation (Sutskever et al., 2014) 关系提取 使用RNN自动提取特征(Socher et al., 2012)，首先构建语法树然后把树上的节点用向量表示。 关系提取 (Zeng et al., 2014; dos Santos et al., 2015)使用CNN来做RE (Xie et al., 2016) 尝试把文本信息包含进关系提取中。 基于注意力的模型 attention-based models attention-based modes的权重可以使用多种方式学习。 图像分类 image classification (Mnih et al., 2014), 语音识别 speech recognition (Chorowski et al., 2014), 图像注释生成 image caption generation (Xu et al., 2015), 机器翻译 machine translation (Bahdanau et al., 2014). 3. Methodology概述：给定一个句子集合$S=\{x_1,x_2,··· ,x_n\}$和两个对应的实体，我们想要评价对于每一个可能关系$r$的概率。分为两个部分： Sentence Encoder : 给定一个句子${x}$和两个对应实体，通过一个CNN来构建一个语句向量$X$。 Selective Attention Over Instances。 3.1 Sentence Encoder 3.1.1 Input Representation 输入是raw句子$x$，类似上一篇文章(Zeng et al., 2014)，同时使用单词向量WF和位置向量PF。 单词向量WF $w$ ：提取单词的语义和语法信息，维度是$d^a$ 位置向量PF : 靠近实体的通常更有意义。维度是$d^b x 2$ 总长度为 $d$ $d=d^a+d^b$ 3.1.2 Convolution, Max-pooling and Non-linear Layers 在关系提取这个问题中，一个主要的挑战是句子长度是变换的，重要的信息可能出现在句子的任何地方。我们应该应用局部特征（Local Features）然后做一个整体的预测（Prediction globally），本文使用卷积层来合并所有的这些特征。 卷积层首先提取长度为 $l$ 的滑动窗口内的局部特征(Local features)，然后使用max-pooling操作提取出一个大小固定的向量。 使用$W$来表示卷积矩阵，其维度为$d^cx(lxd)$,其中$d^c$是句子向量的长度。 使用$q_i$代表第$i$个窗口的单词连接成的向量，超出范围的使用0向量扩展padding。 卷积第$i$个通道filter的计算就是 p_i = [Wq + b]_i其中$b$是偏置向量。 然后对每一个维度做max-pooling得到句子向量$x$ PCNN(Zeng et al., 2015)，是CNN的一种变体，就是用两个实体把句子分成三段，每一段有一个单独的卷积核$(p_{i1},p_{i2},p_{i3})$，然后再对每一段分别进行max-pooling操作，每个维度得到3个值然后连接扩展该维。 使用一个非线性激活函数$\tanh$ 3.2 Selective Attention over Instances 在包含两个相同实体的句子集合$S=\{x_1,x_2,··· ,x_n\}$表示成一个向量$s$，用来代表这个集合用来预测关系$r$。 $s$是$S$中句子向量的加权和。 s=\sum\limits_i {\alpha_i w_i}$\alpha_i$就是每个句子的权值，也正是本文中Selective Attention的含义。 本文使用两种方法来定义权值$\alpha$ 平均：Average Selective Attention计算公式为 \alpha_i = \frac{exp(e_i)}{\sum_k{exp(e_k)}}$e_i$是用来度量输入的句子$x_i$和预测的关系$r$匹配的得分，我们选择bilinear form（双线性形式）。 e_i = x_i A r$A$是一个weighted diagonal matrix，$r$是对应关系的向量表示。最后，我们使用softmax定义一个条件概率$p(r|S,\theta)$，公式为： p(r|S,\theta ) = \frac {exp(o\_r)}{\sum_{k=1}^{n\_r}{exp(o_k)}}$n_r$表示关系总数。 $o$代表神经网络的最后输出，定义为 o=Ms+d$d$ 是偏置向量， $M$ 是关系矩阵（representation matrix of relations） 即在最后添加一个softmax层。 3.3 Optimization and Implementation Details 使用交叉熵cross-entropy 作为目标函数。 使用随机梯度下降SGD最小化目标函数。 在输出层应用了 dropout (Srivastava et al., 2014)技术来防止过拟合，dropout就是在输出层定义一个概率$p$，把输出结果呈上按照概率为$p$的伯努利分布。所以，输出公式应该写为 o=M(s h)+d4. Experiments4.1 Dataset and Evaluation Metrics 使用(Riedel et al., 2010)开发的，(Hoffmann et al., 2011; Surdeanu et al., 2012)也使用的一个数据集。 这个数据集是将NYT和freebase对齐，使用Stanford named entity tagger (Finkel et al., 2005)来识别实体，然后和freebase中的实体进行对齐。 使用语料库中2005-2006年的句子作为训练集，使用2007年的作为测试集。 规模 共有53中关系，其中包含一种特殊的NA，表示两个实体没有关系。 训练集：522611个句子，281270个实体对，18252个关系实例 测试集：172448个句子，96678 个实体对，1950 个关系示例 使用held-out evaluation(可行性存疑) 将从语料库中语句提取的关系和相应实体对在freebase中的语句提取的关系进行对比。 基于这样一个假设：关系示例在freebase内外的结构应该是类似的。 提供了一个不用耗费时间和经历的近似评价。 在试验中使用 precision/recall 曲线和 recision@N (P@N) 评价。 4.2 Experimental Settings4.2.1 Word Embeddings使用google的word2vec训练NYT语料库，把出现次数大于100次的添加到词典中，同时把包含多个单词的实体连接起来，作为一个单词。 4.2.2 Parameter Settings 参数设置如下表所示 含义 符号 数值 窗口大小 Window size $l$ 3 句子向量维度 Sentence embedding size $d^c$ 230 词向量维度 Word dimension $d^a$ 50 位置向量维度 Position dimension $d^b$ 5 每批次大小 Batch size $B$ 160 学习速率 Learning rate $\lambda$ 0.01 Dropout概率 Dropout probability $p$ 0.5 4.3 Effect of Sentence-level Selective Attention 我们选择了CNN (Zeng et al.,2014)和PCNN(Zeng et al., 2015) 作为我们的句子编码器，并且自己实现了，然后得到了可以和我们方法比较的数据。 我们分别对CNN和PCNN应用ATT(Sentence-Level Attention)、AVE(Average)、ONE(at-least-one multi-instance learning (Zeng et al., 2015).)实验结果如图所示 从图中我们可以看出 不管是CNN还是PCNN，加了个ONE方法之后表现都比原来的好，因为原始的远程监督会包含很多噪音，损害了性能。 不管是CNN还是PCNN，加了AVE之后都是有益的，因为这些噪音相互抵消。 不管是CNN还是PCNN，ONE和AVE的性能差不多，因为AVE把每个句子看成相同的，还是引入了错误标记 不管是CNN还是PCNN，ATT方法都是最高性能的方法。ATT方法可以过滤出没有意义的语句，减少DS带来的错误标记的问题。 4.4 Effect of Sentence Number 句子数量的影响 在原先的测试集中，有74,857个实体对只有一个句子包含，接近总数的3/4。因为我们方法的有点主要在从多个句子中选择注意力，所以我们比较了CNN/PCNN+ONE, CNN/PCNN+AVE and CNN/PCNN+ATT 在不只有一个句子时的性能，我们使用了下面的三种设置。 One： 随机选择一个句子，使用这个句子来预测关系。 Two： 随机选择两个句子来预测关系。 All： 使用包含该实体对的所有句子来预测关系。注意：我们使用了所有的句子进行训练。我们分别使用100, 200, 300个测试。 从结果中我们可以发现 不管对CNN还是PCNN，在不同设置中，ATT方法都取得了最好的性能，证明我们的方法是有效的。 在使用One设置时，不管对CNN还是PCNN，AVE和ATT差不多， CNN+AVE和CNN+ATT比CNN+ONE在One的测试中都有5%-8%的提高，此时唯一的区别就是训练过程中有没有使用所有的样本，试验中证明使用所有的样本可以带来更多的信息。 不管是CNN还是PCNN，在Two和All中，ATT方法比另外两个高5%，9%。正说明把更多的句子考虑进来对关系提取很有帮助， 4.5 Comparison with Feature-based Approaches 选择3个基于特征的方法进行比较（都有代码） Mintz (Mintz et al., 2009)传统的DS模型 MultiR (Hoffmann et al., 2011)多实例概率图，并且处理了重叠关系 MIML (Surdeanu et al., 2012) 联合多例和多关系模型。 实验现实 CNN/PCNN+ATT 明显比基于特征的模型好很多。recall高于0.1时，基于特征的方法的准确率就快速下降了，而我们的方法直到recall到0.3的时候准确率还是比较高的。这证明人设计的特征不能很好的表达语句的意义，NLP工具带来的必然误差损害了关系提取的性能。相比之下，CNN/PCNN+ATT自动学习句子的表达可以很好地表示一个句子。 整个曲线，PCNN+ATT比CNN+ATT表现的都好很多。这说明selective attention 是考虑的所有句子的信息而不是每一个句子内部的信息。这说明如果换一个更好的sentence encoder会取得更好的结果。 4.6 Case Study 5. Conclusion and Future Works未来工作 我们的模型把multi-instance learning 和 neural network 通过 instance-level selective attention 结合起来，这不仅可以应用于远程监督的关系提取，还可以应用于其他的多实例学习任务。我们将会在其他领域例如文本分类拓展我们的模型。 CNN对于NRE来说是一个有效的工具。研究者也提出过很多的其他模型，将来我们将我们的instance-level selective attention和其他的模型集合起来。 符号表 含义 符号 数值 窗口大小 Window size $l$ 3 句子向量维度 Sentence embedding size $d^c$ 230 词向量维度 Word dimension $d^a$ 50 位置向量维度 Position dimension $d^b$ 5 每批次大小 Batch size $B$ 160 学习速率 Learning rate $\lambda$ 0.01 Dropout概率 Dropout probability $p$ 0.5 TODO [ ] 测试时以实体对为单位还是以句子为单位？]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>Relation Extraction</category>
      </categories>
      <tags>
        <tag>Relation Extraction</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Relation Extraction Perspective from Convolutional Neural Networks 阅读笔记]]></title>
    <url>%2F2017%2F04%2F17%2F2017-04-17-RelationExtractionPerspectivefromCNN%2F</url>
    <content type="text"><![CDATA[Abstract 使用多重窗口大小的卷积核multiple window sizes for filters 预先训练好的词向量作为非静态结构的初始值 pre-trained word embeddings as an initializer on a non-static architecture 1. Introduction2. Related Work3. Convolutional Neural Network for Relation Extraction我们的CNN模型包含的主要层 把句子中包含的单词查表找词向量 卷积层识别n-grams pooling层决定最相关的特征 一个逻辑回归层（softmax）来分类。(Collobert et al., 2011; Kim, 2014; Kalchbrenner et al., 2014) Word Representation符号表 符号 含义 $n$ relation mentions的长度 $x_i$ relation mention中的第i个词 $x_{i1} x_{i2}$ 两个实体 $X_i$ 句子中词向量表示 $X_i= [e_i,d_{i1},d_{i2}]$ $X$ 句子表示矩阵，维度$ (m_e + 2m_d) × n $ $e_i$ $x_i$的词向量，维度为$m_e$ $m_e$ 词向量的维度 $W$ 词向量字典 $d_{i1} d_{i2}$ 位置向量 $D$ 位置向量字典 维度是$(2n − 1) × m_d$ $w$ 窗口大小 $f$ filter matrix 也就是卷积矩阵 $f = [f_1,f_2,…,f_w] $ ，其中 $f_i$ 是一个维度为$m_e + 2m_d$的列向量。 augmented n-grams n-grams accompanied with relative positions of its words]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>Relation Extraction</category>
      </categories>
      <tags>
        <tag>Relation Extraction</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Relation classification via convolutional DNN 阅读笔记]]></title>
    <url>%2F2017%2F04%2F11%2F2017-04-11-RD_RelationClassificationviaCNN%2F</url>
    <content type="text"><![CDATA[作者：中科院赵军研究组年份：2014 Abstract 现在的先进方式是用统计机器学习，效果依赖于提取的特征，特征主要预先的NLP系统得到，导致了误差传导并阻碍了性能。 使用CDNN提取词语和句子级别的特征， 查表变换成vector，不是用POS、语法分析等其他手段，只用所有的词向量 词汇级别的特征通过给定的名词提取，句子级别的特征通过卷积方法提取 使用softmax对名词对进行分类预测 Introduction 定义：given a sentence S with the annotated（标注的） pairs of nominals e1 and e2, we aim to identify the relations between e1 and e2 (Hendrickx et al., 2010) 给定一个句子S及标注好的名词对e1 和 e1，我们的目标是明确e1 和 e2 的关系。 相关研究： 有监督学习 (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhou et al., 2005; Mintz et al., 2009) 基于特征的 基于核的 缺陷：误差传播和不准确(Bach and Badaskar, 2007) 使用CDNN提取特征的idea:Collobert et al. (2011) 做过POS tagging, chunking (CHUNK), Named Entity Recognition (NER) and Semantic Role Labeling (SRL)等任务 提出了position features (PF) 用来编码两个名词之间的相对距离（encode the relative distances to the target noun pairs） 使用SemEval-2010 Task 8 dataset测试 Related Work unsupervised方法，主要是利用上下文信息。 理论基础：分布假设理论，Distributional hypothesis theory (Harris, 1954)，有相同上下文的词有类似的含义。==&gt; 上下文结构相似的名词对有相似的关系。 Hasegawa et al. (2004) 使用层次聚类法（hierarchical clustering method），将名词上下文聚类，然后简单的选出上下文中出现最多的单词来表示这个关系。 Chen et al. (2005) 提出了基于 model order selection 和 discriminative label identification 来解决这个问题。 supervised方法，作为一个多类别分类问题 基于特征的(feature-based) 问题主要在于如何把结构信息转化成特征。 基于核的的(kernel-based) 多种多样的树convolution tree kernel (Qian et al., 2008), subsequence kernel (Mooney and Bunescu, 2005) and dependency tree kernel (Bunescu and Mooney, 2005) 主要问题是缺乏训练数据 Mintz et al. (2009) 提出了远程学习 distant supervision (DS) 来解决这个问题。DS就是利用知识库（knowledge base）中的知识作为训练的正样本。DS的主要问题是有wrong labels，容易导致噪音。为了解决这个问题，有下面的两个工作。 Riedel et al. (2010) 和 Hoffmann et al. (2011) 提出了the relaxed DS assumption as multi-instance learning Takamatsu et al. (2012) 指出 relaxed DS assumption在某些情况下可能会失效，并提出了一个生成模型（generative model）来进行 the heuristic（启发式的） labeling process. DNN的应用 word embeddings (Turian et al., 2010)词向量 Socher et al. (2012) 使用RNN在语法树上提取特征来做Relation Classification。 Hashimoto et al. (2013) 也使用了RNN来做Relation Classification。 MethodologyThe Neural Network Architecture 神经网络结构包含Word Representation, Feature Extraction and Output三层结构。如图所示。 Word Representation 单词表示直接使用了可免费获得的(Turian et al., 2010)的词向量。 Lexical Level Features 词语级别的特征 以前经常使用nouns themselves, the types of the pairs of nominals and word sequences between the entities，要从NLP工具中得到。 本文中只使用词向量和WordNet hypernyms of nouns , the WordNet hypernyms使用了MVRNN (Socher et al., 2012) Features Remark L1 Noun 1 L2 Noun 2 L3 Left and right tokens of noun 1 L4 Left and right tokens of noun 2 L5 WordNet hypernyms of nouns Sentence Level Features 句子级别的特征 虽然词向量已经能够很好的表示词语间的相似性，但无法提取到长距离的特征和语义合成性（semantic compositionality）。 本文中使用max-pooled convolutional neural network来提取句子级别的特征。 提取句子级别特征的框架如图所示。 Window Processing component部分，每一个单词进一步表示成Word Features (WF) 和 Position Features (PF) convolutional component. Sentence level features. 通过一个非线性变换（a non-linear transformation）tanh得到。 Word Features 单词特征就是直接把窗口内的词向量连接起来。 Position Features 位置特征 (Bunescu and Mooney, 2005) 使用过结构信息（tructure features） (e.g., 最短路径依赖the shortest dependency path between nominals)来解决Relation Classification。 本文中的PF指的是句子中每个单词到名词对（w1，w2）中的两个词的距离， PF = [d1,d2]，并且d1和d2也都被映射到一个$d_e$维的向量空间，和上面的WF连接起来一起投入到下面的CNN中。 Convolution 卷积 每个单词的特征向量只能表示它附近的上下文信息，对于Relation Classification，需要把整个句子都组合起来，自然的想到卷积方法 和Collobert et al. (2011)类似，首先把从window process中得到的向量做一个线性变换。 Z = W_1X其中，$X$是一个$n_0 \times t$ 的矩阵，$n_0 = w \times n$，$n$是每个单词特征向量的维度$[WF PF]$。$w$是窗口大小, $t$是输入句子中单词的个数。$W_1$是一个$n_1 \times n_0$ 的矩阵，$n_1$是第一个隐藏层的输出个数。输出$Z$是一个$n_1 \times t$的向量，使用max-pooling只记录影响最大的那个，如下式所示。 m_i = \max Z(i,\cdot) \qquad 0 \leq i \leq n1 然后我们获得了$m = \{m_1,m_2,···,m_n1\}$向量，与t没有关系了。 Sentence Level Feature Vector 句子级别的特征向量 添加一个全连接层，使用双曲正切tanh作为激活函数，tanh具有一个良好的性质，如下式所示，在BP算法时容易计算导数。 \frac{d}{dx}\tanh x = 1 - \tanh^2x 全连接层的表示为 g = \tanh(W_2 m)其中$W_2$是一个$n_2 \times n_1$的矩阵，$n_2$是本层的输出个数。 输出g是一个n2维的向量，可以认为是一个高级别向量（句子级别向量）— higher level features (sentence level features)。 output 输出 组合词语级别的向量l和上面得到的句子级别的向量g，得到向量f，$f = [l,g]$，其维度为$n_3$。 添加一个softmax层来计算最后的概率。 o = W_3 fp = softmax(o)其中$W_3$是一个$n_4 \times n_3$维的变换矩阵，n4是关系类别个数。o可以看作对应的类别的confidence score，最后使用softmax计算出条件概率。 Backpropagation Training BP训练 本文假设各个句子是相互独立的。 涉及的参数有$\Theta = (X,N,W_1,W_2,W_3)$，其中$N$是the word embeddings of WordNet hypernyms。 本模型就是输入一个句子s，输出其中名词对所属类别的概率。 目标函数是对数似然值log likelihood $J(\Theta)$, 使用随机梯度下降法（stochastic gradient descent (SGD)）进行训练。 $N, W_1, W_2, W_3$ 随机初始化， X 初始为词向量。 Dataset and Evaluation Metrics 数据集和评测 使用SemEval-2010 Task 8 dataset (Hendrickx et al., 2010)数据集（免费开放），其中包含10717个标注实例，有8000个训练样本和2717个测试样本，包含9个带方向的关系类型和1个不带方向的关系类型。 使用9个带方向的关系类型的the macro-averaged F1-scores来评测。 Experiments 实验做了三组实验： 测试三个超参数窗口大小w，第一个隐藏层神经元个数n1，第二个隐藏层神经元个数n2对性能的影响。 将本方法与其他传统方法的性能对比。 测试各个特征的影响。 Parameter Settings 参数设置 使用 5-fold cross-validation w，n1，n2太大的时候参数太多，容易over fitting，所以太大的时候性能差了。 heuristically（启发式的） choose $d_e$ = 5 词向量大小和学习速率与 Collobert et al. (2011)相同。 下表为超参数的值。 Hyperparameter Window size Word dim. Distance dim. Hidden layer 1 Hidden layer 2 Learning rate Value w =3 n =50 de =5 n1 =200 n2 =100 λ =0.01 Results of Comparison Experiments 对比实验结果 使用了7个对照组，前五个使用SVM和MaxEnt在Hendrickx et al. (2010)中出现，RNN使用Socher et al. (2012)中的模型，还有MVRNN。实验结果如表所示： Classifier Feature Sets F1 SVM POS, stemming, syntactic patterns 60.1 SVM word pair, words in between 72.5 SVM POS, stemming, syntactic patterns, WordNet 74.8 MaxEnt POS, morphological, noun compound, thesauri, Google n-grams, WordNet 77.6 SVM POS, prefixes, morphological, WordNet, dependency parse, Levin classed, ProBank, FrameNet, NomLex-Plus, Google n-gram, paraphrases, TextRunner 82.2 RNN - 74.8 POS, NER, WordNet 77.6 MVRNN - 79.1 POS, NER, WordNet 82.4 Proposed word pair, words around word pair, WordNet 82.7 结论 使用传统方法时，特征越多越有效。 使用RNN时，语法树处理时的误差阻止了性能的提高。MVRNN有效的提取了意义组合meaning combination，性能更好。 我们的模型最好。 The Effect of Learned Features 不同特征的影响 Lexical L1 34.7 +L2 53.1 +L3 59.4 +L4 65.9 +L5 73.3 Sentence WF 69.7 +PF 78.9 Combination all 82.7 Conclusion 总结TODO [ ] bag-of-words model [ ] WordNet hypernyms Question 到底在哪里使用了卷积？文中写卷积的地方是一个矩阵相乘啊 为什么要把PF映射成一个向量？]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>Relation Extraction</category>
      </categories>
      <tags>
        <tag>Relation Extraction</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VPS初始化设置]]></title>
    <url>%2F2017%2F04%2F09%2F2017-04-09-VPSsetting%2F</url>
    <content type="text"><![CDATA[安装常用软件1234567891011121314151617181920212223242526272829303132333435363738# 更换apt-get源为清华大学的源wget https://tuna.moe/oh-my-tuna/oh-my-tuna.pysudo python oh-my-tuna.py -y -g# 安装必备软件sudo apt-get updatefor i in 'openssh-server' 'git' 'vim' 'tree' 'htop' 'iotop' 'slurm'dosudo apt-get install -y $idone# 安装常用软件for i in 'gparted' 'sox'dosudo apt-get install -y $idone# 配置ipv6wget https://raw.githubusercontent.com/lennylxx/ipv6-hosts/master/hostssudo cat hosts &gt;&gt; /etc/hostswget https://raw.githubusercontent.com/cziszero/cziszero.github.io/master/uploads/netinterface_ensudo openssl aes-128-cbc -d -in netinterface_en &gt;&gt; /etc/network/interfacessudo /etc/init.d/networking restart# 安装chromewget -q -O - https://raw.githubusercontent.com/longhr/ubuntu1604hub/master/linux_signing_key.pub | sudo apt-key addsudo sh -c 'echo "deb [ arch=amd64 ] http://dl.google.com/linux/chrome/deb/ stable main" &gt;&gt; /etc/apt/sources.list.d/google-chrome.list'sudo apt-get updatesudo apt-get install -y google-chrome-stable#sudo apt-get install google-chrome-beta #安装Google Chrome unstable 版本#sudo apt-get install google-chrome-unstable # 安装Google Chrome beta 版本# 安装x11vnc 运行完5秒内会重启，放在最后wget https://raw.githubusercontent.com/cziszero/cziszero.github.io/master/uploads/ubuntu1604VNC.shchmod +x ubuntu1604VNC.shsudo ./ubuntu1604VNC.sh# 剩余未完成，以后有时间再写 安装JDK参考blog主要方法：下载甲骨文的JDK，解压，添加到PATH中即可。 使用samba，让ubuntu与windows实现文件共享未完成，按照教程配置之后windows无法访问，说无法访问到445端口，可能是被防火墙干掉了。参考连接1鸟哥的系统介绍 安装chrome浏览器这篇博客写的很好了 配置ipv6这篇blog已经写的非常好了，精简如下： 先去ipv6-test看是否支持ipv6 更新hosts文件为这个。windows上hosts的文件路径为%SystemRoot%\system32\drivers\etc\hosts，linux下为/etc/hosts。 使用IPv6 DNS服务器GoogleDNS服务器的IPv6地址是: 2001:4860:4860::8888 2001:4860:4860::8844windows上很容易配置，打开网络适配器找到ipv6的协议然后写上就可以了。ubuntu上，改/etc/resolv.conf文件重启服务会失效。这篇博客有改/etc/network/interfaces和改/etc/resolvconf/rosolv.conf.d/base两个文件两种方法，不过我试了一下，只有第一种方法可行。并且无法通过sudo /etc/init.d/networking restart重启服务，不过可以重启生效。 VNC x11vnc 远程控制，client和server使用同一个桌面，安装可以依照这个博客,博主还提供了一个自动安装的脚本，非常好用。 vncserver 新建一个桌面给client用 shell的使用 Ctrl＋Shift＋C＝复制，Ctrl＋Shift＋V＝粘贴 MATLAB文件目录位于/usr/local/MATLAB/R2017a/bin，可以将其添加到PATH中，每次开机可以直接从终端打开，方式是在~\.bashrc文件最后加一句export PATH=/usr/local/MATLAB/R2017a/bin:$PATH，然后运行source ~\.bashrc使其生效。 查看SATA控制器的类别1dmesg | grep SATA 更多 添加Swap分区首先输入lsblk查看连接的存储设备，然后假设为/dev/xvdf,则执行下面的操作。123sudo mkswap -f /dev/xvdfsudo swapon /dev/xvdffree -h #查看当前内存和swap情况 设置成开机生效sudo vi /etc/fstab,添加一行/dev/xvdf swap swap defaults 0 0 挂载一个磁盘首先输入lsblk查看连接的存储设备，然后假设为/dev/xvdf,则执行下面的操作。然后使用sudo file -s /dev/xvdf查看/dev/xvdf这个设备的情况。如果没有格式化就执行格式化操作。123sudo mkfs -t ext4 /dev/xvdf #格式化sudo mkdir ~/data #创建挂载点sudo mount /dev/xvdf ~/data #挂载 设置成开机生效sudo vi /etc/fstab，添加一行 /dev/xvdf ~/data ext4 defaults,nofail 0 2,然后运行mount -a检查是否有误。如果没有输出则正确。 更改目录或文件所有者sudo chown system_username /location_of_files_or_folders 更改目录或文件权限chmod abc file其中a,b,c各为一个数字，分别表示User、Group、及Other的权限。或者chmod o+w file用户身份主要有如下几类u：拥有文件的用户（所有者）；g：所有者所在的组群；o：其他人（不是所有者或所有者的组群）；a：每个人或全部（u、g、和o）。用户所具有的文件访问权限类型如下：r：读取权；w：写入权；x：执行权。文件权限配置行为有如下几类：+：添加权限；-：删除权限；=：使它成为惟一权限。 安装grive进行数据同步sudo add-apt-repository ppa:nilarimogard/webupd8 sudo apt-get update sudo apt-get install grive 注意，grive进行的是同步，而不能指定文件进行上传，要想达到这样的目的，需要新建一个文件夹，把要上传的东西放进去然后使用-u命令，只上传不下载。如果固定使用一个文件夹作为中转的话，需要在每次上传前删除.grive_state文件，否则会同步本地文件夹的删除操作。 matplotlibmatplotlib报QXcbConnection: Could not connect to display错误import matplotlib matplotlib.use(&#39;Agg&#39;) 使用use命令即可，必须放在调用pyplot前面，最好放在最前面，因为貌似numpy或者其他的什么包会设置。如果放在下面就会报UserWarning: This call to matplotlib.use() has no effect because the backend has already been chosen.除此之外，设置环境变量export QT_QPA_PLATFORM=offscreen好像能不完美的解决这个问题，依旧报warning，但可以生成图像。 Tensorflow安装NVIDIA Cuda Toolkit 和 cuDNN(未实践) Check NVIDIA Compute Capability of your GPU card Download and install Cuda ToolkitInstall version 7.5 if using our binary releases. Install the toolkit into e.g. /usr/local/cudaDownload and install cuDNNDownload cuDNN v5.Uncompress and copy the cuDNN files into the toolkit directory. Assuming the toolkit is installed in /usr/local/cuda, run the following commands (edited to reflect the cuDNN version you downloaded): tar xvzf cudnn-7.5-linux-x64-v5.1-ga.tgz sudo cp cuda/include/cudnn.h /usr/local/cuda/include sudo cp cuda/lib64/libcudnn /usr/local/cuda/lib64 sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn参考 python在virtualenv中使用ipython将alias ipy=&quot;python -c &#39;import IPython; IPython.terminal.ipapp.launch_new_instance()&#39;&quot;添加到~\.bashrc（ubuntu）中。详情见链接 python列出所有安装的模块pip list 或者 pip freeze python列出已导入的模块import sys sys.modules 更新使用anaconda 安装的tensorflow时出现Cannot remove entries…的问题更新tensorflow时出现Cannot remove entries from nonexistent file c:\program files\anaconda3\lib\site-packages\easy-install.pth错误。 原因：相应的清单文件叫做setuptools.pth，而非easy-install.pth 解决：把setuptools.pth复制一份改名成easy-install.pth就可以了。或者使用pip install --upgrade --ignore-installed tensorflow ssh安装参考这个博客ubuntu开启SSH服务 ssh允许使用密钥登陆这个博客设置 SSH 通过密钥登录写的很详细了。不过如果已经生成了密钥对，那就在~/.ssh/authorized_keys中添加id_rsa.pub的内容即可。并且不要忘记设置其访问权限为600。1234567891011121314151617181920212223# 设置允许密钥登陆mkdir ~/.sshcd ~/.sshcat &gt; authorized_keys &lt;&lt; EOFssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC4vLQK62esK6XZUicf+wuUulKi/tdlpfQ7wJ7qtcJ9y2aelpdHirPB0vztOTosS8EAFuuwnvdsISuZfYFwIsGZKqzjKCXDxl4+hEPsHTxovTPyGJglcRa5pQzJLCpTnLlLCoWpxXfdJ7N9nDys6K+ojpyVkuO5Xbq9HF6Vu8vJkZl8zlzGuS+3z8sOr03LWI9rO8DA+0wJRk4qVH1E89itj07BL/GhLoJLUR5Gqo+zca4EzUzSXx1/VS7jc17nuUCJddfNTBLLZ+IsKEcjWIQ+YFvQyji7Dmg7rLDqI5aWLJfxWosKc7BOhNKOpWplgYrGx05vMA/UsZtb7/a6m7sbEOFchmod 600 authorized_keyschmod 700 ~/.ssh# 设置ssh一直保持连接sudo cat &gt;&gt; /etc/ssh/sshd_config &lt;&lt;EOFClientAliveInterval 30ClientAliveCountMax 5EOFsudo cat &gt;&gt; /etc/ssh/ssh_config &lt;&lt;EOFServerAliveInterval 30 # 指定了客户端向服务器端请求消息的时间间隔ServerAliveCountMax 5ConnectTimeout 6000EOFservice sshd restart# 此时需要输入root的密码 使用密钥登陆服务器把私钥文件文件放在~/.ssh/目录即可，不忘记设置的存取权限是400 设置ssh超时sudo vim /etc/ssh/sshd_config添加如下两行 ClientAliveInterval 60 # 指定了服务器端向客户端请求消息的时间间隔, 默认是0, 不发送.而ClientAliveInterval 60表示每分钟发送一次, 然后客户端响应, 这样就保持长连接了. ClientAliveCountMax 5 #允许超时的次数 sudo vim /etc/ssh/ssh_config添加下面一行 ServerAliveInterval 60 # 指定了客户端向服务器端请求消息的时间间隔 ServerAliveCountMax 5 ConnectTimeout 6000然后重启服务 service sshd restart 查看公钥指纹ssh-keygen -l -E md5 -f ~\id_rsa.pub jupyter使用命令行导出pdf文件 jupyter nbconvert --to pdf xxx.ipynb 在windows上需要预先安装pandoc和miktex，在生成的时候自动选择的remote package repository可能总是下载不下来， 这时可以打开MiKTeX setting换一个。 在ubuntu上先安装了Texlive，但是生成pdf的时候还是报错，就没有再搞了，想想可以安装包含texpdf组件的其他工具。 安装jupyter notebook及设置允许远程访问 登陆远程服务器 生成配置文件jupyter notebook --generate-config 生成密码打开ipython，创建一个密文的密码： from notebook.auth import passwd passwd() Enter password: Verify password:输出例如’sha1:4e1cfb362357:ac6688741c2750bfd2bafa2371f22efc827e68b2’把生成的密文‘sha1:ce…’复制下来 配置SSL生成证书和密钥openssl req -x509 -nodes -days 365 -newkey rsa:1024 -keyout mykey.key -out mycert.pem 修改默认配置文件$vim ~/.jupyter/jupyter_notebook_config.py进行如下修改： 123456c.NotebookApp.certfile = u'/absolute/path/to/your/certificate/mycert.pem'c.NotebookApp.keyfile = u'/absolute/path/to/your/certificate/mykey.key'c.NotebookApp.ip='*'c.NotebookApp.password = u'sha:ce...刚才复制的那个密文'c.NotebookApp.open_browser = Falsec.NotebookApp.port =8989 #随便指定一个端口 启动jupyter notebook：$jupyter notebook 远程访问此时应该可以直接从本地浏览器直接访问http://address_of_remote:8888就可以看到jupyter的登陆界面。 建立ssh通道如果登陆失败，则有可能是服务器防火墙设置的问题，此时最简单的方法是在本地建立一个ssh通道：在本地终端中输入ssh username@address_of_remote -L127.0.0.1:1234:127.0.0.1:8989便可以在localhost:1234直接访问远程的jupyter了。 参考1:Running a notebook server参考2:远程访问jupyter notebook jupyter显示python终端实际打开是python3的解决办法 首先使用jupyter kernelspec list查看kernel的配置文件 进入安装内核目录打开kernel.json文件，查看Python编译器的路径是否正确，一般出现的原因是都是python,而系统中的python即为python3，只要把python改为python2就可以了。3.重启jupyter notebook即可 AWS CLI使用 使用前先配置运行aws configure命令，需要配置AWS Access Key ID,AWS Secret Access Key,Default region name,Default output format四项内容，第一次配置后以后可以直接输回车确认。访问密钥（访问密钥 ID 和秘密访问密钥）的是什么参考这里,如何获取参考这里 获取当前区的实例运行aws ec2 describe-instances 查看命令随便输错一个子命令即会列出所有命令的名字，之后搭配help使用即可。 关闭某个实例（stop）aws ec2 stop-instances --instance-ids i-xxxxxx 启动某个实例（start）aws ec2 start-instances --instance-ids i-xxxxxx git使用bitbucket的git lfs服务 安装Bitbucket LFS Media Adapter 安装git-lfs 使用教程]]></content>
      <categories>
        <category>软件安装和使用</category>
        <category>Linux</category>
        <category>初始设置</category>
      </categories>
      <tags>
        <tag>软件安装和使用</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow随笔]]></title>
    <url>%2F2017%2F04%2F09%2F2017-04-09-TensorflowTur%2F</url>
    <content type="text"><![CDATA[get started阅读笔记Getting Started With TensorFlow 一些包含contrib的方法是正在开发中的方法，可能会变。 概念 Graph 表示计算任务，在Session中执行Graph，Graph中的节点是op。 Variable维护状态，feed fetch赋值或取数。 tensor 存储数据，tensor的shape表示其大小，rank表示其维度。 使用update = tf.assign()更新变量，但需要tf.Session.run(update)之后才会真正更新。 op构造器的返回值是这个op的输出，可以传递给其他op构造器作为输入。 Session 对象在使用完后需要关闭以释放资源. 除了显式调用 close 外, 也可以使用 “with” 代码块 来自动完成关闭动作. with…Device 语句用来指派特定的 CPU 或 GPU 执行操作: with tf.Session() as sess: with tf.device(&quot;/gpu:1&quot;): matrix1 = tf.constant([[3., 3.]]) matrix2 = tf.constant([[2.],[2.]]) product = tf.matmul(matrix1, matrix2) ... 为了便于使用诸如 IPython 之类的 Python 交互环境, 可以使用 InteractiveSession 代替 Session 类, 使用Tensor.eval() 和 Operation.run() 方法代替 Session.run() . 这样可以避免使用一个变量来持有会话.（P24） 在执行前，Variable要先初始化，可以调用Variable.initializer.run()进行。或者使用tf.global_variables_initializer().run()进行初始化。 可以在sess.run()中加入多个tensor，然后就可以一次返回，即为fetch。代码如result = sess.run([mul, intermed]) feed机制用于临时代替图中的tensor，使用tf.placeholder()实现，在每次run()的时候要feed上数据，例如sess.run([output], feed_dict={input1:[7.], input2:[2.]}),run()结束后失效。使用tensor.eval(feed_dict={})也可以。 可以使用feed_dict代替任何tensor，而不仅仅是placeholder。 在创建模型之前，我们要先初始化权重和偏置，一般来说，初始化应加入轻微的噪声来打破对称性，防止0梯度的问题。 为了减少过拟合，我们在输出层之前加入 dropout。我们用一个 placeholder 来代表一个神经元在 dropout 中被保留的概率。这样我们可以在训练过程中启用 dropout，在测试过程中关闭 dropout。TensorFlow 的tf.nn.dropout操作会自动处理神经元输出值的scale。所以用 dropout 的时候可以不用考虑 scale。 tensorflow使用指定显卡及按需分配显存1234567# 声明可使用几号显卡import osos.environ['CUDA_VISIBLE_DEVICES'] = '0'# 显存按需分配config = tf.ConfigProto()config.gpu_options.allow_growth = Truesess = tf.Session(config=config) 非线性激活函数tensorflow中共有7中不同的非线性激活函数： tf.nn.relu tf.sigmoid tf.tanh 损失函数原则：损失函数的定义应该尽可能反应真实需求。 分类问题交叉熵Cross EntropyH(p,q)=-\sum_x{p(x)log{q(x)}}衡量了两个概率分布的距离，$p(x)$为真实概率，$q(x)$为预测概率。 回归问题均方误差MSEMSE(y,y')=\frac{1}{n}\sum_{i=1}^n{(y-y')^2}训练技巧学习速率设置 指数衰减Exponential Decaytf.train.exponential_decay(init_learning_rate, global_step, decay_steps, decay_rate, staircase=False, name=None)计算公式为: learning\_rate = init\_learning\_rate * decay\_rate^ {\frac{global\_step}{decay\_steps}}一般所有样本过一个轮次换一次learning rate。 获取所有op123456vs = []for i in [x for x in tf.get_default_graph().get_operations() ]: vs.append(i.name)vs = sorted(vs)for i in vs: print(i) APItf tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)产生一个截尾正态随机分布 tf.reshape(tensor, shape, name=None)将tensor转成shape的形状，shape中可以有一个维度是-1，此时由原tensor和其他的维度值共同计算出这个维度的值来。 tf.expand_dims(input, axis=None, name=None, dim=None)在input的第axis位置添加一个长度为1的维度 tf.name_scope(name, default_name=None, values=None)上下文管理器，定义op时使用 tf.variable_scope(name)完整的签名为tf.variable_scope(name_or_scope, default_name=None, values=None, initializer=None, regularizer=None, caching_device=None, partitioner=None, custom_getter=None, reuse=None, dtype=None, use_resource=None, constraint=None, auxiliary_name_scope=True)，作用即为声明一个变量的作用域。 tf.get_variable(name)完整的签名为tf.get_variable(name, shape=None, dtype=None, initializer=None, regularizer=None, trainable=True, collections=None, caching_device=None, partitioner=None, validate_shape=True, use_resource=None, custom_getter=None, constraint=None),需要在variable_scope上下文中声明是否reuse，with tf.variable_scope(&quot;foo&quot;, reuse=tf.AUTO_REUSE):可以这样写，找不到则创建一个，如果找到了则返回已经创建的。 tf.add_n(inputs, name=None)将inputs这个列表里面的tensor逐个相加 tf.control_dependencies(control_inputs)创建一个 context manager 来控制依赖关系 tf.clip_by_value(t, clip_value_min, clip_value_max, name=None)将t限制在[clip_value_min, clip_value_max]内。 tf.greater(x, y, name=None)逐元素计算x&gt;y，返回一个与x，y大小相同的bool张量。 tf.where(condition, x=None, y=None, name=None)根据condition逐元素的选择是x对应的值还是y对应的值。 tf.add_to_collection(name, value)将一些tensor加入名为name的collection，反向操作是tf.get_collection(key, scope=None) tf.nntf.nn.conv2d(input, filter, strides, padding)tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=True, data_format=&#39;NHWC&#39;, dilations=[1, 1, 1, 1], name=None)strides步长，第一维和最后一维必须为1。padding表示填充方式，’VALID’表示不填充，SAME表示填充0. tf.nn.max_pool/avg_pooltf.nn.max_pool(value, ksize, strides, padding, data_format=&#39;NHWC&#39;, name=None) max-pooling tf.nn.relu(features, name=None) 计算relu型激活函数 tf.nn.bias_add(value, bias)tf.nn.bias_add(value, bias, data_format=None, name=None)用于在经过卷积核后加上bias tf.nn.dropout(x, keep_prob, noise_shape=None, seed=None, name=None) 实现dropout技术。 tf.nn.softmax(logits, dim=-1, name=None) 实现softmax计算 tf.nn.softmax_cross_entropy_with_logits(_sentinel=None, labels=None, logits=None, dim=-1, name=None)接在softmax后面计算cross entropy。 tf.nn.zero_fraction(value, name=None) 计算value向量中0的比例，常用于评估稀疏状况 tf.nn.sparse_softmax_cross_entropy_with_logits(_sentinel=None, labels=None, logits=None, name=None)tf.nn.softmax_cross_entropy_with_logits针对非多标记问题的优化实现。 tf.nn.l2_loss(t, name=None)计算$output = sum(t^2)/2$，注意，这里没有开方。 tf.traintf.train.ExponentialMovingAverage(decay, num_updates=None, zero_debias=False)可以对一组var做指数滑动平均，对每个var维护一个shadow变量，算式为: shadow_var = decay * shadow_var + (1-decay)*var首先生成使用ema = tf.train.ExponentialMovingAverage(decay, num_updates=None, zero_debias=False)生成一个实例，然后使用op = ema.assign([va,vb...vc])指定要维护的变量列表，可以使用ema.average(va)指定要求滑动平均的变量。 tf.train.Saver tf.train.Saver(var_list=None, reshape=False, sharded=False, max_to_keep=5, keep_checkpoint_every_n_hours=10000.0, name=None, restore_sequentially=False, saver_def=None, builder=None, defer_build=False, allow_empty=False, write_version=2, pad_step_number=False)创建一个Saver，用于保存变量等 tf.train.AdamOptimizer `tf.train.AdamOptimizer.compute_gradients(self, loss, var_list=None, gate_gradients=1, aggregation_method=None, colocate_gradients_with_ops=False, grad_loss=None)minimize()的第一步，对loss中的var_list中的变量计算梯度，返回(gradient, variable)列表 optimizer.apply_gradients(grads_and_vars, global_step=None, name=None)minimize()的第二步，根据上一部中计算出的梯度更新参数值。 tf.contrib.layerstf.contrib.layers.l1_regularizer(scale, scope=None)返回一个可以计算给定参数L1正则的函数，与之类似的还有tf.contrib.layers.l1_regularizer(scale, scope=None) 和 tf.contrib.layers.l1_l2_regularizer(scale_l1=1.0, scale_l2=1.0, scope=None)。与tf.nn.l2_loss(t)不同的是：这个返回一个函数，用于计算带系数的L2正则项，L2正则的计算完全相同。]]></content>
      <categories>
        <category>类库</category>
        <category>语言</category>
        <category>Python</category>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[chrome使用]]></title>
    <url>%2F2017%2F04%2F09%2F2017-10-11-chromeuse%2F</url>
    <content type="text"><![CDATA[查看dnschrome://net-internals/#dns]]></content>
      <categories>
        <category>软件安装和使用</category>
      </categories>
      <tags>
        <tag>软件安装和使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS224d 笔记]]></title>
    <url>%2F2017%2F03%2F20%2F2017-03-20-CS224d_lecture1%2F</url>
    <content type="text"><![CDATA[Lecture 1What’s Deep Learnging? Representation learning Representation learning attempts to automatically learn good features or representations Deep Learning algorithms attempt to learn(multiple levels of) representation and an output. 突破 for Speech Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition for Computer Vision ImageNet Classification with Deep Convolutional Neural Networks Lecture 2 Word VectorHow do we represent the meaning of a word? 如何定义一个单词的含义？对照表 synonym 同义词 sentiment 情绪 nuance 细微差别 subjective 主观的 atomic 原子的 dimensionality 维数 cooccurrence 共现关系 latent 潜在的 least squares 最小二乘法 Singular Value Decomposition(svd) 奇异值分解 Pearson correlation coefficient 皮尔森相关系数，即相关系数 quadratically 二次的，$N^2$的，表示复杂度 Objective function 目标函数 tangent line 切线 derivative 导数 drive 导出 transpose 转置 monotonically 单调的]]></content>
      <categories>
        <category>公开课</category>
        <category>CS224d</category>
      </categories>
      <tags>
        <tag>CS224d</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep Learning Tutorial李宏毅 笔记]]></title>
    <url>%2F2017%2F03%2F19%2F2017-03-19-DeepLearningTutorialLHYNOTE%2F</url>
    <content type="text"><![CDATA[Introduction of Deep LearningIntroduction Three Steps for Deep Learning define a set of function (Neural Network) goodness of function pick the best function Neural Network Neuron Softmax layer as the output layer Gradient Descent Gradient descent may not guarantee global minima Backpropagation(反向传播算法):an efficient way to compute ${\partial}L/{\partial}W$ Tips for Training Deep Neural Network Choosing proper loss When using softmax output layer, choose cross entropy Mini-batch Mini-batch has better performance and accuracy New activation function ReLU Maxout Adaptive Learning Rate Popular &amp; Simple Idea: Reduce the learning rate by some factor every few epochs. Learning rate is smaller an smaller for all parameters Smaller derivatives, larger learning rate, and vice versa Adagrad [John Duchi, JMLR’11] RMSprop Adadelta [Matthew D. Zeiler, arXiv’12] “No more pesky learning rates” [Tom Schaul, arXiv’12] AdaSecant [Caglar Gulcehre, arXiv’14] Adam [Diederik P. Kingma, ICLR’15] Nadam Momentum Still not guarantee reaching global minima, but give some hope Panacea for Overfitting have more training data Create more training data Early Stopping Regularization Weight Decay Dropout Dropout is a kind of ensemble More reference for dropout [Nitish Srivastava, JMLR’14] [Pierre Baldi,NIPS’13][Geoffrey E. Hinton, arXiv’12] Dropout works better with Maxout [Ian J. Goodfellow, ICML’13] Dropconnect [Li Wan, ICML’13] Dropout delete neurons; Dropconnect deletes the connection between neurons Annealed dropout [S.J. Rennie, SLT’14] Dropout rate decreases by epochs Standout [J. Ba, NISP’13] Each neural has different dropout rate Network StructureVariants of Neural NetworkConvolutional Neural Network(CNN)Why CNN for Image? Some patterns are much smaller than the whole image The same patterns appear in different regions Subsampling the pixels will not change the object 1.2. ==&gt; Convolutional3 ==&gt; Max Pooling Recurrent Neural Network(RNN) Long Short-term Memory (LSTM) Backpropagation through time (BPTT)RNN Learning is very difficult in practice Gated Recurrent Unit (GRU):simpler than LSTMNext WaveSupervised Learning Ultra Deep Network Attention Mode.ecm.mp4/index.html)Reinforcement LearningLectures of David SilverDeep Reinforcement LearningUnsupervised Learning 进一步阅读Keras GithubKeras DocumentsMNIST Datahttp://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>论文阅读笔记</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Python基于SMTP发送邮件]]></title>
    <url>%2F2017%2F03%2F15%2F2017-03-15-%E4%BD%BF%E7%94%A8Python%E5%9F%BA%E4%BA%8ESMTP%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[本篇blog引用自这里，此处仅作备份。 在这篇笔记的最前面，我一定要说，网络协议的设计者都是天才。 使用SMTP发送邮件其实十分简单，就如同我们和人交谈别无二致。这里我们使用SMTP协议发送一封邮件，小小体会一下网络协议之美。我在这里使用Windows平台上的VS2013 作为编码环境，使用C++，和WinSock中的socket函数通信。 什么是SMTP，用Python中的SMTP模块发送一封邮件SMTP—Simple Mail Transfer Protocol,简单邮件传输协议。这是一个应用层协议，我们可以使用此协议，发送简单的邮件。SMTP基于TCP协议，在不使用SSL，TLS加密的SMTP协议中，我们默认使用端口号25，在使用SSL\TLS的SMTP协议中，使用端口号465\587。 一次传输邮件的过程，其实就是一次和服务器对话的过程。为了标识这些对话中的各种动作，我们需要使用语言来和服务器沟通。例如客户端发送给服务器一条信息EHLO（Hello），服务器就知道这个客户端要给我发邮件了，这类似于我们人与人之间打招呼，我和服务器说:我要发邮件了！！！，于是服务器知道我要发邮件了，会给我回答一声：发吧。在SMTP协议中，也是同样，协议会返回一个标识码，来告诉我们服务器现在的状态，在我们打招呼之后，服务器一般会返回250，这告诉我们：“一切OK，放马过来吧”。 而这背后的数据传输，不是我们应用层协议需要关心的了，一般，我们可以使用socket传输信息。 在Python中，我们可以很轻松的发送一封邮件，Python中内置了SMTP模块，下面的代码演示了Python中发送一封邮件的过程：123456789101112131415161718192021222324import smtplibfrom email.MIMEMultipart import MIMEMultipartfrom email.MIMEText import MIMETextmsg = MIMEMultipart()msg['From'] = 'me@gmail.com'msg['To'] = 'you@gmail.com'msg['Subject'] = 'simple email in python'message = 'here is the email'msg.attach(MIMEText(message))mailserver = smtplib.SMTP('smtp.gmail.com',587)# identify ourselves to smtp gmail clientmailserver.ehlo()# secure our email with tls encryptionmailserver.starttls()# re-identify ourselves as an encrypted connectionmailserver.ehlo()mailserver.login('me@gmail.com', 'mypassword')mailserver.sendmail('me@gmail.com','you@gmail.com',msg.as_string())mailserver.quit() 我们用email模块构建一个我们的邮件，然后，使用SMTP连接服务器，如果我们需要SSL或者TLS加密（并且服务器还支持的话，或者服务器强制要求开启TLS），可以看到，使用12345在Python中，一切都是这么简单，但这也隐藏了很多SMTP的细节，在下面的部分，我们会实现一个SMTP的客户端，我们首先不支持SSL，这会使我们的代码量迅速膨胀，我们在这里仅仅实现SMTP的内核。### 实现一个SMTP客户端我们首先浏览一遍客户端与服务器之间交互的流程： S: 220 smtp.example.com ESMTP PostfixC: HELO relay.example.orgS: 250 Hello relay.example.org, I am glad to meet youC: MAIL FROM:&#x62;&#x6f;&#x62;&#64;&#101;&#x78;&#97;&#x6d;&#112;&#x6c;&#x65;&#x2e;&#111;&#x72;&#x67;S: 250 OkC: RCPT TO:&#x61;&#108;&#105;&#x63;&#x65;&#64;&#101;&#x78;&#x61;&#x6d;&#x70;&#x6c;&#x65;&#x2e;&#x63;&#x6f;&#x6d;S: 250 OkC: RCPT TO:&#116;&#x68;&#x65;&#x62;&#111;&#x73;&#x73;&#64;&#101;&#x78;&#x61;&#109;&#x70;&#108;&#101;&#46;&#x63;&#x6f;&#x6d;S: 250 OkC: DATAS: 354 End data with .C: From: “Bob Example” &#x62;&#111;&#x62;&#x40;&#x65;&#120;&#97;&#109;&#112;&#108;&#x65;&#x2e;&#111;&#x72;&#x67;C: To: “Alice Example” &#x61;&#108;&#105;&#99;&#x65;&#x40;&#x65;&#120;&#97;&#109;&#112;&#108;&#101;&#46;&#x63;&#x6f;&#109;C: Cc: theboss@example.comC: Date: Tue, 15 January 2008 16:02:43 -0500C: Subject: Test messageC:C: Hello Alice.C: This is a test message with 5 header fields and 4 lines in the message body.C: Your friend,C: BobC: .S: 250 Ok: queued as 12345C: QUITS: 221 Bye{The server closes the connection}1234567891011121314151617181920212223C代表客户端，S代表服务器端。我们可以看见的是，发送邮件的过程就是一个不断的对话的过程，C发送指定的指令，S接受，返回指定的指令，如此反复，在客户端，我们非常清楚，在整个邮件发送过程中发生的一切。我们就按照这个流程来和服务器对话。我们首先定义一个类，```SMTPClient```，可想而知的是，类之中肯定有上面Python代码中的那几个函数，所以，我们的类先定义成下面的这个样子：``` cppclass SMTPClient&#123;private: SOCKET m_smtpSocket; SOCKADDR_IN m_ServerAddr; ... ...public: ~SMTPClient(); void sayHello(); void connectServer(std::string server, int port); void login(std::string username, std::string password); void send(std::string from, std::string to, std::string content); ... ...&#125;; 想要发送一封邮件，首先我们需要连接邮件服务器。我们在这里使用socket连接服务器，至于邮箱服务器，到各个邮箱的web版的帮助中心都能得到。 使用socket连接服务器如下，windows的socket编程，首先我们需要include两个头文件：123#include &lt;WinSock2.h&gt;#pragma comment(lib, "ws2_32.lib") 这之后，我们可以使用socket连接服务器了，这和连接其他的服务器没有任何区别。 1234567891011121314151617181920212223242526272829303132333435363738void SMTPClient::connectServer(std::string server, int port)&#123; //初始环境创建 WSADATA wsaD; WSAStartup(MAKEWORD(1, 1), &amp;wsaD); //建立套接字 m_smtpSocket = socket(AF_INET, SOCK_STREAM, IPPROTO_TCP); if (INVALID_SOCKET == m_smtpSocket)&#123; std::cout &lt;&lt; "socket init failed" &lt;&lt; std::endl; WSACleanup(); &#125; m_ServerAddr.sin_family = AF_INET; //通过url得到server的IP地址 hostent* host = gethostbyname(server.data()); memcpy(&amp;m_ServerAddr.sin_addr.S_un.S_addr, host-&gt;h_addr_list[0], host-&gt;h_length); printf("IP of %s is : %d:%d:%d:%d", server.data(), m_ServerAddr.sin_addr.S_un.S_un_b.s_b1, m_ServerAddr.sin_addr.S_un.S_un_b.s_b2, m_ServerAddr.sin_addr.S_un.S_un_b.s_b3, m_ServerAddr.sin_addr.S_un.S_un_b.s_b4); //初始端口 m_ServerAddr.sin_port = htons((u_short)port); //连接SMTP服务器 while (connect(m_smtpSocket, (LPSOCKADDR)&amp;m_ServerAddr, sizeof(m_ServerAddr))) &#123; std::cout &lt;&lt; "connect failed!...reconnect after 2 seconds" &lt;&lt; std::endl; Sleep(2000); //closesocket(m_smtpHost); //WSACleanup(); &#125; //接收服务器数据&#125; 上面的代码主要展示了windows的socket编程，这其中还有一段IP获取的过程，代码比较清晰。在最后，其实服务器还会返回一段信息，我这里没有写出来，只需要调用12```QQ SMTP coremail server, port ......... 类似如此 在建立连接之后，我们需要发出我们的第一条指令123456789101112131415161718192021222324252627282930313233发送指令的思想很简单，效果却很动人，因为这就像对面有一个人和你对话一样,而无论是SayHello，还是发送信息，我们都只需要把信息压在字符串中，通过socket的```send```函数发送出去就OK，**而后面的函数，与SayHello的实现几乎同样，只不过是换了一个发送的字符串**：``` cppvoid SMTPClient::SayHello()&#123; if (!connected) &#123; throw SMTPError(UNCONNECTED); &#125; char ret[MAX_RESPONSE_LENGTH] = &#123; 0 &#125;; memset(ret, MAX_RESPONSE_LENGTH, 0); std::string buffer = &quot;EHLO &quot; + s_hostInfo; int i = send(m_smtpSocket, buffer.data(), buffer.size(), 0); if (i == SOCKET_ERROR) &#123; wprintf(L&quot;send failed with error: %d\n&quot;, WSAGetLastError()); closesocket(m_smtpSocket); connected = false; WSACleanup(); return; &#125; do &#123; i = recv(m_smtpSocket, ret, MAX_RESPONSE_LENGTH, 0); if (i &gt; 0) wprintf(L&quot;Bytes received: %d\n&quot;, i); else if (i == 0) wprintf(L&quot;Connection closed\n&quot;); else wprintf(L&quot;recv failed with error: %d\n&quot;, WSAGetLastError()); &#125; while (i &gt; 0); std::cout &lt;&lt; ret &lt;&lt; std::endl;&#125; 在与QQ服务器123456789```250 smtp.qq.comPIPELININGSIZE 52428800AUTH LOGIN PLAINAUTH=LOGINMAILCOMPRESS8BITMIMESTARTTLS 这些信息中，有几条非常的重要： 250：表示一切正常 AUTH LOGIN PLAIN，这一排表示了服务器支持的验证类型，一会我们说说这三种类型的验证 STARTTLS，开启SSL/TLS，我们需要在客户端openssl的支持，本文不会讨论SSL、TLS的实现，这起码还需要一整篇文章来说明编程细节。 AUTH在EHLO后，很重要的一点是认证，SMTP（RFC 2554）有三种认证方式： AUTH PLAIN AUTH CRAM-MD5 AUTH LOGIN 这三种方式唯一的区别就是，我们需要发送过去的认证报文是不一样的，必须严格按照这三种格式来走。 如果我们有12首先 AUTH PLAIN，我们需要发送给服务器的字符串为： “LOGIN PLAIN \0username\0password”1第二种，如果是AUTH CRAM-MD5，那么我们需要按照下面的流程发送指令： C: AUTH CRAM-MD5S: 334PENCeUxFREJoU0NnbmhNWitOMjNGNndAZWx3b29kLmlubm9zb2Z0LmNvbT4=C: ZnJlZCA5ZTk1YWVlMDljNDBhZjJiODRhMGMyYjNiYmFlNzg2ZQ==S: 235 Authentication successful.1就是说，我们需要先发送一条：```AUTH CRAM-MD5```,服务器会返回一段base64编码，我们称其为```C```，我们处理来自服务器的这个编码，返回给服务器一个处理后的结果，处理的流程如下： response = ‘’ c = base64.decode(C) response = user + ‘ ‘ + hmac.HMAC(password, c).hexdigest() response = base64.encode(response) 1最后是```AUTH LOGIN```，这是现在最常用的认证方法，这个方法按照下面的流程进行： C: AUTH LOGIN ZHVtbXk=S: 334 UGFzc3dvcmQ6C: Z2VoZWltS: 235 Authentication successful.1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283第一句```AUTH LOGIN ZDASDADH=```,后面的这段base64编码是username的编码，这之后，服务器会返回一个334，之后第二句话，我们发送password的base64编码，如果验证成功，服务器会返回235。在实现的过程中，我参考了Python的smtplib中login的编码实现，其实同SayHello函数一样，就是简单的```send```字符串（base64处理加密的字符串），```recv```服务器的应答，实现原理是同样的，在这里，Python的编码实现很漂亮，我给出Python的编码：```pythondef login(self, user, password): &quot;&quot;&quot;Log in on an SMTP server that requires authentication. The arguments are: - user: The user name to authenticate with. - password: The password for the authentication. If there has been no previous EHLO or HELO command this session, this method tries ESMTP EHLO first. This method will return normally if the authentication was successful. This method may raise the following exceptions: SMTPHeloError The server didn&apos;t reply properly to the helo greeting. SMTPAuthenticationError The server didn&apos;t accept the username/ password combination. SMTPException No suitable authentication method was found. &quot;&quot;&quot; def encode_cram_md5(challenge, user, password): challenge = base64.decodestring(challenge) response = user + &quot; &quot; + hmac.HMAC(pa ssword, challenge).hexdigest() return encode_base64(response, eol=&quot;&quot;) def encode_plain(user, password): return encode_base64(&quot;\0%s\0%s&quot; % (user, password), eol=&quot;&quot;) AUTH_PLAIN = &quot;PLAIN&quot; AUTH_CRAM_MD5 = &quot;CRAM-MD5&quot; AUTH_LOGIN = &quot;LOGIN&quot; self.ehlo_or_helo_if_needed() if not self.has_extn(&quot;auth&quot;): raise SMTPException(&quot;SMTP AUTH extension not supported by server.&quot;) # Authentication methods the server supports: authlist = self.esmtp_features[&quot;auth&quot;].split() # List of authentication methods we support: from preferred to # less preferred methods. Except for the purpose of testing the weaker # ones, we prefer stronger methods like CRAM-MD5: preferred_auths = [AUTH_CRAM_MD5, AUTH_PLAIN, AUTH_LOGIN] # Determine the authentication method we&apos;ll use authmethod = None for method in preferred_auths: if method in authlist: authmethod = method break if authmethod == AUTH_CRAM_MD5: (code, resp) = self.docmd(&quot;AUTH&quot;, AUTH_CRAM_MD5) if code == 503: # 503 == &apos;Error: already authenticated&apos; return (code, resp) (code, resp) = self.docmd(encode_cram_md5(resp, user, password)) elif authmethod == AUTH_PLAIN: (code, resp) = self.docmd(&quot;AUTH&quot;, AUTH_PLAIN + &quot; &quot; + encode_plain(user, password)) elif authmethod == AUTH_LOGIN: (code, resp) = self.docmd(&quot;AUTH&quot;, &quot;%s %s&quot; % (AUTH_LOGIN, encode_base64(user, eol=&quot;&quot;))) if code != 334: raise SMTPAuthenticationError(code, resp) (code, resp) = self.docmd(encode_base64(password, eol=&quot;&quot;)) elif authmethod is None: raise SMTPException(&quot;No suitable authentication method found.&quot;) if code not in (235, 503): # 235 == &apos;Authentication successful&apos; # 503 == &apos;Error: already authenticated&apos; raise SMTPAuthenticationError(code, resp) return (code, resp) 如果认证成功，我们就可以发送邮件的主题内容了，这段的实现方法和前面的实现原理都是类似的，发送，接收，发送，接收，我们能够体会到那种和服务器对话的过程。在实现中，我们可以发现，发送和接受，是可以用复用的函数进行实现的，所以在这之后的过程我们可以大概的写成下面的样子：123456_sendString(("MAIL FROM:&lt;" + m_fromAddr + "&gt;\r\n"));_sendString(("RCPT TO:&lt;" + m_toAddr +"&gt;\r\n"));_sendString("DATA\r\n");_sendString("From:\"Jason\"&lt;847383158@qq.com&gt;\r\nTo: \"Nick\"&lt;1835857335@qq.com&gt;\r\nSubject: " + subject + "\r\n\r\n" +content + "\r\n.\r\n");_sendString("QUIT\r\n"); 函数123456```DATA``` 指令，标识着正文数据的开始，注意在正文数据中，我们还有一个```From：....```，这个From是邮件接收方看到的那个发信人名称，不过，如果这个发信人From和前面的Mail From标签中的不一致，收信方会弹出警告。最后，终结正文使用标签```\r\n.\r\n```。使用```QUIT```结束这次会话，服务器还会返回数据,一般是： 221 Bye``` So cute ，我们始终希望计算机能像人一样工作，一样思考，在写程序时，与SMTP协议这之间的交互真真切切是激动人心的。 实际上，发送一封邮件没有这么简单，虽然这看起来已经有一些吓人（虽然很有趣），在实现SMTP客户端的过程中，我们还必须解决几个问题： SSL/TLS支持，Windows这一部分的编程略微复杂。但绝大多数的SMTP服务器必须使用SSL/TLS。 Multipurpose Internet Mail Extensions (MIME)，也就是附件，扩展文本的支持（HTML等） 而上述两点是我们在发送邮件的过程中不可或缺的。 如果我们需要的是一个邮件客户端呢？那么我们还需要POP3协议和IMAP。 网络协议，也能如此动人。]]></content>
      <categories>
        <category>语言</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SemEval2010Task8数据集介绍]]></title>
    <url>%2F2017%2F03%2F15%2F2017-03-15-SemEval2010Task8%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Task #8: Multi-Way Classification of Semantic Relations Between Pairs of Nominals训练集 Readme概述有8000个已经标注好实体的句子。一部分来自SemEval-1 Task #4 (Classification of Semantic Relations between Nominals)，其它的是本次从web上搜集的。 关系选择了9中关系，分别为(1) Cause-Effect(2) Instrument-Agency(3) Product-Producer(4) Content-Container(5) Entity-Origin(6) Entity-Destination(7) Component-Whole(8) Member-Collection(9) Message-Topic1-5 关系SemEval-1 Task #4中也有，详细说明在Task8_Relation*.pdf中也有。 数据格式DATA FORMAT例如 15 “They saw that the equipment was put inside rollout drawers, which looked aesthetically more pleasing and tidy.”Content-Container(e1,e2)Comment: the drawer contains the equipment, typical example of Content-Container; no movement 20 “”Any time,” he told her before turning to the boy who was in the desk next to him.”OtherComment: the desk does not contain the boy. 第一行包含一个标注好了实体的句子。（1） 实体按先后顺序使用标注（2） 9个有方向，Other无方向，所以一共19个类（3） 第三行是给人看的注释 评测测试集 Readme总结2717个从web上收集的句子，与8000个训练集中的句子不重叠。和SemEval-1 Task #4 (Classification of Semantic Relations between Nominals).中的数据也不重叠。 重要数据格式例如：8001 “The most common audits were about waste and recycling.”每一行包含一个标注好实体的句子，并且在前面有一个数字标注。实体使用标注，下标仅表明出现顺序。标注出来的对应于“Base NP”，比表明实体的full NP可能要小些。 评估任务是给一个句子，预测标注出来实体的关系。输出格式例如1 Content-Container(e2,e1)2 Other3 Entity-Destination(e1,e2)…随评测程序的发布还发布了一个格式检查器，在提交结果之前，参赛者要用它来检查一下。评测时使用出Other外的9个关系，分别计算F1然后平均。 测试过程分别使用TD1 training examples 1-1000TD2 training examples 1-2000TD3 training examples 1-4000TD4 training examples 1-8000进行训练后测试。在每个过程中可以使用训练集训练，进一步开发或者交叉验证，但不能使用该阶段训练集外的数据。训练集中的最后891个训练数据(examples 7110-8000)是从SemEval-1 Task #4 datasets，只包含关系1-5，因此关系类的分布是倾斜的。参赛者可以使用也可以不使用。外部资源的使用没有限制。 提交提交包含5个文件 NUS_TD1.txt NUS_TD2.txt NUS_TD3.txt NUS_TD4.txt NUS_description.txt前四个是相应的结果，最后一个描述个人信息 有用的链接Google group: http://groups.google.com.sg/group/semeval-2010-multi-way-classification-of-semantic-relations?hl=enTask website: http://docs.google.com/View?docid=dfvxd49s_36c28v9pmwSemEval-2 website: http://semeval2.fbk.eu/semeval2.php]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>Relation Extraction</category>
        <category>数据集</category>
      </categories>
      <tags>
        <tag>Relation Extraction</tag>
        <tag>数据集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WOJ水题指南]]></title>
    <url>%2F2017%2F03%2F15%2F2017-03-15-EasyProblemGuide%2F</url>
    <content type="text"><![CDATA[Difficulty 5 1206 数论,不过没大看懂 1170 简单排序 1290 简单排序 1315 快速幂 1059 动规 1204 找超过半数元素，但用排序的方法可过，用O(n)的方法反而不能过，奇怪 Difficulty 3 1142 高精乘 1010 数论问题，主要用到的知识是一个集合的子集数是2^n 1029 简单模拟 1034 数论问题，N刀可以把蛋糕切成几块 (m^3 + 5 * m + 6) / 6) 1074 模拟，数据很弱 1087 非常简单的计算几何 1145 模拟，找规律 1152 寻找第K大元素，数据很水 1171 简单递归，需要使用记忆化搜索 1186 简单模拟 1202 异或的应用，x xor x = 0 1203 经典的找超过半数元素 1313 有点技巧的模拟题，考察mod运算 Difficulty 1 1413 大水题 1296 不知道在说什么的一道题]]></content>
      <categories>
        <category>算法和数据结构</category>
      </categories>
      <tags>
        <tag>算法和数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[参数估计]]></title>
    <url>%2F2017%2F03%2F09%2F2017-03-09-ParameterEstimation%2F</url>
    <content type="text"><![CDATA[参数估计数理统计的基本问题就是根据样本所提供的信息，对总体的分布或者分布的数字特征等做出统计判断。参数估计是这样一类问题，其总体所服从的分布类型是知道的，而他的某些参数确实位置的。对于这一类问题，想要确定总体的分布，关键是构造合理的方法将这些参数估计出来。 点估计概念：构造一个合适的统计量并将其观察值作为参数的估计值。 矩估计法原理：当样本容量n趋向于无穷时，样本的r阶矩依概率收敛于总体的r阶矩。 方法：用样本的r阶矩代替总体的r阶矩。 注意： 期望不存在时无法使用。 有时矩估计的结果不唯一。此时的原则是尽量使用低阶矩。 最大似然估计法原理：随机实验时有若干个可能的结果，如果在一次试验中结果A发生，而导致结果A发生的原因很多，在分析导致结果A发生的原因时，我们更愿相信使结果A发生概率最大的原因为导致A发生的真实原因。 公式： 似然函数 $L(\theta;x_i)$ 最大似然估计：$max(L(\theta;x_i)) of \theta$ 似然方程：$d(ln(L(\theta)))=0$ 理解：似然函数可以看成有了结果（X）之后，此结果是由原因（$\theta$）导致的可能性的一种度量。 注意： 并不是任何情况下最大似然估计都存在，即使存在也未必唯一。 最大似然估计与矩估计的结果也不一定是相等的。常见的正态分布，泊松分布，伯努利分布，其最大似然估计和矩估计是一致的，均匀分布是不一致的。 最大似然估计的不变性:若$\hat{\theta}$是总体分布中未知参数$\theta$的最大似然估计，则$g(\hat{\theta})$也是$g(\theta)$的最大似然估计。 估计量的优良性准则相合性样本容量越来越大时，$\hat{\theta}$越来越接近$\theta$真值。 定义：n-&gt;oo时，$\theta$估计量依概率收敛于$\theta$真值，则称$\theta$估计量时$\theta$相合估计量（一致估计量）。 不变性：相合性具备不变性。 意义：估计所必须具备的最基本的性质。 无偏性$\hat{\theta}$的均值离$\theta$的真值越近越好，即$E(\hat{\theta})=\theta$。 定义：无偏估计量，渐进无偏估计量 不变性：无偏估计不具有不变性 意义：在大量重复该估计时，多次估计的平均值给出可以任意接近真值的估计，即无系统偏差（$E(\hat{\theta})-\theta$） 注意：可能没有无偏估计量，也可能有多个不同的无偏估计量。 有效性于C-R不等式$\hat{\theta}$围绕$\theta$真值波动的范围越小越好。即$D(\hat{\theta})$越小越好 定义： C-R不等式(Cramer-Rao)不等式：$D(\hat{\theta})&gt;=1/(nI(\theta))$。指明了$\hat{\theta}$方差的下界。 $I(\theta)$表示了单个样本$x_i$所包含的关于$\theta$的信息量，$nI(\theta)$则表示样本总信息量，随着样本容量n增大， 样本包含的总信息量成线性递增。称$I(\theta)$为费希尔信息量(Fisher information)。 有效估计量（到达下界），渐进有效估计量（趋向下界） 注意：可能不存在有效估计，此时所有无偏估计中方差最小的成为一致最小方差无偏估计 区间估计点估计实际上给出了未知参数的真值的近似值，但人们通常不以得到近似值为满足，还希望知道其误差，即要求更确切地知道近似值地精确程度（亦即近似值地取值范围）。故对未知参数$\theta$，除了求出他的点估计，我们还希望估计出一个范围，并希望知道这个范围包含未知参数真值地可靠程度。 概念：置信区间，置信度（置信水平） 含义：置信区间地长度刻画了区间估计地精度，置信度则表达了置信区间包含参数真值的可信度。 ————-以下内容未深入看————- 求法：枢轴量 正态总体均值与方差的区间估计 正态总体均值的区间估计 单个正态总体的情形 当 $\sigma^2$ 已知时$\mu$ 的区间估计 当 $\sigma^2$ 未知时$\mu$ 的区间估计 两个正态总体的情形 正态总体方差的区间估计 单侧置信区间]]></content>
      <categories>
        <category>课程</category>
        <category>概率论</category>
      </categories>
      <tags>
        <tag>概率论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Distant Supervision for Relation Extraction via Piecewise CNN论文阅读笔记]]></title>
    <url>%2F2017%2F03%2F09%2F2017-03-09-DSforREviaPCNN%2F</url>
    <content type="text"><![CDATA[Abstract DS的问题 错误标注 提取出的特征中的错误进行传导 新模型 当作一个多例问题（multi-instance），每一个标签实例带一个置信度 使用PCNN(Piecewise CNN)避免第二个问题，更改了网络结构（piecewise max pooling）而不是进行了特征工程，]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>Relation Extraction</category>
      </categories>
      <tags>
        <tag>论文阅读笔记</tag>
        <tag>Relation Extraction</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[假设检验]]></title>
    <url>%2F2017%2F03%2F09%2F2017-03-09-HypothesisTesting%2F</url>
    <content type="text"><![CDATA[基本任务是，在总体的分布函数完全未知或者只知形式不知参数的情况下，为了推断总体的某些性质，首先提出某些关于总体的假设，然后根据样本提供的信息对所提出的假设做出“是”或“否”的结论性论断。 假设检验的基本思想、概念与方法基本思想实际推断原理，即小概率时间在一次试验中不会发生。 概念 原假设$H_0$，备选假设$H_1$。 奈曼-皮尔逊范式(Neyman-Pearson):在控制第一类错误概率$\alpha$的前提下，尽量犯使第二类错误的概率$\beta$尽量小。 因此，原假设$H_0$使被保护的，只有条件足够强时才拒绝$H_0$,因此原假设的提出必须经过慎重考虑，一般情况下不可轻易拒绝。 显著性水平$\alpha$:在某个样本上做估计，犯第一类错误的概率$\alpha$称为该检验法$W$的显著性水平。 检验水平：显著性水平的上确界$sup\{\alpha\}$称为检验水平。 简单假设和复合假设：当$\Theta_0$或者$\Theta_1$为单点集即$H_0$或者$H_1$是一个分布而不是一族分布，则称为简单假设，否则称为复合假设。 参数假设检验和非参数假设检验：参数假设检验即对未知参数提出假设，再根据样本进行检验。非参数假设检验例如对未知分布的假设，检验两个分布是否相同（女票的问题即是这类问题，检验元话语在两个语料库中的分布是否相同以得到是否有使用差异的结论），是否独立等。 拒绝域（检验法）$W$。 即拒绝$H_0$的条件。一个拒绝域决定一个检验法，反之亦然。 检验统计量$T$。用于进行检验的统计量。选择统计检验量首先要知道其分布。 第一类错误和第二类错误。第一类错误即弃真，第二类错误即取伪。 方法临界值法步骤： 根据问题的实际情况，合理建立原建设$H_0$及备选假设$H_1$ 选定检验统计量$T$并分析拒绝域的形式 给定显著性水平$\alpha$，在$H_0$下求出临界值，确定出拒绝域$W$ 取样，根据样本观察值是否落入$W$中做出是否拒绝$H_0$的判断 p值法步骤 1~2步于临界值法相同。 计算满足$H_0$时,对于样本$T$的观察值$t_0$， 计算出$p={\sup \limits_{\theta\in{\Theta}_0} P_{\theta}(T&gt;t_0)}$ 对于给定的显著性水平$\alpha$，如果出现$p&lt;\alpha$，则拒绝$H_0$，否则接受。 直观含义$p$描述了原假设成立时，出现比观测到的情况更极端的概率。 检验方法对数似然比检验这个就是女票的问题。重新表述下问题：现有两个语料库A和B，现想要探究某个词w在语料库A和B中的分布是否相同。对数似然比的方法由Ted Dunning在Accurate Methods for the Statistics of Surprise and Coincidence中提出。下面重新回顾一下。Comparing Corpora using Frequency Profiling 对数似然比检验的概念对于两个假设可信程度的指标，一个比较自然的度量是两个假设的似然比。由此引出似然比检验的方法。对于在假设$H_0$和$H_1$上的似然比： $L_{\Theta_0}(x)=\sup_{\theta\in\Theta_0}f(x,\theta)$ $L_{\Theta_1}(x)=\sup_{\theta\in\Theta_1}f(x,\theta)$ 考虑其比值 ${\lambda}=\frac{L_{\Theta_0}(x)}{L_{\Theta_1}(x)}$ 如此比值较大，则说明真参数在$\Theta_0$内的可能性更大，我们倾向于则接受$H_0$，反之亦然。在实际中，常使用$L_{\Theta}(x)$代替分母${L_{\Theta_1}(x)$来方便计算。 此即为次然比检验的直观理解，详细定义参见附件 Accurate Methods for the Statistics of Surprise and Coincidence笔记 以前使用asymptotic normality assumptions（渐进正态分布假设），但该假设在rare events（罕见事件）中能力不足， 然而真实文本中有很多rare events（例如只出现了几次的词语）。相似比检验在相对较小的样本上有更好的结果。感觉应该是${\chi}^2$检验在rare events时出现问题，因为${\chi}^2$检验要求频数要&gt;5，总量要大于10(一说40)，下面列举了${\chi}^2$检验的前提：来源 Quantitative data. One or more categories. Independent observations. Adequate sample size (at least 10). Simple random sample. Data in frequency form. All observations must be used. 同时，根据${\chi}^2$检验的原理，即残差平方和，直观上来看${\chi}^2$检验也是可以应用与多种多样的分布，并不是只可以应用于正态分布。另外，为何残差平方和符合${\chi}^2$分布是没有解决的问题。 现假设整个语料库中单词w出现的次数为二项分布，每个词是否出现该单词w为伯努利分布。本假设成立基于另两个事实，即一、每个单词的出现显然不是独立的，但这种相互影响会随着距离的增加儿急剧减小。二、假设中某个单词的概率是不变的，这在一定程度上是不对的，因为单词的概率显然是领域相关的。但对于某个领域的语料库，我们是可以这样认为的。 二项分布在n-&gt;oo时与正态分布类似。文中认为$np(1-p)&gt;5$时，二项分布与正态分布已经相同。 二项分布的对数似然比$\lambda$具有一个重要的特性，即$-2\log{\lambda}$符合自由度为$\Theta$和$\Theta_0$的维度差的${\chi}^2$分布。这就是本文主要利用的思想。 总结本文描述了关于假设检验的一些概念和方法，主要介绍了对数似然比检验及其在检验某个单词在两个语料库中分布是否相同的应用，对假设检验的理解更加深刻了一下，假设检验产生的目的是为了验证观察是否具备某个性质以及在多大程度上具备，其方法是将这个问题转化为假设统计量的分布问题，预先知道假设统计量的分布特性，通过求解其观察值来得到其可信度，像${\chi}^2$检验是残差平方和除以np符合${\chi}^2$分布，本文提到的二项分布的对数似然比$\lambda$的函数$-2\log\lambda$也是符合${\chi}^2$分布。 参考 Accurate Methods for the Statistics of Surprise and Coincidence Comparing Corpora using Frequency Profiling 张伟平老师的课件Lec18 张伟平老师的课件Lec11 人大假设检验课件 复旦大学卡方检验 卡方检验 百度百科 二项分布、泊松分布、正态分布的关系]]></content>
      <categories>
        <category>课程</category>
        <category>概率论</category>
      </categories>
      <tags>
        <tag>概率论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[长久以来关于CRT显示器的一些问题]]></title>
    <url>%2F2017%2F01%2F24%2F2017-01-24-CRT%2F</url>
    <content type="text"><![CDATA[首先是原理就是电子束打在磷光材料上显色，这个早就知道了。然后我之前的主要问题是我知道三原色混合形成各种颜色，但如何让它显示不同的三原色来，是电子速度不一样还是什么原因。然后我知道电子速度是一样的，显示成不同的颜色是因为显色材料不一样，一样的电子束打上有的显成绿有的显成红有的显成蓝。然后就是有几个电子束呢，我看示意图都是只有一个电子枪。我想这里也有两种方法，一种就是的确只有一个电子束，然后在每个像素点处分别轰击三种鳞光材料构成一个像素点，或者就是发三个电子束，每个电子束分别只负责红蓝绿三种基色。然后实际是后一种方法，估计是第一种对频率要求太高了。然后有三个电子束，但是有几个电子枪呢，我看示意图都是只有一个电子枪。原来早期确实是三枪三束，后来技术发展能把三个电子束做在一个电子枪里。然后还有个问题，CRT有没有像素的概念呢？CRT也是有像素的概念的，因为显色材料的涂布就是按照像素来涂的，三原色并不是一层一层的涂布，因为电子束是相同的啊，要是一层一层的涂就没法分辨三种原色了。他有两种涂布方式，第一种就是一个像素里面放三种显色材料分成三个小点，还有一种就是竖直涂布，因为电子束是水平扫描的，这样横竖交叉也是像素点。你看一个像素都这么小，那三种显色材料挨的很近啊，对精度要求很高，要是电子束篇了本来应该打红色打到了蓝色上不就出错了。所以会在显像屏幕那里加个荫罩，就是很多小孔的一个钢板，对准了才能打进去。后一种涂布方式就用金属线就好，叫做栅状荫罩。最后一个问题是电子是从哪儿来的呀，然后打在了荧光材料上又到哪儿去了。我开始的时候傻了，以为有种特殊的材料能够释放电子，然后我就想你不断的放不断的放放完了咋办。。。后来才想起来电流就是电子的移动啊，高压发射出去就是电子束了，另一端再接个电极就又回去了呗（最后这一句是我猜的。。） 主要参考：阴极射线管wiki稀土发光材料CRT指南彩色电视机显像管及其显色原理彩色电视机显像管及其显色原理 离线单枪三束]]></content>
      <categories>
        <category>生活常识</category>
      </categories>
      <tags>
        <tag>生活常识</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于windows编译环境的一些东西]]></title>
    <url>%2F2016%2F09%2F09%2F2016-09-09-windows%E7%BC%96%E8%AF%91%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[关于windows编译环境的一些东西事情的起源是这样的，因为要做安全保密课的大作业，需要用到openSSL，于是下载了openSSL的源代码，下载下来发现没有make文件，需要先下个perl，然后就放弃了。找到了一个编译好的windows上可以用的库，然后想这下大功告成了吧，万万没想到啊，这才是苦难的开始。 首先是QT中引用库的配置，可以以交互的方式来添加库文件，即右键项目，选择添加库，然后发现有三个选项：内部库，外部库，系统库。鸟知道这三个啥区别啊，那就逐个点点试试呗。点内部库发现需要添加的是在项目目录中的库，点外部库发现是要添加项目目录外的库文件，不光要添加库文件，还要添加头文件，点系统库只让添加一个库，发现只要选一个库文件添加上就好。然后观察才pro文件中添加的信息发现并没有什么大的区别，只是库文件目录不大一样而已。只是增加几个项，INCLUDEPATH ，DEPENDPATH，LIBS。LIBS和INCLUDEPATH的含义很好理解，即库文件和包好目录，LIBS的实例如：LIBS += -LC:/OpenSSL-Win64/lib/ -llibeay32 可以观察出结构，即-L添加目录，-l添加文件。注意，库文件最后的.lib是要省略的，但前面的不能省略，另外目录分隔符是unix的格式。INCLUDEPATH的书写更简单，如：INCLUDEPATH += C:/OpenSSL-Win64/include ，这里还要吐槽一点，采用上面说的交互的方式添加库时会自动在目录前面添加$$PWD/，导致编译的时候找不到库目录，删掉就好了。另外，采用交互方式添加库是还让选择平台（win，linux，mac）和静态库还是动态库，以及debug模式还是release模式，经过观察发现，选为静态库的时候会加入PRE_TARGETDEPS一项的配置，但目前还不知道什么用。总之，经过实验发现，添加库的时候采用直接更改pro文件的方式就可以了，比较简单，如果有平台要求等可以用交互方式添加下看看格式。另外，还有QT的两个配置项（DEPENDPATH，PRE_TARGETDEPS）没有搞清楚其含义，留待以后查一下。 另外，关于QT中添加动态库有两种方式，显式添加手动解析和隐式链接。添加知道怎么在QT工程里添加库后，我兴奋的添加上，想马上就可以大展手脚了。然而。。下载下来的库目录是这样的 │ 4758cca.lib │ aep.lib │ atalla.lib │ capi.lib │ chil.lib │ cswift.lib │ gmp.lib │ gost.lib │ libeay32.lib │ nuron.lib │ padlock.lib │ ssleay32.lib │ sureware.lib │ ubsec.lib │ └─VC │ libeay32MD.lib │ libeay32MDd.lib │ libeay32MT.lib │ libeay32MTd.lib │ ssleay32MD.lib │ ssleay32MDd.lib │ ssleay32MT.lib │ ssleay32MTd.lib │ └─static libeay32MD.lib libeay32MDd.lib libeay32MT.lib libeay32MTd.lib ssleay32MD.lib ssleay32MDd.lib ssleay32MT.lib ssleay32MTd.lib[MT和MD]的区别前段时间我刚刚知道，然后排除带d后缀的是debug时使用的，然后还剩下了 │ 4758cca.lib │ aep.lib │ atalla.lib │ capi.lib │ chil.lib │ cswift.lib │ gmp.lib │ gost.lib │ libeay32.lib │ nuron.lib │ padlock.lib │ ssleay32.lib │ sureware.lib │ ubsec.lib └─VC │ libeay32MD.lib │ libeay32MT.lib │ ssleay32MD.lib │ ssleay32MT.lib └─static libeay32MD.lib libeay32MT.lib ssleay32MD.lib ssleay32MT.lib但还有上面的杂七杂八的库是干嘛用的，我看了下readme文档和帮助文档，也没说怎么使用，不过难不住会使用搜索引擎的我啊。经过一番搜索，发现在调用openSSL的时候只要连接libeay32和ssleay两个库就可以了。其中，libeay32主要用于加密解密相关功能，ssleay主要用于SSL相关功能。确定了这个，接下来只要确定QT使用的什么编译选项（MD/MT），然后在pro文件里添加上就行了呗，这时我的内心是充满希望的。确定QT使用的什么编译选项（MD/MT）怎么能难倒我呢，我一下就找到对应平台的qmake的配置文件（qmake现在也不大清楚，需要查一下），例如我的是在C:\Qt\Qt5.4.1\5.4\msvc2013_64_opengl\mkspecs\win32-msvc2013\qmake.conf，打开看了下是使用/MD的编译选项，然后我就把static文件夹下的libeay32MD.lib ssleay32MD.lib都给添加上了，然后去网上找了段代码，没添加库之前一大堆找不到符号，添加库之后只有三个找不到符号了。。。尼玛，就三个。。还是编译不过，我也不知道是什么问题，我以为是我还漏掉了哪个库文件。然后各种google，奈何别人好像都没碰到。。。不过在还是学到了一些新知识。前面有贴了下载下来的编译好的库目录，看到他是我还比较困惑为什么外层有.lib还有个内层的static文件夹存了一些.lib文件。lib不已经是静态链接库了么，那里面的和外面的有什么区别呢？以前也查过这方面的资料，但都是浅尝辄止了，没有真正认认真真学过，这次终于知道在windows以.lib结尾的并不一定都是静态链接库。首先库分为静态链接库和动态链接库两种，静态链接库是在编译链接的时候就和其他的打包在一起，链接好之后就可以单独运行了，而静态库也只需要提供一个lib库文件。但动态链接库在生成的时候还会伴随着产生一个lib文件，这个文件是动态链接库的索引文件，会比静态链接库产生的lib文件小很多。动态链接库是在运行时链接的，在build阶段link时并不会打包到一起，所以如果需要在别的地方运行还要把该dll考过去。在程序中使用动态链接库时，如果显式的引用动态链接库则不需要添加lib文件，但需要手动解析。如果隐式的使用动态链接库，则需要把索引用的lib文件添加在编译选项中，然后之后的加载由操作系统自动完成。[更多资料请参考] 然后一段时间过去了，我想不会是下载下来的这个编译好的包有问题吧。。。那我还是自己编译试试吧。然后我又去查了openSSL的编译方法，这个资料挺多的，例如这个。VS的目录结构cl的编译选项 如何在Windows下编译OpenSSL （VS2010使用VC10的cl编译器） 1、安装ActivePerl,初始化的时候,需要用到perl解释器 2、使用VS2010下的Visual Studio 2010 Command Prompt进入控制台模式 （这个模式会自动设置各种环境变量） 3、解压缩openssl的包,进入openssl的目录 4、perl configure VC-WIN32 尽量在这个目录下执行该命令，否则找不到Configure文件，或者指定完整的Configure文件路径。 5、ms\do_ms.bat,在解压目录下执行ms\do_ms.bat命令 6、nmake -f ms\ntdll.mak 7、nmake -f ms\nt.mak 测试动态库： nmake -f ms\ntdll.mak test 测试静态库： nmake -f ms\nt.mak test 安装动态库： nmake -f ms\ntdll.mak install 安装静态库： nmake -f ms\nt.mak install 清除上次动态库的编译，以便重新编译： nmake -f ms\ntdll.mak clean 清除上次静态库的编译，以便重新编译： nmake -f ms\nt.mak clean咦，当时找的时候不是找到的这篇博文，找到这篇博文的话可能还省点事，当时找到的第二步都是让在运行vcvarsall.bat x86，我也找到了这个文件，是在C:\Program Files (x86)\Microsoft Visual Studio 12.0\VC这个目录下，在这个目录下成功的执行 vcvarsall.bat x86，然后继续往后做，悲剧发生了。后面的第6步第7步不行，老是提示找不到include的文件，现在看来这很明显是没有配好环境嘛，但当时头脑有点混乱。算了算了，过去这么长时间了，当时的也记不大清楚了，就只写结果吧，本来还想写的生动有趣一点呢。。。 发现是环境没有配好后，我就开始探索VS的目录结构，毕竟这个已经困扰我好久了。首先在C:\Program Files (x86)目录下我一下就看到了Microsoft Visual Studio 11.0、Microsoft Visual Studio 12.0、Microsoft Visual Studio 14.0，然后知道这是不同的版本，其中11.0对应Visual Studio 2012，12.0对应Visual Studio 2013，14.0对应Visual Studio 2015，那我找到主版本就行了。我机子上安装的版本是2013。 感悟：想到就立即去做遇事要冷静，想想怎样做最优。 https://msdn.microsoft.com/en-us/library/x4d2c09s(VS.80).aspx版本号http://baike.baidu.com/view/261613.htm#6linux编译64位和32位的编译选项。]]></content>
      <categories>
        <category>软件安装和使用</category>
        <category>C/C++</category>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>软件安装和使用</tag>
        <tag>C/C++</tag>
        <tag>环境配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IPython的使用]]></title>
    <url>%2F2016%2F09%2F09%2F2016-09-09-IPython%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Ipython使用Magic命令IPython有一些特殊的命令。有的为常见任务提供便利，有的则使你能够轻松控制IPython系统的行为。分为行模式和cell模式，行模式使用%打头，cell模式使用%%打头。行模式即只有一行，cell模式即可以有多行，只有第一行需要%%当automagic打开时，行模式可以省略%，使用lsmagic列出所有可用magic命令。]]></content>
      <categories>
        <category>软件安装和使用</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>软件安装和使用</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[QFile 和 QTextStream 的问题]]></title>
    <url>%2F2016%2F03%2F12%2F2016-03-12-QT_Problem%2F</url>
    <content type="text"><![CDATA[QFile 和 QTextStream 的问题通常我们会这样使用(本例来源于文档)： QFile data(&quot;output.txt&quot;); if (data.open(QFile::WriteOnly | QFile::Truncate)) { QTextStream out(&amp;data); out &lt;&lt; &quot;Result: &quot; &lt;&lt; qSetFieldWidth(10) &lt;&lt; left &lt;&lt; 3.14 &lt;&lt; 2.7; // writes &quot;Result: 3.14 2.7 &quot; } 但有次因为需要多次读相同的结构，我就把QFile data作为局部变量，QTextStream out作为成员，这样导致的后果是前几个数据块读的很正常，但大概四五个之后就会出现内存错误。分析发现，QTextStream大概相当于一个修饰者类，真正执行I/O的还是QFile类，因为data是局部变量，所以函数退出后data就被销毁了，QIODevice没有了，所以后面再调用out读写的时候就会报内存错误。而之所以前面还执行了几个正常的读函数是因为QTextStream有一个自己的缓冲区，这样指针还在缓冲区内的时候就正常，超过缓冲区后就会报内存错。表现特征上面已经说了，这个问题应该会在多个有缓冲区的类中出现。还有就是对内存的分配和销毁要搞清楚，要不然还是容易出现这种问题。]]></content>
      <categories>
        <category>类库</category>
        <category>QT</category>
      </categories>
      <tags>
        <tag>QT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Qt学习：深拷贝&浅拷贝&隐式共享]]></title>
    <url>%2F2016%2F02%2F28%2F2016-02-28-QT_copy%2F</url>
    <content type="text"><![CDATA[本文档来自于这个网页 ,感谢原博主，仅做记录，以备查找。 Qt学习：深拷贝&amp;浅拷贝&amp;隐式共享综述： 当两个对象要共享一份数据时，如果数据不改变，不进行数据的复制，通过浅拷贝就可以数据的共享；而当对象需要改变数据时，则做深拷贝。程序在处理共享对象时，会使用浅拷贝和深拷贝这两种方法复制对象。 1.深拷贝：即就是生成对象的一个完整的复制品； 2.浅拷贝：只是一个引用复制(比如仅仅复制指向共享数据的指针)。这样看来，深拷贝其实是代价比较高的，要占用更多的内存和CPU资源；而浅拷贝的效率要更好些，因为它仅仅需要设置一个指向共享数据块的指针以及修改引用的计数器。 3.隐式共享：也叫做回写复制(copy on write)。隐式共享可以降低对内存和CPU资源的使用，提高程序的运行效率。使用隐式共享能使得在函数中(eg. 参数、返回值)使用值传递更有效率。 QString采用隐式共享技术，将深拷贝和浅拷贝很好地结合了起来。 举例说明隐式共享是如何工作的： QString str1 = &quot;ubuntu&quot;; QString str2 = str1;//str2 = &quot;ubuntu&quot; str2[2] = &quot;m&quot;;//str2 = &quot;ubmntu&quot;,str1 = &quot;ubuntu&quot; str2[0] = &quot;o&quot;;//str2 = &quot;obmntu&quot;,str1 = &quot;ubuntu&quot; str1 = str2;//str1 = &quot;obmntu&quot;, line1: 初始化一个内容为”ubuntu”的字符串； line2: 将字符串对象str1赋值给另外一个字符串str2(由QString的拷贝构造函数完成str2的初始化)。在对str2赋值的时候，会发生一次浅拷贝，导致两个QString对象都会指向同一个数据结构。该数据结构除了保存字符串“ubuntu”之外，还保存一个引用计数器，用来记录字符串数据的引用次数。此处，str1和str2都指向同一数据结构，所以此时引用计数器的值为2. line3: 对str2做修改，将会导致一次深拷贝，使得对象str2指向一个新的、不同于str1所指的数据结构(该数据结构中引用计数器值为1，只有str2是指向该结构的)，同时修改原来的、str1所指向的数据结构，设置它的引用计数器值为1(此时只有str1对象指向该结构)；并在这个str2所指向的、新的数据结构上完成数据的修改。引用计数为1就意味着该数据没有被共享。 line4: 进一步对str2做修改，不过不会引起任何形式的拷贝，因为str2所指向的数据结构没有被共享。 line5: 将str2赋给str1.此时，str1修改它指向的数据结构的引用计数器的值位0，表示没有QString类的对象再使用这个数据结构了；因此str1指向的数据结构将会从从内存中释放掉；这一步操作的结构是QString对象str1和str2都指向了字符串为“obmntu”的数据结构，该结构的引用计数为2. Qt中支持引用计数的类有很多(QByteArray, QBrush, QDir, QBitmap… …).]]></content>
      <categories>
        <category>类库</category>
        <category>QT</category>
      </categories>
      <tags>
        <tag>QT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[QPixmap类文档的部分翻译]]></title>
    <url>%2F2016%2F02%2F28%2F2016-02-28-QT_Trans_QPixmap%2F</url>
    <content type="text"><![CDATA[翻译详细介绍QPixmap是一个可以用来当作绘图设备的屏幕无关的图像表现类。 QT提供了QImage，QPixmap，QBitmap 和 QPicture四种处理图像数据的类。QImage用来处理I/O并且可以直接操作每个像素。QPixmap用来在屏幕上显示图像。QBitmap只是继承自QPixmap确保深度为1的方便使用的类。如果isQBitmap()返回true，则一个QPixmap对象确实是一个bitmap对象，否则返回false。QPicture类是用来记录和表现QPainter操作的绘画设备。 QPixmap可以使用QLabel或者QAbstractButton的一个子类方便的显示在屏幕上。QLabel有一个pixmap的属性，QAbstractButton有一个icon的属性。 QPixmap进行的是值传递，因为QPixmap使用隐式数据共享。可以通过隐式数据共享的文档来了解更多信息。QPixmap也可以流化。 注意，QPixmap的像素数据外部无法访问，有低层窗口系统直接操作。因为QPixmap是QPaintDevice的子类，因此QPainter能够直接在QPixmap上画。只能通过QPainter的函数或者转化为QImage来访问QPixmap的像素数据。但可以使用fill()函数来把整个QPixmap初始化为一种颜色。 可以使用函数在QImage和QPixmap之间互相转换。经常是使用QImage来载入图像文件并操作图像，然后转化为QPixmap来在屏幕上显示。但如果不需要操作图像，那么也可以直接使用QPixmap来读取图像文件。 QPixmap提供了一系列函数来获取图像的多种信息。还有一些函数提供了图像变换的功能。 读写图像文件QPixmap提供了好几种方式来读取图像文件。即可以在构造的时候载入，也可以使用load()或者loadFromData()载入，当载入一个图像的时候，即可使用真实的文件名，也可以资源文件中文件别名。可以看QT的资源系统(Qt Resource System)来获取更多信息。 可以使用save()函数保存图像文件。 支持文件格式的完整列表可以使用QImageReader::supportedImageFormats()和QImageWriter::supportedImageFormats()来获取。新的文件格式可以以插件的形式加载。默认情况下，QT支持下列文件格式。 获取Pixmap的信息图像变换与其他类转化原文Detailed DescriptionThe QPixmap class is an off-screen image representation that can be used as a paint device. Qt provides four classes for handling image data: QImage, QPixmap, QBitmap and QPicture.QImage is designed and optimized for I/O, and for direct pixel access and manipulation,while QPixmap is designed and optimized for showing images on screen.QBitmap is only a convenience class that inherits QPixmap, ensuring a depth of 1.The isQBitmap() function returns true if a QPixmap object is really a bitmap, otherwise returns false.Finally, the QPicture class is a paint device that records and replays QPainter commands. A QPixmap can easily be displayed on the screen using QLabel or one of QAbstractButton‘s subclasses (such as QPushButton and QToolButton).QLabel has a pixmap property, whereas QAbstractButton has an icon property. QPixmap objects can be passed around by value since the QPixmap class uses implicit data sharing.For more information, see the Implicit Data Sharing documentation.QPixmap objects can also be streamed. Note that the pixel data in a pixmap is internal and is managed by the underlying window system.Because QPixmap is a QPaintDevice subclass, QPainter can be used to draw directly onto pixmaps.Pixels can only be accessed through QPainter functions or by converting the QPixmap to a QImage.However, the fill() function is available for initializing the entire pixmap with a given color. There are functions to convert between QImage and QPixmap.Typically, the QImage class is used to load an image file, optionally manipulating the image data,before the QImage object is converted into a QPixmap to be shown on screen.Alternatively, if no manipulation is desired, the image file can be loaded directly into a QPixmap. QPixmap provides a collection of functions that can be used to obtain a variety of information about the pixmap.In addition, there are several functions that enables transformation of the pixmap. Reading and Writing Image FilesQPixmap provides several ways of reading an image file:The file can be loaded when constructing the QPixmap object, or by using the load() or loadFromData() functions later on.When loading an image, the file name can either refer to an actual file on disk or to one of the application’s embedded resources.See The Qt Resource System overview for details on how to embed images and other resource files in the application’s executable. Simply call the save() function to save a QPixmap object. The complete list of supported file formats are available through the QImageReader::supportedImageFormats() and QImageWriter::supportedImageFormats() functions.New file formats can be added as plugins. By default, Qt supports the following formats: || Format || Description || Qt’s support || || BMP || Windows Bitmap || Read/write || || GIF || Graphic Interchange Format (optional) || Read || || JPG || Joint Photographic Experts Group || Read/write || || JPEG || Joint Photographic Experts Group || Read/write || || PNG || Portable Network Graphics || Read/write || || PBM || Portable Bitmap || Read || || PGM || Portable Graymap || Read || || PPM || Portable Pixmap || Read/write || || XBM || X11 Bitmap || Read/write || || XPM || X11 Pixmap || Read/write || Pixmap InformationQPixmap provides a collection of functions that can be used to obtain a variety of information about the pixmap: Available Functions Geometry The size(), width() and height() functions provide information about the pixmap’s size. The rect() function returns the image’s enclosing rectangle. Alpha component The hasAlphaChannel() returns true if the pixmap has a format that respects the alpha channel, otherwise returns false. The hasAlpha(), setMask() and mask() functions are legacy and should not be used. They are potentially very slow.The createHeuristicMask() function creates and returns a 1-bpp heuristic mask (i.e. a QBitmap) for this pixmap. It works by selecting a color from one of the corners and then chipping away pixels of that color, starting at all the edges. The createMaskFromColor() function creates and returns a mask (i.e. a QBitmap) for the pixmap based on a given color. Low-level information The depth() function returns the depth of the pixmap. The defaultDepth() function returns the default depth, i.e. the depth used by the application on the given screen.The cacheKey() function returns a number that uniquely identifies the contents of the QPixmap object.The x11Info() function returns information about the configuration of the X display used by the screen to which the pixmap currently belongs. The x11PictureHandle() function returns the X11 Picture handle of the pixmap for XRender support. Note that the two latter functions are only available on x11. Pixmap ConversionA QPixmap object can be converted into a QImage using the toImage() function. Likewise, a QImage can be converted into a QPixmap using the fromImage(). If this is too expensive an operation, you can use QBitmap::fromImage() instead. The QPixmap class also supports conversion to and from HICON: the toWinHICON() function creates a HICON equivalent to the QPixmap, and returns the HICON handle. The fromWinHICON() function returns a QPixmap that is equivalent to the given icon. Pixmap TransformationsQPixmap supports a number of functions for creating a new pixmap that is a transformed version of the original: The scaled(), scaledToWidth() and scaledToHeight() functions return scaled copies of the pixmap, while the copy() function creates a QPixmap that is a plain copy of the original one. The transformed() function returns a copy of the pixmap that is transformed with the given transformation matrix and transformation mode: Internally, the transformation matrix is adjusted to compensate for unwanted translation, i.e. transformed() returns the smallest pixmap containing all transformed points of the original pixmap. The static trueMatrix() function returns the actual matrix used for transforming the pixmap. Note: When using the native X11 graphics system, the pixmap becomes invalid when the QApplication instance is destroyed. See also QBitmap, QImage, QImageReader, and QImageWriter.]]></content>
      <categories>
        <category>类库</category>
        <category>QT</category>
      </categories>
      <tags>
        <tag>QT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python环境配置]]></title>
    <url>%2F2016%2F02%2F22%2F2016-2-22-pythonEnv%2F</url>
    <content type="text"><![CDATA[将wing的自动补全改为回车(Enter)键入依次点选Edit\Perference\Editor\Auto-completion，找到Completion Keys，选中Enter即可 安装pip从pip官网下载get-pip.py，然后python get-pip.py即可自动安装pip，一般会安装pip到C:\Python34\Scripts，将其添加到path中，之后即可使用pip进行python库的安装。 安装requestsrequests: pip install requests 安装BeautifulSoupBeautifulSoup : pip install beautifulsoup4]]></content>
      <categories>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>环境配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java学习笔记]]></title>
    <url>%2F2015%2F12%2F26%2F2015-12-26-javaNote%2F</url>
    <content type="text"><![CDATA[JLabel设置setBackground后不显示JLabel 可以使用setBackground(new java.awt.Color(R, G, B));来设置背景色，但在实践中遇到问题，即设置后也不显示，查找后发现需要setOpaque(true);即设置为不透明，但发现有时候在代码中设置了也不能立即更新界面，所以需要手动调用repaint();方法。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows下一些常用软件的配置]]></title>
    <url>%2F2015%2F11%2F26%2F2015-11-26-windowsUse%2F</url>
    <content type="text"><![CDATA[打开服务services.msc 使用桌面版onenote时，绘图模式自动变为键入模式使用桌面版onenote时，用笔绘图时，绘图模式老是自动变为键入模式，表现为第一笔可用，然后变为选中框，然后又变为笔，苦寻多处设置无果，最后查到是因为打开了bing词典的划译功能，取消即可。同适用于打开了有道词典划译功能的环境。 notepad++有红色下划线菜单栏—-插件—-DSpellCheck然后将勾选的去掉即可。 notepad++添加对makedown（md）格式的支持参见简略：将配置文件userDefineLang.xml复制到%APPDATA%\Notepad++即可。 高分屏下netbeans菜单字太小的解决办法找到netbeans的安装目录，在etc 目录找到netbeans.conf 文件。打开文件，找到netbeans_default_options的配置项。在最后加上 --fontsize 22,然后重新启动netbeans即可。参考 VS2013中文带红色下划线代码中中文会带红色下划线，因为在VisualAssist X设置了拼写检查，取消拼写检查即可。打开Visual AssistX Options，找到underline spelling errors in comments and string using，取消选中该项即可。 禁止某软件运行1、按windows+R，在运行框中输入“gpedit.msc”，打开组策略。2、选择“用户配置”-“系统”，在右侧双击“不要运行指定的Windows应用程序”，打开属性页，点击“已启用”，点击“显示”，输入要禁用的程序名称（主运行程序名称，不需要加路径），例如notepad.exe然后点一直点确定，完成设置。]]></content>
      <categories>
        <category>软件安装和使用</category>
      </categories>
      <tags>
        <tag>软件安装和使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[真正敢想]]></title>
    <url>%2F2015%2F11%2F10%2F2015-11-10-luojialuntan%2F</url>
    <content type="text"><![CDATA[事情起因是今天下午有珞珈校友论坛，我还挺想去看的。因为有很多成功人士来演讲，他们的知识、眼界等非常值得学习，我也确实去了门口，但看到门口有在查票的，就问了是否需要入场券才能进，回答为是，转身就走了，走回来之后有挺后悔没有去，一直到现在浪费了一个小时来心思不属。现在记录下这件事情，主要是有点感想。 上次职业展望课上听杨柳讲了一句话，要真正敢想，什么是真正敢想？真正敢想就是真正敢做，要为了想的东西去努力，这才是真正敢想。当时她以她自身的例子做了诠释，想要那个副总裁的职位，即使条件不够也要发邮件询问并开始准备。这就是真正敢想。反观这次我，只是想了，但并没有达到真正敢想的程度。我想去看，我就准备去了，但需要入场券，我没有，于是就回来了。这样才只是迈出了一小步。回想当时的过程，查票的并不严，可以混进去，主要是隐约听到了句进去要按票上的座位坐就觉着没有可能了。但回来的路上知道这也不应该阻挡我，就算是票上是有座位号的，那我进去了再出来就好了。如果没有，这是有很大可能的，因为联系上次的参加这个的经历，再稍加分析即可发现不可能在印刷的票上印上座位号。所以还是很大可能进去并听完的。现在想来当时的正确做法应该马上回去然后混进去的，当时为什么没有做呢？因为害怕查票的认出我来了，因为我问过他们了。这又有一个启示，就是做什么事情特别是自己想干的事情时，一定要多观察，并且想好步骤做好计划。例如如果这次我先在外面找个同学看下他的票上有没有座位号，如果没有的话就可以顺利进去了，如果有的话那也没法办，也就不用后悔了。说到后悔，这是一大忌，这也是我正在努力避免的。结合前面两点总结起来就是，先要想要是不是真的想做，如果真的想做就要多观察多思考，把做的方法步骤想好，然后照着做，就算最后失败了那就放下吧，也不用后悔。可以发现，不后悔必定是以前两点为前提的，要想不后悔就要真正敢想，真正敢做，真正有方法的做。如果没有达到这个目标，那应该静下来思考下回应该怎么办。就想现在可以写一篇文档记录下来，这也是好的，主要是别乱了阵脚，心思不属了，这样也会耽误了其他的事情。 另外，其实还是要表扬下我自己的，因为比起之前来我确实成长了许多，能够把想的去付诸现实，以前可能就会怯懦了。就应该这样，世界大得很，谁认识谁啊，想什么大胆去做就好了，不用考虑尴尬什么的。]]></content>
      <categories>
        <category>观察思考</category>
      </categories>
      <tags>
        <tag>观察思考</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自底向上的语法分析]]></title>
    <url>%2F2015%2F11%2F04%2F2015-11-04-CompilerBottomUp%2F</url>
    <content type="text"><![CDATA[Bottom-up Parsing的基本特点 从左到右对Input Terminal 扫描. 根据输入的Input 选择相应的产生式模拟最右推导的逆(也可看成自下而上建立语法树，由叶索根). 无回溯（Non-backtracking） 归约(reduction)是推导关系的逆，最左归约是最右推导关系的逆。最左规约的性质 被归约的对象一定是某一产生式RHS 的文法符号串a，将a 看成是T ∪ N 为字母表的一个正则表达式，b ∈ (T ∪ N)* 是要归约的对象，则在b 中寻找a 子串可以用自动机解决. 被归约对象之后一定是终结符号串. 不确定性 正确的被归约的对象称为句柄(handle)，即它能保证被归约后一定还保持着最右句型.例子:1) exp PLUS term TIMES ID ⇐rmexp PLUS term TIMES fac2) exp PLUS term TIMES ID ⇐rmexp PLUS exp TIMES ID3) exp PLUS term TIMES ID ⇐rmexp TIMES ID只有(1) 是句柄.如果文法是没有二义性的，则一个最右句型的句柄是唯一的. 寻找句柄是解决自底向上分析的关键.移进-规约（shift-reduce）分析器的设计分析栈保留已经归约的句型.自左向右扫描.“stack” + “rest of scan” = 最右句型.句柄总是在栈顶形成.无回溯.问题. 在归约无误的前提下，句柄是否能保证总是在栈顶形成. 如何不回看栈中元素，仅根据栈顶的状态和当前的输入就能够正确地做出移进或归约的操作.即如何保证规约无误。 对于问题一，有栈内的元素一定是已经归约到不能再归约的对象，否则，由于可以规约，则在新的移进前还要归约；句柄的最后一个文法符号一定是栈顶元素。 对于问题二，如何判断句柄已经在栈顶形成活前缀（viable prefix）.语法树的垂直+水平遍历= 活前缀自动机与活前缀利用自动机记录语法树所有可能的``垂直+ 水平’’遍历. 用NFA 的状态记录语法树水平遍历的轨迹 用树的结点作为NFA 的边 用最右边的状态表示不能再水平遍历，是终止状态 状态编号用LR(0) 项目表示 父亲到最左儿子的垂直遍历用epsilon边连接 构造过程 每个产生式对应于语法树可能的水平遍历，为每个产生式构造DFA 用epsilon连接产生式对应的NFA，对• 右边为非终结符的状态用epsilon边连接该非终结符所有的 DFA 的开始状态 引入新的文法开始符号S’。&gt;拓广文法:引入新的文法开始符号后的等价文法称为拓广文法，该文法能保证新的开始符号永远不会出现在RHS中，这样如果归约到该符号，并且输入结束就意味着分析成功. 否则，由于原开始符号可能出现在句型中，分析器在归约到该符号并面对输入结束时不能正确地判断分析是否成功。 性质 活前缀是语法树由父亲到其最左儿子垂直向下遍历或具有相同父亲结点从最左端开始自左向右水平遍历所经过的节点序列，即最右推导。 自动机从开始状态出发的任何一条路经与上述遍历一一对应，即某一文法符号串是活前缀，当且仅当，它是自动机从开始状态到某一个或多个状态所经历的边的序列，形成某一活前缀的所有可能的最右推导与自动机在接受该活前缀后所到达的状态集一致。 自动机从开始状态出发到达任何一个状态，其状态对应的项目一定能最终形成句柄，该自动机称为识别活前缀的自动机(简称前缀自动机). 有效项目集合= 自动机到达状态对应的项目集NFA到DFA 的直接转换由生成式生成DFA，但生成式之间父子关系之间添加的epslion边使之成为了NFA，因此需要将NFA转为DFA。主要为求epsilon-closure()的过程。与之前NFA转DFA的算法差不多。规范LR(0)项目集识别活前缀DFA 中每状态对应的LR(0) 项目集称为规范LR(0) 项目集。 在项目集中形如: A → B • a C 的项目称为移进项目，该项目表示自动机在该状态下还没有形成句柄，希望移进终结符a而形成句柄。 在项目集中形如A → a • C y 的项目称为goto项目，该项目表示自动机在该状态下还没有形成句柄，希望移进Cy 而形成句柄, 而非终结符C 不可能通过移进得到，它只能是通过归约过程获得,因此向下转移。 自动机的终止状态一定含有一个形如B → a• 的项目，称为归约项目，如果自动机到达一个含有归约项目的状态，表示该自动机最后所经历的文法符号串一定是a，即句柄已经在活前缀中形成，需要向上规约。 冲突两种冲突 如果在项目集中同时存在移进和归约项目，称为移进- 归约(S-R) 冲突。在该状态下，分析器可以移进终结符，也可以对已经在分析栈中形成的句柄归约，从而导致分析操作的不确定性。 如果在项目集中同时存在两个不同的归约项目，称为归约- 归约(R-R) 冲突。在该状态下，分析栈中形成的两个句柄归约，分析器不知道用哪一个对应的产生式归约，从而导致分析操作的不确定性。 解消冲突解消S-R冲突：根据当前输入的终结符号决定是否进行归约操作,进行归约的必要条件是∃A → • ∈ I and a ∈ Follow(A),否则，在归约后，由于可能没有含有Aa子串的句型，所以分析器不可能再移进a而形成新的最右句型。 分析表的构造及分析动作分析栈有两项内容: 识别活前缀自动机的状态和文法符号。I0 是自动机的开始状态, 构成自动机的一个接受链。 分析表有两个: Action 表和Goto 表;Goto 表记录归约为非终结符后，状态在接受该非终结符后的转移情况，即是含有goto 项目在接受对应的非终结符到达的状态：goto [sn][A] = Dtrans(sn; A)action 表 对应分析栈栈顶状态面对当前输入进行的移进-归约和状态转移操作，分为以下几种情况： action[sn][ai] = shift sm (简记: ssm): 当Dtrans(sn; ai) = sm; 对应的操作:ai，sm入栈。此时action[sn][ai]的内容代表一个移进事件，如果硬要填上内容的话，则应该填上移进之后的DFA的状态。 action[sn][ai] = reduce A → b (简记: rA → b ): 如果ai ∈ Follow(A) and A → b• ∈ sn, 设|b| = k, 则分析操作:从分析栈中弹出k个元素, 设弹出元素之后的栈顶状态是Sm-k,goto[sm􀀀-k][A] = sp。此时action[sn][ai]代表了一个规约事件，并且需要记录用于规约的产生式（在看这里的时候看action表就是因为这个没搞明白导致好长时间没看懂），如果硬要填上内容的话应该填上文法产生式的代号。 action[sn][$] = accept, if S′ → S• ∈ sn。此时action[sn][ai]表示成功事件。 action[sn][a] = error, 在其他情况下, 分析器报错。此时，action[sn][ai]表示一个error事件。 SLR 文法.SLR 分析表的构造识别活前缀的自动机和规范LR(0) 项目集.First 集和Follow 集.构造Action 表和Goto 表，其中归约操作要看Follow 集. SLR 文法. 由上述方法构造的分析表如果没有S-R 和R-R 冲突，则称对应的文法是SLR(1)（Simple LR(1)，简记SLR）文法. 一个由终结符号组成的符号串是SLR 文法的语句，当且仅当，该串能被SLR 分析表对应的S-R 分析程序所接受. 一个SLR 文法一定是无二义的文法，因为无冲突的SLR 分析表保证了最右推导的唯一性. 一个二义文法也一定不是SLR LR 分析法更欢迎左递归(右增长) 文法, 它能保证移进的终结符在最早的时间进行归约. 文法G是无二义文法，但是也有可能不是SLR 文法，可能存在S-R冲突，因为Follow 集作为归约的条件不充分。解决方法：将后随符号和最右句型联系起来，精确定位每个项目的后随符号.LR(1)项目集的Dtrans.LR(1)项目即一个二元组[LR(0) 项目, 终结符号或$]。[A → alpha • beta ; a]，[A → alpha • beta ; b]，可简记为：[A → alpha • beta ; a/b].[A → alpha • beta; a] 是一个对活前缀 Delta 有效LR(1)项目iff:（1）. S ∗⇒gamma A w ⇒gamma alpha beta w;（2）. delta = gamma alpha;（3）. a ∈ First(w) or a = $, if w = epsilon;LR(1)项目集的epsilon-closure.求解Dtrans(I,X)LR(1)文法 由上述方法构造的分析表如果没有S-R 和R-R 冲突，则称对应的文法是LR(1)文法. 一个由终结符号组成的符号串是LR(1)文法的语句，当且仅当，该串能被LR(1)分析表对应的S-R 分析程序所接受. 一个LR(1)文法一定是无二义的文法，因为无冲突的LR(1)分析表保证了(最左归约) 最右推导的唯一性. 一个二义文法也一定不是LR(1)文法. SLR 文法一定是LR(1)文法, 但是LR(1)文法不一定是SLR 文法,如上例文法G 就不是SLR 文法,可以看出，SLR文法即Fellow集恰好等于LR(1)项目第二分量的特殊情况。. 现有的程序设计语言一般都是LR(1)文法. 分析表太庞大，占用内存太大，如：C 语言的LR(1)分析表有几千个状态和几百个文法符号.]]></content>
      <categories>
        <category>课程</category>
        <category>编译原理</category>
      </categories>
      <tags>
        <tag>编译原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自顶向下的语法分析 LL(1)]]></title>
    <url>%2F2015%2F11%2F01%2F2015-11-01-CompilerTopdown%2F</url>
    <content type="text"><![CDATA[基本概念自顶向下的语法分析（Top-down Parsing） 从左到右对输入的终结符进行扫描. 对输入的语句寻找到一个最左推导. 对输入的语句自顶向下建立语法树模拟最左推导. 无回溯(Backtracking): 分析出错一定是输入的语句出错，而不是分析过程的推导出错. 预测分析法(predictive parser): 根据当前输入和当前需要展开的非终结符选择唯一的产生式. 预测分析法（Predictive Parser）算法 从左到右对输入的终结符扫描，句型的开始状态是文法开始符号和扫描的第一个终结符。 根据当前的终结符a和当前句型的符号X，执行如下操作：if (X ∈ T and X == a) then读下个终结符;句型取下一个符号；即match advancelse 分析出错。if (X /∈ T), 设A = X，选择唯一的一个产生式A →B, 该产生式满足：A -&gt;B -*-&gt;ac,,将句型中的X 替换为B, 并且置当前句型的符号为B的第一个符号. 重复上步直到状态句型中没有符号要处理并且当前输入是文件结束标记. 分类递归下降分析法(Recursive descent parsing)，如:XL 语言；LL(1) 分析法(表驱动)。 共同点共同特点: match() —- advance()，并且对文法有特定的要求. 对文法的要求产生式A → a1 | a2 | a3 ···· | an 只有唯一的一个能展开为a 的句型. 不能有相同的首符号，以消除根据终结符和非终结符来选择产生式的二难，例如A → Aa|b,A 的两个产生式推出的首符号都是b；A → aB | aC，A 的两个产生式推出的首符号都是a，这两个产生式要消除。 消除左递归，以解决不知道推导深度的问题。消除方法：A-&gt;Aa | b1 | b2 | … | Bn,则可替换为A-&gt;bia*,i=1..n。[见课件P9] 消除间接左递归，通过非终结符号分层, 开始符号层次最高,产生终结符的层次最低，低层不能产生高层非终结符，如果产生则进行替换，替换为本层非终结符，从而把间接左递归转化为直接左递归，然后依照上一条进行消除。 消除左公因子。第一点的更一般形式。若A-&gt;ab | ac | d则等价为A-&gt;aA’ | d; A’-&gt;b | c;进行消除。 递归下降分析方法原理 每个非终结符对应于一个函数， 每个终结符对应“match-advance”; 每个函数翻译为其对应的产生式右边符号串对应的调用序列; 多个可选的产生式，用分支语句(“if”; “switch”); 函数调用过程和非终结符展开过程一致，函数的递归调用和语法树的前序遍历一致; 特点 程序简单 递归调用增加内存开支 适用较小规模的文法,如XL语言 LL(1) 分析法(表驱动)原理原理与递归下降分析方法类似，只不过递归下降分析方法是通过函数调用来表示产生式，而LL(1)是通过分析表来存储和表示产生式。 分析表的构造两种情况：直接，间接。直接即通过生成式直接展开到终结符，间接即此非终结符展开为空，下一非终结符展开到一个终结符。 First集含义：每个语法符号串所展开后第一个终结符可能是什么的集合。递归定义见P27。求法：先求文法层次较低非终结符，再求较高的，自底向上继承来对应间接展开的问题。 Follow集含义：每个非终结符后面可能跟随什么终结符。递归定义见P28。注意，Follow集最终不包含ε。求法：先求文法层次较高非终结符，再求较低的，自顶向下继承来对应间接展开的问题。 select集合含义：每个产生式最终展开后第一个终结符集合。求法：见P29 分析表根据select集合来填表。 注：在分析表中的出错栏上添加项目，将不会影响到分析器的识别能力，但是将会影响到分析器报错的时间。 LL(1) 分析表中没有冲突项的文法称为LL(1) 文法, 该文法生成的语言称为LL(1) 语言。LL(1) 文法一定是无二义性的文法，二义文法也一定不是LL(1) 文法。 构成LL(1) 文法的条件对文法的每个产生式 A → a | b有 a 和 b 没有左公因子 a 和 b 最多只有一个能推出ε 如果a=&gt;ε, 则b的首符号一定不在Follow(A) 中 向前查看k个符号，且Select集不冲突的文法称为LL(k)文法; 非LL语言:无论向前查看多少个符号k个符号都不能构造LL(k)文法; 出错处理错误分为语法错误和语义错误。 Parser的任务是精确定位及恢复错误继续分析。 同步符号的处理：a ∈ Follow(A) 称为A 的同步符号。若栈顶为a，M[A][a] = err; a∈ Follow(A)，则弹出A，因为漏掉A的成分是比较合理且可以继续分析的措施。 非同步符号的处理：a /∈ Follow(A), 分析器恢复分析的动作是报错并跳过a,继续读下一个单词，直到出现分析表能够使分析器继续动作的符号或同步符号。 小结LL(1) 分析表的构造步骤 文法转换：消除左递归和左公因子. 对每个非终结符求First和Follow 集合. 对每个产生式求Select 集合. 对同一个非终结符的不同产生式查看Select 集合是否有冲突. 如果没有，填写分析表，注意文件结束标志$ 也是分析表的一项. 出错处理：同步符号. LL(1) 文法的特点 直观，易于理解，实现容易，分析表体积小. 出错处理简单. 文法变换后破坏了原文法的完整性，不利于以后的语义分析. 但现代的LL 分析器自动生成工具使用Extended Backus-Naur Form, 这样消除左递归后不破坏语法结构的完整性. 如：exp → term(+term)*，因此LL 分析法也开始受到欢迎，如Princeton Univ. lcc就是用TopdownParser ANTLR,也广泛使用. 不易于二义性的解消. 并不是所有无二义的上下文无关文法都有LL(k) 文法. 参考大部分内容来自上课时老师的课件，仅作总结，如有侵权，告知即删。]]></content>
      <categories>
        <category>课程</category>
        <category>编译原理</category>
      </categories>
      <tags>
        <tag>编译原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[词法分析 RE DFA 与 NFA]]></title>
    <url>%2F2015%2F10%2F30%2F2015-10-30-CompilerLexer%2F</url>
    <content type="text"><![CDATA[目标单词的形式化描述方法和识别方法 概念 连接 Token（单词） Pattern（模式） Lexeme（词形） 形式化描述：正则表达式此处定义的正则表达式中只有|（并运算，union），.(连接运算，concatenation)，*闭包运算（kleene closure）。正则表达式仅能表达可递归定义的语言集合中的一部分，能被正则表达式描述的叫做正则语言。为了方便书写, 特约定正则表达式的优先级别如下: a&gt;*&gt;.(连接运算)&gt;| 如果语言在递归构造中有两个部分相关联，则不能用正则表达式描述。上例表明正则表达式不能描述在程序语言中嵌套的符号对，如：(和)，注释嵌套等。课件P27 识别方法：DFA(Deterministic Finite Automata).无分词，整个输入串看成一个要识别的整体，没有分词处理，只判断该字符串能否被接受。 有分词，如果DFA 的接受状态还有引出的边到其他的状态,扫描程序必须要要继续向前扫描搜索到一个出错状态,再回退到扫描过程的某个接受状态，从而实现分词，即实现最长匹配。 RE到DFA的转化：NFA(Nondeterministic Finite Automata)为了实现从描述词法的正则表达式到以确定的有限自动机为支撑的识别该词法的词法分析器的自动转换，我们利用NFA作为该转换的桥，通过Thompson构造将正则表达式转换为与之等价的NFA，再用子集构造法将所得到的NFA 转换为与之等价的DFA，从而构造出以该DFA 状态转换表为驱动的识别给定正则表达式的词法分析器. Thompson 构造法.利用正则表达式的递归结构，实现正则表达式到NFA的自动转换. 分别对每个字母a ∈ Σ 和 kong 构造由唯一的接受状的NFA; 如果正则表达式r 和s 对应的NFA Mr 和Ms 已构造，则2.1. 正则表达式 r | s 对应的自动机是将 Mr 和 Ms 并联&#39;&#39;后所得到的自动机 2.2. 正则表达式 rs 对应的自动机是将 Mr 和 Ms串联’’ 后所得到的自动机2.3. 正则表达式 r*对应的自动机是将 Mr 中加上短路边和回路 ε-closureε-closure(B)即B集合中状态通过ε边可达的状态集合。 解决NFA 的不确定性(多条路经接受一个输入串): 枚举所有可能的路径. 子集构造法从NFA到DFA的转换 DFA的最小化定义：可区别状态如果A 和B 是不可区别的, 则从A 状态出发接受的串和从B 状态出发接受的串相同，称之为等价状态. 合并所有的价状态所得到的自动机一定是状态数最少的自动机. 一些性质 L 是正则语言，则Σ*-L 也是正则语言. 可判定性问题 MDFA(r) ∼= MDFA(s) 则r=s 设L1 和L2 是正则语言, 则L1 L2, L1 ∪ L2, L1 ∩ L2,L1 - L2和L1* 也是正则语言 参考本内容来自上课时老师的课件，仅作总结，如有侵权，告知即删。]]></content>
      <categories>
        <category>课程</category>
        <category>编译原理</category>
      </categories>
      <tags>
        <tag>编译原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则&Flex]]></title>
    <url>%2F2015%2F10%2F02%2F2015-10-02-flex%2F</url>
    <content type="text"><![CDATA[Regular Expression语法 x 匹配一个字母，此例即为字符x . 代表除换行符外的任意一个字符 [xyz] 匹配xyz [abj-o] 匹配a，b，j到o中的字母 [^A-Z] 匹配除A-Z之外的所有字符 r* *匹配0个或任意个r r+ 一个或多个r r? 0个或1个r r{2,5} 2-5个r r{2,} 两个及以上个r r{4} 4个r {NAME} the expansion of the “NAME”definition (see below); \X if X is an ‘a’, ‘b’, ‘f’, ‘n’, ‘r’, ‘t’, or ‘v’, then the ANSI-Cinterpretation of \X. Otherwise, a literal ‘X’ (used to escape operators such as ‘*’); \123 代表八进制的123 \x2a 十六进制2a (r) 匹配r，圆括号用来改变运算顺序。match an r; parentheses are used to override precedence (see below); rs 匹配rs，叫做连接”concatenation”， r|s 匹配r或s r/s 匹配模式r，但是要求其后紧跟着模式s。当需要判断本次匹配是否为“最长匹配（longest match）时，模式s匹配的文本也会被包括进来，但完成判断后开始执行对应的动作（action）之前，这些与模式s相配的文本会被返还给输入。所以动作（action）只能看到模式r匹配到的文本。这种模式类型叫做尾部上下文（trailing context）。（有些‘r/s’组合是flex不能识别的；请参看后面deficiencies/bugs一节中的dangerous trailing context的内容。） ^r 行首的r r$ 行尾的r &lt;S&gt;r 匹配一个开始条件为S的r，an r, but only in start condition S (see below for discussion of start conditions); &lt;&lt;EOF&gt;&gt; 匹配文件尾 注意 运算优先级别按上述列表的先后由高到低排列 空格只能出现在” “和[ ]中 “ “和[ ]支持转义字符, 如：”\\x” will match \x; 支持汉字： [\x81-\xfe][\x40-\xfe]匹配一个汉字GBK 码。 [\xb0-\xf7][\xa0-\xfe]匹配一个汉字GB 码。 “土豪”：如果本flex 源程序是用的GBK 码，则该模式匹配GBK 码汉字“土豪” Flex语法结构 DEFINITION PART %% RULE PART %% USER C Source Code DEFINITION PART%{ C语言说明部分（全局变量、包含文件、宏定义和引用说明） %} [indent]C语句 /* pattern definition 模式定义 */ NAME pattern /* inclusive start condition list 包含还是开始条件列表 */ %s S1 S2 /* exclusive start condition list 独占开始开始条件列表 */ %x X1 X2 定义部分的%{}和非顶行的开始直到该行结束所有的文字将直接拷贝到输出文件(去掉%{}), flex不进行任何处理；%{}一定要顶行书写； Pattern Definition 所定义的NAME, 在Rule Part 的Pattern 中可以用{NAME}引用，flex将其替换为对应的正规表达式，如： ID [_a-zA-Z][_0-9a-zA-Z]* 在Rule Part 中可以： {ID} printf (&quot;id: %s\n&quot;, yytext); 等价于 [_a-zA-Z][_0-9a-zA-Z]* printf (&quot;id: %s\n&quot;, yytext); flex不识别任何C 语句，C 代码只能出现在其格式规定的位置，但是为了能正确地将C 语句拷贝到输出文件指定的地方，flex能识别C 的{ }块结构, 注释和字符串常量。 flex提供了机制有条件地激活规则。如果规则的pattern有前缀”“，表示扫描器在名为”sc”的开始条件下该规则才是活动的。例如，]]></content>
      <categories>
        <category>课程</category>
        <category>编译原理</category>
      </categories>
      <tags>
        <tag>编译原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[各种常用软件的安装和配置]]></title>
    <url>%2F2015%2F09%2F19%2F2015-09-30-install%26config%2F</url>
    <content type="text"><![CDATA[VS2013 GLUT配置下载下载链接 配置glut.dll glut32.dll ==&gt; C:\Windows\SysWOW64glut.h ==&gt;C:\Program Files (x86)\Microsoft Visual Studio 12.0\VC\include\glglut32.lib glut.lib ==&gt; C:\Program Files (x86)\Microsoft Visual Studio 12.0\VC\lib 测试#include &lt;gl\glut.h&gt; void myDisplay(void) { glClear(GL_COLOR_BUFFER_BIT); glRectf(-0.5f , -0.5f , 0.5f , 0.5f); glFlush(); } int main(int argc , char *argv[]) { glutInit(&amp;argc , argv); glutInitDisplayMode(GLUT_RGB | GLUT_SINGLE); glutInitWindowPosition(100 , 100); glutInitWindowSize(400 , 400); glutCreateWindow(&quot;第一个OpenGL程序&quot;); glutDisplayFunc(&amp;myDisplay); glutMainLoop(); return 0; } 另外，Windows下为mingw安装OpenGL环境（GLUT)]]></content>
      <categories>
        <category>软件安装和使用</category>
      </categories>
      <tags>
        <tag>软件安装和使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络之中英术语对照表]]></title>
    <url>%2F2015%2F09%2F19%2F2015-09-19-ComputerNet_English2Chinese%2F</url>
    <content type="text"><![CDATA[ARQ (Automatic Repeat reQuest)：自动请求重传。 IEEE 802.3 ： 以太网的协议]]></content>
      <categories>
        <category>课程</category>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络学习之数据链路层]]></title>
    <url>%2F2015%2F09%2F19%2F2015-09-19-ComputerNet_DataLinkLayer%2F</url>
    <content type="text"><![CDATA[基本概念 链路(link)——是一条无源的点到点的物理线路段，中间没有任何其他的交换结点。 通路（path）——由一连串的链路和中间节点构成的源到目的路径。 数据链路(data link)—— 除了物理线路外，还必须有通信协议来控制数据在链路上的传输。若把实现这些协议的硬件和软件加到链路上，就构成了数据链路。 帧（frame）——在数据链路上传递的数据单元一般被称为帧，帧一般由帧头、帧内容、帧尾三部分构成。－帧头表示帧的开始；－帧尾表示帧的结束；－帧内容一般分为协议控制和数据两部分。 涉及问题提供的服务链路管理帧定界差错控制流量控制将数据和控制信息区分透明传输寻址 提供的服务数据链路层的最主要作用在于通过建立长期、临时数据链路，实现帧的传输，为上层提供信息的非路由传递功能。分类：（面向连接，无连接(有确认，无确认)） 帧定界与透明传输字符计数法利用头部中的一个域来指定该帧中的字符数，接收端通过字符计数值来知道帧的结束，要求线路是按字符为单位进行传递的，字符的界定是由低层完成。 带字符填充的首尾界符法在字符计数法上考虑到错误之后的同步问题，让每一个帧都用一些特殊的字符作为帧的开始与结束；绝大多数协议都使用相同的字节作为开始与结束标志，被称为标志字节（flag byte）,也称为起始和结束分界符，连续出现的两个字符代表了当前帧的结束和下一帧的开始。 透明传输的含义在于避免在有效载荷域中出现的与首尾界符相同的字符对帧界定工作的影响。其采用的方法是在有效载荷域中出现的每个标志字节前插入一个特殊的转义字节（ESC），接收端在将数据传送给网络层之前删除掉转义字符，这种技术被称为字节填充（byte stuffing）或字符填充（character stuffing）。有效载荷中出现的转义字符也必须同样处理。 带位填充的首尾标志法在以二进制为最小传输单位的数据链路中，由于数据帧的长度是任意的比特长度，所以采用在每一个帧的开始与结束都添加一个特殊的标志二进制串，例如“01111110”，称为帧起始与结束标志。 透明传输的含义在于避免在有效载荷域中出现的与首尾标志相同的二进制串对帧的界定造成影响。 例如对“01111110”,其采用的方法是：如果发送端在发送有效载荷时，如果每遇到５个“1”后，就在其后添加一个“0”；接收端如果在接收有效负载时，遇到连续的５个“1”，就会讲其后的“0”去掉，这种位填充（bit stuffing）机制与字节填充机制类似。 物理编码违例法 在传输过程中，利用码元的非有效状态来做为数据帧的起始。(详见ppt page13) 差错控制差错控制主要用于对传递的帧进行校验，发现错误就采用重发或纠错。 主要差错情况(正常；数据帧出错；数据帧丢失；确认帧丢失)实用的数据链路层协议大都不采用否认帧进行差错通知，由是采用检验出错误后，不发确认帧，导致发送方超时重传。 两大机制(计时器；帧编号) 计时器重传时间的作用是：数据帧发送完毕后若经过了这样长的时间还没有收到确认帧，就重传这个数据帧。为方便起见，我们设重传时间为tout = tp + tpr+ ta + tp + tpr 设上式右端的处理时间 tpr 和确认帧的发送时间 ta 都远小于传播时延 tp，因此可将重传时间取为两倍的传播时延，即tout = 2tp 两个发送成功的数据帧之间的最小时间间隔是tT = tf + tout = tf + 2tp 设数据帧出现差错(包括帧丢失)的概率为 p，但假设确认帧不会出现差错。设正确传送一个数据帧所需的平均时间 tav tav = tT ( 1 + 一个帧的平均重传次数) 帧编号任何一个编号系统的序号所占用的比特数一定是有限的。因此，经过一段时间后，发送序号就会重复。序号占用的比特数越少，数据传输的额外开销就越小。对于停止等待协议，由于每发送一个数据帧就停止等待，因此用一个比特来编号就够了。一个比特可表示 0 和 1 两种不同的序号。数据帧的发送序号 N(S) 以 0 和 1 交替的方式出现在数据帧中。每发一个新的数据帧，发送序号就和上次发送的不一样。用这样的方法就可以使收方能够区分开新的数据帧和重传的数据帧了。 流量控制流量控制——最重要的就是高速发送者与低速接受者之间的流量控制的问题 。 停止等待协议最简单流量控制在发送结点的发送操作： (1) 从主机取一个数据帧。 (2) 将数据帧送到数据链路层的发送缓存。 (3) 将发送缓存中的数据帧发送出去。 (4) 等待。 (5) 若收到由接收结点发过来的信息(此信息的格式与内容可由双方事先商定好)，则从主机取一个新的数据帧，然后转到(2)。在接收结点的接收操作： (1) 等待。 (2) 若收到由发送结点发过来的数据帧则将其放入数据链路层的接收缓存。 (3) 将接收缓存中的数据帧上交主机。 (4) 向发送结点发一信息，表示数据帧已经上交给主机。 (5) 转到(1)。 实际算法按照习惯的表示法，ACKn 表示“第 n – 1 号帧已经收到，现在期望接收第 n 号帧”。在帧序列位数为1时：ACK1 表示“0 号帧已收到，现在期望接收的下一帧是 1 号帧”；ACK0 表示“1 号帧已收到，现在期望接收的下一帧是 0 号帧”。发送节点：(1) 从主机取一个数据帧，送交发送缓存。(2) V(S)←0。// V(S)、N(S) 是当前帧号，N(S)写入帧中，V(S)存于本节点。(3) N(S)←V(S)。(4) 将发送缓存中的数据帧发送出去。(5) 设置超时计时器。(6) 等待。//等待以下(7)和(8)这两个事件中最先出现的一个(7) 收到确认帧 ACKn， 若 n = 1 – V(s)，则： 从主机取一个新的数据帧，放入发送缓存； V(S)←[1 -V(S)]，转到 (3)。 否则，丢弃这个确认帧，转到(6)。(8) 若超时计时器时间到，则转到(4)。接受节点：(1) V(R)←0。(2) 等待。(3) 收到一个数据帧； 若 N(S) = V(R)，则执行(4)； 否则丢弃此数据帧，然后转到(6)。(4) 将收到的数据帧中的数据部分送交上层软件 （也就是数据链路层模型中的主机）。(5) V(R)←[1 - V(R)]。(6) n←V(R)；发送确认帧 ACKn，转到(2)。优点：比较简单 。缺点：通信信道的利用率不高，也就是说，信道还远远没有被数据比特填满。为了克服这一缺点，就产生了另外两种协议，即连续 ARQ 和选择重传 ARQ。 连续ARQ由于出现帧丢失或确认丢失后，导致连续n帧重传，所以又被称为退后n帧协议。工作原理 在发送完一个数据帧后，不是停下来等待确认帧，而是可以连续再发送若干个数据帧。如果这时收到了接收端发来的确认帧，那么还可以接着发送数据帧。由于减少了等待时间，整个通信的吞吐量就提高了。 要点 接收端只按序接收数据帧。虽然在有差错的 2号帧之后接着又收到了正确的 3 个数据帧，但接收端都必须将这些帧丢弃，因为在这些帧前面有一个 2 号帧还没有收到。虽然丢弃了这些不按序的无差错帧，但应重复发送已发送过的最后一个确认帧（防止确认帧丢失）。 ACK1 表示确认 0 号帧 DATA0，并期望下次收到 1 号帧；ACK2 表示确认 1 号帧 DATA1，并期望下次收到 2 号帧。依此类推。 结点 A 在每发送完一个数据帧时都要设置该帧的超时计时器。如果在所设置的超时时间内收到确认帧，就立即将超时计时器清零。但若在所设置的超时时间到了而未收到确认帧，就要重传相应的数据帧（仍需重新设置超时计时器）。在等不到 2 号帧的确认而重传 2 号数据帧时，虽然结点 A 已经发完了 5 号帧，但仍必须向回走，将 2号帧及其以后的各帧全部进行重传。连续 ARQ 又称为Go-back-N ARQ，意思是当出现差错必须重传时，要向回走 N 个帧，然后再开始重传。 滑动窗口 发送端和接收端分别设定发送窗口和接收窗口 。 发送窗口用来对发送端进行流量控制。 发送窗口的大小 WT 代表在还没有收到对方确认信息的情况下发送端最多可以发送多少个数据帧。 当用 n 个比特进行编号时，若接收窗口的大小为 1，则只有在发送窗口的大小 WT &lt;=2^n -1时，连续 ARQ 协议才能正确运行。 (所有确认帧都丢失,重传帧被当成新的一轮帧) 选择重传ARQ]]></content>
      <categories>
        <category>课程</category>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++学习笔记]]></title>
    <url>%2F2015%2F09%2F19%2F2015-09-19-C%2B%2Bnote%2F</url>
    <content type="text"><![CDATA[iostream 和 stdio的使用iostream可以关闭同步加速，即ios::sync_with_stdio(false);，但顾名思义，此时stdio和iostream不同步，如果混用的话就会出错。 对象赋值一般情况下的对象赋值会把该对象的成员赋给左值，相当于值传递，而不像java一样是引用传递。 例子#include &lt;stdio.h&gt; class A { public: int a; }; int main() { A al[2]; A tt; tt.a = 1; al[0] = tt; tt.a = 2; al[1] = tt; printf(&quot;al[0].a = %d al[1].a=%d&quot; , al[0].a , al[1].a); return 0; } 则输出al[0].a = 1 al[1].a=2 虚函数用virtual修饰的函数称为虚函数，用virtual修饰的函数和java中默认的函数相同。即向上转型为父类声明（指针）后，对父类和子类都有实现的方法会调用子类的方法。这就是C++中的运行时多态。不用virtual修饰时，向上转型为父类声明（指针）后，对父类和子类都有实现的方法会调用父类的方法，这与java是不同的。虚函数并不是抽象函数，因此必须实现。 纯虚函数使用类似 virtual void pure_virtual() = 0;[最后面的”=0”并不表示函数返回值为0,它只起形式上的作用告诉编译系统这是纯虚函数”]的语法声明，包含纯虚函数的类为抽象类，不能实例化，且子类必须实现。和java中的抽象函数类似。但C++中纯虚函数也允许实现，虽然实现以后依然不能实例化，子类还是必须实现，也不能隐式的调用。。。。不过可以显式的调用。所以通常不会使用，也不太符合规范。如果一个类中只有纯虚函数，那这个类就是接口interface了。 例子代码#include &lt;iostream&gt; using namespace std; class A { public: A() { cout &lt;&lt; &quot;construct A&quot; &lt;&lt; endl; } void f1() { cout &lt;&lt; &quot;f1 in \tA&quot; &lt;&lt; endl; } virtual void f2(){ cout &lt;&lt; &quot;f2 in \tA&quot; &lt;&lt; endl; } virtual void f3() = 0{ cout &lt;&lt; &quot;virtual void f3() = 0&quot; &lt;&lt; endl; } }; //子类: class B : public A { public: B() { cout &lt;&lt; &quot;construct B&quot; &lt;&lt; endl; } void f1() { cout &lt;&lt; &quot;f1 in \tB&quot; &lt;&lt; endl; } void f2() { cout &lt;&lt; &quot;f2 in \tB&quot; &lt;&lt; endl; } void f3() { cout &lt;&lt; &quot;f3 in \tB&quot; &lt;&lt; endl; } //virtual ~B(); }; //主函数: int main(int argc , char* argv[]) { //A a();//语法错误，抽象类不允许实例化 A *m_j = new B(); m_j-&gt;f1();//编译时多态 m_j-&gt;f2();//运行时多态 m_j-&gt;f3();//纯虚函数（抽象函数） m_j-&gt;A::f3();//纯虚函数的显式调用（C++特性） B *b = new B(); b-&gt;f1(); b-&gt;f2(); b-&gt;f3(); b-&gt;A::f1();//调用父类方法（C++特性） system(&quot;pause&quot;); return 0; } 输出construct A construct B f1 in A //非虚函数，所以按照声明的调用父类的方法 f2 in B //虚函数，调用子类的实现 f3 in B virtual void f3() = 0 construct A construct B f1 in B f2 in B f3 in B f1 in A 请按任意键继续. . . 参考： 虚函数wikipedia#Java) C语言中文网 c++标准的 接口和抽象类]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C语言中的一些问题]]></title>
    <url>%2F2015%2F09%2F14%2F2015-09-14-Clanguage%2F</url>
    <content type="text"><![CDATA[C语言中的变量声明与定义int a = 10; int a; int a; int main(){ int b = 11; int b; printf(&quot;%d&quot;,a); } 编译过程中全局变量a并不会报错，但局部变量b会报error：redeclaration of ‘b’ with no linkage. 奇怪的函数定义freename(s) char *s; { if( Namep &gt; Names ) *--Namep = s; else fprintf (stderr, &quot;%d: (Internal error) Name stack underflow\n&quot;, 10 ); } 命令行参数int main( int argc, char **argv )中argc为命令行参数的个数，至少为1，**argv代表的命令行输入的参数，第一个为程序本身。 64位编译时，每个指针为8bytesconst char * stra = &quot;helloworld&quot;; const char strb[] = &quot;helloworld&quot;; 结果为： sizeof(str) = 8//字符指针长度 sizeof(*str)= 1//char类型长度 sizeof(strb) = 11//字符数组长度 C语言中类型的解释的问题问题起源是写了一个将IEEE 754标准表示的16进制串转化为float输出的小程序（只是想偷懒不手算。。），代码如下，但没想到的是这个小程序带来了挺多的麻烦。 unsigned int e = 0xBFC00000; printf(“p\n”,e); printf(“e = %f\n”,e);程序输出 BFC00000 e = 0.000000这个输出看看就行，因为我发现上下文环境不同这个输出竟然也是不同的！！！联想到之前看到的一篇文章，感觉应该是printf的问题。之后搜索发现了这个网页找到了正确的方法，即如下的代码： unsigned int e = 0xBFC00000; float b = &e; printf(“%p\n”,e); printf(“e = %f\n”,e); printf(“b = %f \n”,b); b = 1.75; printf(“%p\n”,e); printf(“e = %f\n”); printf(“b = %f \n”,b);输出的是： BFC00000 e = 0.000000 *b = -1.500000 3FE00000 e = -1.500000 *b = 1.750000这下正常了，IEEE 754标准的二进制串可以转换成浮点数输出，浮点数也可以转换成IEEE 754标准的二进制串输出。并且还有意外收获，观察第5行输出，发现其输出竟然与第三行输出一样，回想到上面说的printf上下文相关，竟然就是这么相关，进一步验证得到如下代码： printf(“e = %f\n”,10.0); printf(“e = %f\n”); printf(“a = %d %d\n”,11,12); printf(“a = %d %d \n”);输出的是： e = 10.000000 e = 10.000000 a = 11 12 a = 11 12我勒个去，printf格式化出错的话竟然会直接把上次输出队列中的重新输出一下，这是我之前没想到的。通过这次实验就学到两个知识，一是C中对地址的解释是按照指针的定义进行的，声明成什么样的指针就会按照什么类型进行解释。二是上面说的关于printf的问题。]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PC之死]]></title>
    <url>%2F2015%2F09%2F11%2F2015-09-11-theDeathOfPC%2F</url>
    <content type="text"><![CDATA[关于共享经济虚拟的东西更容易共享，高性能PC的性能就是虚拟的，容易共享。 计算中心+终端的形式是否会有需求对于移动设备便携性进化到极致（但对网络要求高） 给追求高性能PC的用户提供了方便 企业级用户 计算中心+终端对硬件有什么要求当前PC某一时刻的在线量有多少 多少硬件可以给多少终端提供服务？ 计算中心+终端的方式是否会降低成本呢电费，流量费 用户按需付费 计算中心+终端对网络有什么要求计算题：某一时刻会有多少用户在线*流媒体的数据量 流媒体，保证1080P的传输需要多快的网络 当前中国及世界各国的网络情况，以估计所需时间 计算中心+终端对系统有什么要求多用户的系统软件 多用户的应用软件 计算中心+终端对软件的发展有什么影响软件即服务 现有的产品历史及趋势telnet chrome book]]></content>
      <categories>
        <category>观察思考</category>
      </categories>
      <tags>
        <tag>观察思考</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cmd常用命令]]></title>
    <url>%2F2015%2F09%2F11%2F2015-09-11-cmd%2F</url>
    <content type="text"><![CDATA[查看和清空dns缓存ipconfig /displaydns 查看本地缓存的DNS信息ipconfig /flushdns 清除本地缓存的DNS信息 cd更改目录 md/mkdir建立文件夹 rd删除文件夹 copy复制文件夹 del删除文件 dir查看目录 format格式化 ren重命名 type显示文档内容 more逐屏显示文件内容 cls清屏 netstat -an查看所有连接本机IP ipconfig /all显示本机网络完整配置信息 netsh advfirewall firewallwindows自带的防火墙配置，可以完成添加规则，查看规则，更新规则等任务 nslookup可以使用指定的DNS服务器解析域名 net user net user 查看本用户 net user 用户名 密码 /add 建立一个用户（需要管理员权限） net user 用户名 /del 删除一个用户 （需要管理员权限） net localgroup administrators 用户名 /add 把用户添加到管理员 net localgroup administrators 用户名 /del 把用户从管理员组删除 time显示当前时间 date显示当前日期 ver显示当前CMD版本]]></content>
      <categories>
        <category>脚本</category>
        <category>CMD</category>
      </categories>
      <tags>
        <tag>脚本</tag>
        <tag>CMD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[鸣谢]]></title>
    <url>%2F2015%2F08%2F26%2F2015-08-26-thanks%2F</url>
    <content type="text"><![CDATA[鸣谢：感谢YueXy，让我发现了GitHub Pages这个好东西~ 感谢BeiYuu，提供了详细指导，并且本模板直接来自于BeiYuu的GitHub项目。]]></content>
  </entry>
  <entry>
    <title><![CDATA[shadowvpn使用方法]]></title>
    <url>%2F2015%2F08%2F17%2F2015-08-27-shadowvpn%2F</url>
    <content type="text"><![CDATA[shadowvpn的配置方法详见ShadowVPN Linux服务端+Windows客户端编译配置简单教程使用时服务器端$ sudo service shadowvpn start windows客户端shadowvpn.exe -c client.conf -s start]]></content>
      <categories>
        <category>软件安装和使用</category>
      </categories>
      <tags>
        <tag>软件安装和使用</tag>
      </tags>
  </entry>
</search>
