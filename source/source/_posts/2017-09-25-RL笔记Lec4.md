---
layout: post
title: RL课程笔记·第四节·免模型预测
description: 
date: 2017-09-25
categories: 
  - 课程 
  - RL
tags:
  - RL
---


## Introduction
## 蒙特卡洛学习Monte-Carlo Learning
原理：大数定理
1. 初访蒙特卡洛（First-Visit MC）Policy Evaluation: 只记开始状态的$G$
2. 每访蒙特卡洛（Every-Visit MC）Policy Evaluation: 每次遇到都会记下这个状态的估计
递推求平均值的意义：向偏离方向靠近

$N \leftarrow N+1$  
$avg \leftarrow avg+\frac{1}{N}(X_i-avg)$  
将$\frac{1}{N}$换成一个常数$\alpha$得到
$avg \leftarrow avg+\alpha (X_i-avg)$
起到遗忘的作用。


```python
import numpy as np
import matplotlib.pyplot as pl
x = np.random.randint(1,100,[1000])
avga = [x[0]]
for n in range(2,1001):
    p = avga[len(avga)-1]
    avga.append(p+(x[n-1]-p)/n)
pl.plot(range(0,1000),avga)

alpha = 100
avgb = [x[0]]
for n in range(2,1001):
    p = avgb[len(avgb)-1]
    avgb.append(p+(x[n-1]-p)/alpha)
pl.plot(range(0,1000),avgb,'r')
pl.show()
```


![png](/images/output_2_0.png)


## Temporal-Difference Learning
bootstrapping  
这里计算$V(S_t)$使用这个公式（最简单的形式$TD(0)$）：

$$V(S_t) \leftarrow V(S_t)+\alpha (R_{t+1}+ \gamma V(S_{t+1})-V(S_t))$$  
其中，$R_{t+1}+ \gamma V(S_{t+1})$称为TD target，$\delta_t=R_{t+1}+ \gamma V(S_{t+1})-V(S_t)$称为TD error。
## MC和TD对比
1. 蒙特卡洛需要把complete episodes，not by bootstrapping，TD方法使用incomplete episodes，by bootstrapping
2. TD可以在连续环境（没有终止状态）中进行，MC只能在有终止状态的环境中进行。
3. MC中的Return $G_t$是$V_{\pi}(S_t)$ 的非偏估计（unbiased estimate）。大多数情况下TD中的TD target $R_{t+1}+ \gamma V(S_{t+1})-V(S_t)$是实际$V_{\pi}(S_t)$的有偏估计。
4. TD Target比Return的方差更小，因为它只基于一步。
5. MC是高Variance，低bias的，其优点（1）很好的收敛（2）（even with function approximation）（3）对初始值不敏感（4）非常容易理解和使用。
6. TD低Variance，但有bias，其优点（1）通常更高效（2）TD(0)可以收敛到$V_{\pi}(S_t)$（3）（but not always with function approximation）（4）对初始值敏感
7. MC收敛到均方差最小，TD收敛到最可能的马尔科夫模型（solution of max likelihood Markov model）
8. TD利用了Markov性质，MC没有利用，所以在Markov环境中TD方法更有效，在非Markov环境中，MC方法更有效
9. 

| | bootstrapping | sampling |
|:---:|:---:|:---:|
|MC   | ×   | √  |
|DP   | √   | ×  |
|TD   | √   | √  |

## $TD(\lambda)$
MC是走到头，TD是走一步，所以有很多中间状态，即走$n$步，甚至步数不同时也是可以组合在一起的。
*把两个TD Target分开更新和合并到一起更新有区别么？*
  
