---
layout: post
title: CS224d学习过程记录
description: 
date: 2017-03-19
categories: 
  - 公开课
  - CS224d
tags:
  - CS224d
---

# Intro to NLP and Deep Learning	
## Suggested Readings
1. [Linear Algebra Review]
2. [Probability Review]
3. [Convex Optimization Review]
4. [More Optimization (SGD) Review]
5. [From Frequency to Meaning: Vector Space Models of Semantics]
## [Lecture Notes 1]
## [python tutorial] 
## ~~[slides]~~
## ~~[Video](https://www.youtube.com/watch?v=Qy0oEkCZkBI&list=PLlJy-eBtNFt4CSVWYqscHDdP58M3zFHIG)~~
# Simple Word Vector representations: word2vec, GloVe	
## Suggested Readings
1. [Distributed Representations of Words and Phrases and their Compositionality]
2. [Efficient Estimation of Word Representations in Vector Space]
## [slides]

# Pset1
1. [Pset 1] 
2. [Pset 1 Solutions] 
3. [Pset 1 Solutions Code]

# Advanced word vector representations: language models, softmax, single layer networks	
## Suggested Readings
1. [GloVe: Global Vectors for Word Representation]
2. [Improving Word Representations via Global Context and Multiple Word Prototypes]
## [Lecture Notes 2] 
## [slides]

# Neural Networks and backpropagation -- for named entity recognition	
## Suggested Readings
1. [UFLDL tutorial]
2. [Learning Representations by Backpropogating Errors]
## [Lecture Notes 3] 
## [slides]

# Project Advice, Neural Networks and Back-Prop (in full gory detail)	
## Suggested Readings
1. [Natural Language Processing (almost) from Scratch]
2. [A Neural Network for Factoid Question Answering over Paragraphs]
3. [Grounded Compositional Semantics for Finding and Describing Images with Sentences]
4. [Deep Visual-Semantic Alignments for Generating Image Descriptions]
5. [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank]
# [slides]

# Practical tips: gradient checks, overfitting, regularization, activation functions, details	
## Suggested Readings
1. [Practical recommendations for gradient-based training of deep architectures]
2. [UFLDL page on gradient checking]
## [slides]

# Introduction to Tensorflow	
## Suggested Readings
1. [TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems]
## [slides] 
## AWS
1. [AWS Tutorial] 
2. [AWS Tutorial Supplementary] 
3. [AWS Tutorial Video] 

# Pset2
1. [Pset 2]
2. [Pset 2 Solutions] 
3. [Pset 2 Solutions Code]

# Recurrent neural networks -- for language modeling and other tasks	
## Suggested Readings
1. [Recurrent neural network based language model]
2. [Extensions of recurrent neural network language model]
3. [Opinion Mining with Deep Recurrent Neural Networks]
## [slides] 
## [minimal net example (karpathy)] 
## vanishing grad
1. [vanishing grad example] 
2. [vanishing grad notebook] 

# GRUs and LSTMs -- for machine translation	
## Suggested Readings
1. [Long Short-Term Memory]
2. [Gated Feedback Recurrent Neural Networks]
3. [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling]
## [slides]

# Recursive neural networks -- for parsing	
## Suggested Readings
1. [Parsing with Compositional Vector Grammars]
2. [Subgradient Methods for Structured Prediction]
3. [Parsing Natural Scenes and Natural Language with Recursive Neural Networks]
## [Lecture Notes 5] 
## [slides]

# Recursive neural networks -- for different tasks (e.g. sentiment analysis)	
## Suggested Readings
1. [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank]
2. [Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection]
3. [Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks]
## [slides]	

# Review Session for Midterm	
## [slides]

# In-class midterm
1. [midterm solutions]

# Pset3
1. [Pset 3] 
2. [Pset 3 Solutions] 
3. [Pset 3 Solutions Code]

# Convolutional neural networks -- for sentence classification	
## Suggested Readings
1. [A Convolutional Neural Network for Modelling Sentences]
## [slides]

#Course Project Milestone	
1. [milestone description]

# Guest Lecture with Andrew Maas: Speech recognition	
## Suggested Readings
1. [ Deep Neural Networks for Acoustic Modeling in Speech Recognition]
## [slides]

# Guest Lecture with Thang Luong: Machine Translation	
## Suggested Readings
1. [ Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models]
2. [ Addressing the Rare Word Problem in Neural Machine Translation]
3. [ Advances in natural language processing]
4. [ Neural machine translation by jointly learning to align and translate]
## [slides]

# Guest Lecture with Quoc Le: Seq2Seq and Large Scale DL	
## Suggested Readings
1. [ Sequence to Sequence with Neural Networks]
2. [ Neural Machine Translation by Jointly Learning to Align and Translate]
3. [ A Neural Conversation Model]
4. [ Neural Programmer: Include Latent Programs with Gradient Descent]
## [slides]

# The future of Deep Learning for NLP: Dynamic Memory Networks	
## Suggested Readings
1. [Ask me anthing: Dynamic Memory Networks for NLP]
## [slides]
