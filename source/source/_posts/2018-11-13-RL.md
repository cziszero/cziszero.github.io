---
layout: post
title: 深入浅出强化学习 原理入门
description:  深入浅出强化学习 原理入门 读书笔记
date: 2018-09-19
categories: 
  - RL
  - 读书笔记
tags:
  - RL
  - 读书笔记
---

# 基本
## $G_t$定义
$$G_t=\sum_{k=0}^{\infty}{\gamma^kR_{t+k+1}}$$

## $v_{\pi}(s)$函数定义
$$ v_{\pi}(s)=E_{\pi}[G_t|S_t=s]=E_{\pi}[\sum_{k=0}^{\infty}{\gamma^kR_{t+k+1}|S_t=s}] $$

## $q_{\pi}(s,a)$函数定义
$$q_{\pi}(s,a)=E_{\pi}[G_t|S_t=s,A_t=a]=E_{\pi}[\sum_{k=0}^{\infty}{\gamma^kR_{t+k+1}|S_t=s,A_t=a}]$$

## 迭代计算平均
这是下面很多迭代式更新公式的主要根据

$$S_n = \frac{1}{N}\sum_{i=1}^{n}{a_i}=S_{n-1}+\frac{1}{N}(a_n-S_{n-1})$$

## 重要性抽样

# Model-based DP方法
$$v_{\pi}(s)=\sum_{a\in A}{\pi(a|s)}(R_{s}^{a}+\gamma\sum_{s'\in S}{P_{ss'}^{a}v_{\pi}(s')})$$
是一个bootstrpd的方法，因为有${P_{ss'}^{a}}$项，所以是model-based的。

# Model-free的基于表格方法
## MC方法
因为是model-free的, 上面的${P_{ss'}^{a}}$项未知, 所以从定义出发  

$$ v_{\pi}(s)=E_{\pi}[G_t|S_t=s] \\
=E_{\pi}[\sum_{k=0}^{\infty}{\gamma^kR_{t+k+1}|S_t=s}] $$
对$E_{\pi}$使用MC方法采样代替。

**更新公式**为:

$$v_k(s)=\frac{1}{k}\sum_{j=1}^k{G_j}(s)  \\
=v_{k-1}(s)+\frac{1}{k}(G_k(s)-v_{k-1}(s))$$

实际中使用固定的$\alpha$代替$\frac{1}{k}$, 得到

$$v_k(s) \leftarrow v(s) + \alpha (G_k(s)-v(s))$$

使用$\leftarrow$表示更新操作.

类似的有:

$$Q(s,a) \leftarrow Q(s,a)+\alpha(G_t-Q(s,a))$$

## TD方法

$$Q(s,a) \leftarrow Q(s,a)+\alpha[r+\gamma Q(s',a')-Q(s,a)]$$

## $TD(\lambda)$方法

$$Q(s,a) \leftarrow Q(s,a)+\alpha[G_t^\lambda-Q(s,a)]$$

其中$G_t^\lambda$为:

$$G_t^\lambda=(1-\lambda)[G_t^1+\lambda G_t^2 +\lambda^2 G_t^3+...+\lambda^{n-1} G_t^n $$

其中$G_t^n$表示向后看几步. 本质上$TD(\lambda)$方法是TD方法和MC方法的折衷, 因为MC方法虽然是无偏估计, 但因为需要一个回合结束才能计算, 这个过程中随机性很大, 所以方差非常大. 而TD方法由于计算TD-Target时使用了$Q(s,a)$本身, 所以是有偏估计, 但好处是方差较小, 落实到具体的事件中, 就是较容易训练. $TD(\lambda)$方法采用多步估计值组合, 比TD方法扩大了方差但估计更准确.

# Model-free的基于值函数逼近的方法


