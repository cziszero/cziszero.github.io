---
layout: post
title: RL课程笔记·第一节·RL简介
description: 
date: 2017-09-23
categories: 
  - 课程 
  - RL 
tags:
  - RL
---

## Lec 1

### 关于RL

RL和监督学习，非监督学习是并列的

#### RL的特性

1. 没有监督，通过reward信号来训练
2. 反馈是延迟的而不是即时的
3. time非常重要，通常并不是独立同分布的
4. agent的action影响他之后环境

### RL问题

#### RL术语

* Rewards $R_t$  
  一个标量的反馈信号，表明在t步agent的表现，agent的目标就是最大化各步累计的reward。
  RL就是基于reward假设，即所有的目标都可以通过最大化期望累计reward来描述
* 序列决策 Sequential Decision Marking  
  目标是做一系列action来最大化所有的未来reward，其中有个限制：action可能有长期的影响，reward可能被延迟，可能牺牲掉眼前利益可以获得更大的长期利益。
* Agent and Environment
* Observation $O_t$
* Action $A_t$  
* History $H_t$  
  $H_t = O_1,R_1,A_1,...,A_{t-1},O_t,R_t$
* State $S_t$  
  用来决定将要发生什么的信息，正式的，state是history的函数，$S_t=f(H_t)$
* Environment State $S_t^e$  
  Environment用来决定下阶段的发射的observation和reward
* Agent State $S_t^a$  
  Agent用来决定下阶段的action，是RL算法中需要的信息，$S_t^a=f(H_t)$
* Information State(Markov State)  
  当前状态包含了历史中所有有用信息的状态，即Markov过程。$H_{1:t}\rightarrow S_t \rightarrow H_{t+1:\infty}$
* 完全可观察的环境 Fully Observable Environment（MDP）
  agent可以直接观察环境，$O_t=S_t^a=S_t^e$，正式的，这就是马尔科夫决策过程（Markov decision process，MDP）
* 部分可观察环境 Partially Observable Environment（POMDP）  
  agent间接的观察环境，此时$S_t^a \neq S_t^e$，称作部分可观察马尔科夫决策过程（Partially observable Markov decision process，POMDP）。agent必须自己构造$S_t^a$

* Policy $A_t \pi$
  agent的行为函数，从状态s到动作action的映射，可以是确定式的，$a = \pi (s)$，也可以是随机形式的，$\pi(a|s) = P[A_t=a|S_t=s]$
* Value function $V$
  描述这个state/action有多好，即预测将来的reward，从而做出一个好的action，$$V_{\pi}(s)=\mathbb{E}_{\pi}[R_{t+1}+\gamma R_{t+1}+\gamma^2 R_{t+2}+...| S_t=s]$$
* Model  
  model预测环境接下来干什么，$P_{ss'}^a$描述状态的变化，$R_s^a$描述未来所能获得的reward，有$$P_{ss'}^a=\mathbb{P}[S_{t+1=s'| S_t=s,A_t=a}]$$  $$R_s^a=\mathbb{E}[R_{t+1}| S_t=s,A_t=a]$$
  

### Inside An RL Agent

组成，一个RL agent必须包含Policy，Value function，Model中的一个或几个部分。

#### 分类

* Value Based
* Policy Based
* Actor Critic
* Model Free
* Model Based
  
### RL中的问题

#### Learning 和 Planning

* RL中，环境初始是未知的，agent和环境进行交互，agent改进的是他的policy
* Planning中，环境（Model）初始是已知的，agent不和环境交互，也是改进的他的policy

#### 探索和开发 Exploration and Exploitation

RL 类似于不断试错然后学习，需要不断探索（Exploration），发现环境的更多信息，然后利用已知信息上开发（Exploitation）最大化reward的方法。

#### 预测和控制 Prediction and Control

预测是给你这Policy让你去评估将来reward，Control让你找到一个最优的Policy来得到一个最优的将来reward
